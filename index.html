<!DOCTYPE html>
<html lang="zh-CN">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>AWS Certified AI Practitioner AIF-C01 答题系统</title>
    <style>
      /* AWS Certified AI Practitioner AIF-C01 答题系统样式 */

      * {
        margin: 0;
        padding: 0;
        box-sizing: border-box;
      }

      body {
        font-family: "Segoe UI", Tahoma, Geneva, Verdana, sans-serif;
        line-height: 1.6;
        color: #333;
        background-color: #f5f7fa;
      }

      .container {
        max-width: 1400px;
        margin: 0 auto;
        padding: 20px;
      }

      /* 头部样式 */
      header {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        padding: 30px;
        border-radius: 15px;
        margin-bottom: 30px;
        box-shadow: 0 10px 30px rgba(0, 0, 0, 0.1);
      }

      header h1 {
        font-size: 2.2em;
        margin-bottom: 20px;
        text-align: center;
        text-shadow: 0 2px 4px rgba(0, 0, 0, 0.3);
      }

      .progress-info {
        display: flex;
        align-items: center;
        gap: 20px;
      }

      .progress-bar {
        flex: 1;
        height: 10px;
        background-color: rgba(255, 255, 255, 0.3);
        border-radius: 5px;
        overflow: hidden;
      }

      .progress-fill {
        height: 100%;
        background: linear-gradient(90deg, #4ade80, #22c55e);
        width: 0%;
        transition: width 0.3s ease;
      }

      .stats {
        display: flex;
        gap: 20px;
        font-weight: bold;
        font-size: 1.1em;
      }

      /* 主要内容区域 */
      main {
        display: grid;
        grid-template-columns: 3fr 1fr;
        gap: 30px;
        margin-bottom: 30px;
      }

      /* 题目容器 */
      .question-container {
        background: white;
        border-radius: 15px;
        padding: 30px;
        box-shadow: 0 5px 15px rgba(0, 0, 0, 0.08);
      }

      .question-header {
        display: flex;
        justify-content: space-between;
        align-items: center;
        margin-bottom: 25px;
        padding-bottom: 15px;
        border-bottom: 2px solid #e5e7eb;
      }

      .question-number {
        font-size: 1.5em;
        color: #1f2937;
        font-weight: bold;
      }

      .question-number.answered {
        color: #059669;
      }

      .question-number.answered::after {
        content: " ✓";
        color: #10b981;
        font-weight: bold;
        margin-left: 5px;
      }

      .navigation {
        display: flex;
        gap: 10px;
      }

      .navigation button {
        padding: 8px 16px;
        border: none;
        border-radius: 8px;
        background: linear-gradient(135deg, #3b82f6, #1d4ed8);
        color: white;
        cursor: pointer;
        font-weight: 500;
        transition: all 0.2s ease;
      }

      .navigation button:hover {
        background: linear-gradient(135deg, #2563eb, #1e40af);
        transform: translateY(-1px);
      }

      .navigation button:disabled {
        background: #9ca3af;
        cursor: not-allowed;
        transform: none;
      }

      .question-content {
        margin-bottom: 30px;
      }

      .question-text {
        font-size: 1.1em;
        line-height: 1.7;
        margin-bottom: 25px;
        color: #374151;
        font-weight: 400;
      }

      /* 选项样式 */
      .options {
        display: flex;
        flex-direction: column;
        gap: 15px;
        margin-bottom: 30px;
      }

      .option {
        position: relative;
      }

      .option input[type="radio"] {
        display: none;
      }

      .option label {
        display: flex;
        align-items: flex-start;
        gap: 15px;
        padding: 15px;
        border: 2px solid #e5e7eb;
        border-radius: 10px;
        cursor: pointer;
        transition: all 0.3s ease;
        background: #fafafa;
      }

      .option label:hover {
        border-color: #3b82f6;
        background: #eff6ff;
      }

      .option input[type="radio"]:checked + label {
        border-color: #3b82f6;
        background: linear-gradient(135deg, #dbeafe, #bfdbfe);
        box-shadow: 0 0 0 3px rgba(59, 130, 246, 0.1);
      }

      /* 已答题目的选项高亮样式 */
      .option.correct-answer {
        background: linear-gradient(135deg, #dcfce7, #bbf7d0) !important;
        border-color: #16a34a !important;
        box-shadow: 0 0 0 3px rgba(22, 163, 74, 0.2) !important;
      }

      .option.correct-answer .option-letter {
        background: #16a34a !important;
        color: white !important;
      }

      .option.wrong-answer {
        background: linear-gradient(135deg, #fef2f2, #fecaca) !important;
        border-color: #dc2626 !important;
        box-shadow: 0 0 0 3px rgba(220, 38, 38, 0.2) !important;
      }

      .option.wrong-answer .option-letter {
        background: #dc2626 !important;
        color: white !important;
      }

      .option-letter {
        background: #3b82f6;
        color: white;
        width: 30px;
        height: 30px;
        border-radius: 50%;
        display: flex;
        align-items: center;
        justify-content: center;
        font-weight: bold;
        font-size: 0.9em;
        flex-shrink: 0;
      }

      .option-text {
        flex: 1;
        line-height: 1.5;
      }

      /* 题目操作按钮 */
      .question-actions {
        display: flex;
        gap: 15px;
        margin-bottom: 25px;
        position: relative;
      }

      .question-actions button {
        padding: 12px 24px;
        border: none;
        border-radius: 8px;
        font-size: 1em;
        font-weight: 600;
        cursor: pointer;
        transition: all 0.2s ease;
      }

      #submit-btn {
        background: linear-gradient(135deg, #10b981, #059669);
        color: white;
        flex: 1;
        transition: all 0.3s ease;
      }

      #submit-btn:hover:not(:disabled) {
        background: linear-gradient(135deg, #059669, #047857);
        transform: translateY(-1px);
      }

      #submit-btn:disabled {
        background: #9ca3af !important;
        color: #ffffff !important;
        cursor: not-allowed !important;
        transform: none !important;
        opacity: 0.6 !important;
        box-shadow: none !important;
      }

      /* 解释区域 */
      .explanation {
        background: #f0f9ff;
        border: 1px solid #0ea5e9;
        border-radius: 10px;
        padding: 20px;
        margin-top: 20px;
      }

      .explanation h3 {
        color: #0ea5e9;
        margin-bottom: 10px;
        font-size: 1.2em;
      }

      .explanation p {
        color: #374151;
        line-height: 1.6;
      }

      /* 题目导航 */
      .question-nav {
        background: white;
        border-radius: 15px;
        padding: 25px;
        box-shadow: 0 5px 15px rgba(0, 0, 0, 0.08);
        height: fit-content;
      }

      .question-nav h3 {
        margin-bottom: 20px;
        color: #1f2937;
        font-size: 1.3em;
      }

      .nav-grid {
        display: grid;
        grid-template-columns: repeat(12, 1fr);
        gap: 4px;
      }

      .nav-btn {
        width: 35px;
        height: 35px;
        border: 1px solid #d1d5db;
        border-radius: 6px;
        background: white;
        color: #374151;
        font-size: 0.65em;
        font-weight: 600;
        cursor: pointer;
        transition: all 0.2s ease;
        display: flex;
        flex-direction: column;
        align-items: center;
        justify-content: center;
        gap: 2px;
      }

      .nav-question-number {
        font-size: 1.2em;
        font-weight: bold;
        line-height: 1;
      }

      .nav-question-type {
        font-size: 1.2em;
        color: #6b7280;
        font-weight: bold;
        line-height: 1;
      }

      /* 为不同题型的图标设置不同颜色 */
      .nav-btn[data-type="single"] .nav-question-type {
        color: #059669; /* 单选题 - 绿色 */
      }

      .nav-btn[data-type="multiple"] .nav-question-type {
        color: #dc2626; /* 多选题 - 红色 */
      }

      .nav-btn[data-type="dropdown"] .nav-question-type {
        color: #7c3aed; /* 下拉选择题 - 紫色 */
      }

      .nav-btn:hover {
        background: #f3f4f6;
        border-color: #9ca3af;
      }

      .nav-btn:hover .nav-question-type {
        opacity: 0.8;
      }

      .nav-btn.answered {
        background: linear-gradient(135deg, #dbeafe, #bfdbfe);
        border-color: #3b82f6;
        color: #1e40af;
        font-weight: bold;
      }

      .nav-btn.correct {
        background: #dcfce7;
        border-color: #16a34a;
        color: #166534;
      }

      .nav-btn.incorrect {
        background: #fef2f2;
        border-color: #dc2626;
        color: #991b1b;
      }

      .nav-btn.current {
        border-color: #3b82f6 !important;
        box-shadow: 0 0 0 3px rgba(59, 130, 246, 0.5) !important;
        position: relative;
      }

      .nav-btn.current::after {
        content: "";
        position: absolute;
        top: -2px;
        left: -2px;
        right: -2px;
        bottom: -2px;
        background: transparent;
        border: 2px solid #3b82f6;
        border-radius: 10px;
        z-index: -1;
      }

      /* Toast 提示样式 */
      .toast {
        position: absolute;
        bottom: -60px;
        left: 50%;
        transform: translateX(-50%);
        padding: 12px 20px;
        border-radius: 8px;
        color: white;
        font-weight: 600;
        font-size: 0.9em;
        z-index: 1000;
        opacity: 0;
        transform: translateX(-50%) translateY(20px);
        transition: all 0.3s ease;
        max-width: 300px;
        box-shadow: 0 4px 12px rgba(0, 0, 0, 0.2);
        display: flex;
        align-items: center;
      }

      .toast.show {
        opacity: 1;
        transform: translateX(-50%) translateY(0);
      }

      .toast.success {
        background: linear-gradient(135deg, #10b981, #059669);
      }

      .toast.error {
        background: linear-gradient(135deg, #ef4444, #dc2626);
      }

      .toast-icon {
        margin-right: 10px;
        font-size: 1.1em;
        flex-shrink: 0;
      }

      .toast-content {
        flex: 1;
      }

      /* 页脚 */
      footer {
        text-align: center;
        color: #6b7280;
        padding: 20px;
        background: white;
        border-radius: 10px;
        box-shadow: 0 2px 10px rgba(0, 0, 0, 0.05);
      }

      /* 响应式设计 */
      @media (max-width: 1024px) {
        main {
          grid-template-columns: 1fr;
        }

        .nav-grid {
          grid-template-columns: repeat(10, 1fr);
        }
      }

      @media (max-width: 768px) {
        .container {
          padding: 10px;
        }

        header {
          padding: 20px;
        }

        header h1 {
          font-size: 1.8em;
        }

        .progress-info {
          flex-direction: column;
          gap: 15px;
        }

        .stats {
          flex-wrap: wrap;
          gap: 10px;
          font-size: 1em;
        }

        .stats span {
          flex: 1 1 calc(50% - 5px);
          min-width: 140px;
        }

        .question-container {
          padding: 20px;
        }

        .question-header {
          flex-direction: column;
          align-items: flex-start;
          gap: 15px;
        }

        .nav-grid {
          grid-template-columns: repeat(6, 1fr);
        }

        .question-actions {
          flex-direction: column;
        }
      }

      @media (max-width: 480px) {
        header h1 {
          font-size: 1.5em;
          margin-bottom: 15px;
        }

        .stats {
          font-size: 0.9em;
          gap: 8px;
        }

        .stats span {
          flex: 1 1 100%;
          text-align: center;
          padding: 5px;
          background: rgba(255, 255, 255, 0.1);
          border-radius: 5px;
        }

        .nav-grid {
          grid-template-columns: repeat(8, 1fr);
        }

        .nav-btn {
          width: 32px;
          height: 32px;
          font-size: 0.6em;
        }
      }
    </style>
    <script>
      window.questionsData = [
        {
          number: "1",
          question:
            "A company makes forecasts each quarter to decide how to optimize operations to meet expected demand. The company uses ML models to make these forecasts. An AI practitioner is writing a report about the trained ML models to provide transparency and explainability to company stakeholders. What should the AI practitioner include in the report to meet the transparency and explainability requirements?",
          options: {
            A: "Code for model training",
            B: "Partial dependence plots (PDPs)",
            C: "Sample data for training",
            D: "Model convergence tables",
          },
          correct_answer: "B",
          explanation:
            "Partial Dependence Plots (PDPs) are a powerful tool for understanding and explaining how the features in a machine learning model impact predictions. They are often used to meet transparency and explainability requirements for stakeholders. Let's go over why this is the correct choice, along with why the other options are less suitable: Partial Dependence Plots (PDPs) Purpose: PDPs show the relationship between a feature (or multiple features) and the model's predicted output, which helps to explain the effect of each feature on the model’s predictions. Explainability: By visualizing how each feature influences the prediction, stakeholders can better understand how the model works and why it makes certain predictions. This level of interpretability is essential for gaining trust from non-technical stakeholders. Transparency: PDPs improve transparency by providing an intuitive way to analyze and present the effects of individual features.",
          id: 2,
        },
        {
          number: "2",
          question:
            "A law firm wants to build an AI application by using large language models (LLMs). The application will read legal documents and extract key points from the documents. Which solution meets these requirements?",
          options: {
            A: "Build an automatic named entity recognition system.",
            B: "Create a recommendation engine.",
            C: "Develop a summarization chatbot.",
            D: "Develop a multi-language translation system.",
          },
          correct_answer: "C",
          explanation:
            "A summarization chatbot can effectively read legal documents and generate concise versions that highlight key points. This directly addresses the requirement of extracting essential information, unlike the other options, which focus on different tasks.",
          id: 3,
        },
        {
          number: "3",
          question:
            "A company wants to classify human genes into 20 categories based on gene characteristics. The company needs an ML algorithm to document how the inner mechanism of the model affects the output. Which ML algorithm meets these requirements?",
          options: {
            A: "Decision trees",
            B: "Linear regression",
            C: "Logistic regression",
            D: "Neural networks",
          },
          correct_answer: "A",
          explanation:
            "Decision trees provide clear transparency into how the model makes decisions, allowing easy documentation of how the inner mechanism influences the output. The other options do not offer this level of interpretability.",
          id: 4,
        },
        {
          number: "4",
          question:
            "A company has built an image classification model to predict plant diseases from photos of plant leaves. The company wants to evaluate how many images the model classified correctly. Which evaluation metric should the company use to measure the model's performance?",
          options: {
            A: "R-squared score",
            B: "Accuracy",
            C: "Root mean squared error (RMSE)",
            D: "Learning rate",
          },
          correct_answer: "B",
          explanation:
            "Accuracy measures how many images were correctly classified out of the total images, making it the appropriate metric for evaluating the performance of an image classification model. The other metrics are either not suitable for classification tasks or not used for performance evaluation.",
          id: 5,
        },
        {
          number: "5",
          question:
            "A company is using a pre-trained large language model (LLM) to build a chatbot for product recommendations. The company needs the LLM outputs to be short and written in a specific language. Which solution will align the LLM response quality with the company's expectations?",
          options: {
            A: "Adjust the prompt.",
            B: "Choose an LLM of a different size.",
            C: "Increase the temperature.",
            D: "Increase the Top K value.",
          },
          correct_answer: "A",
          explanation:
            "Adjusting the prompt allows you to specify the desired length and language of the LLM's responses, making it suitable for tailoring the output to meet the company's needs. The other options do not directly control response length or language.",
          id: 6,
        },
        {
          number: "6",
          question:
            "A company uses Amazon SageMaker for its ML pipeline in a production environment. The company has large input data sizes up to 1 GB and processing times up to 1 hour. The company needs near real-time latency. Which SageMaker inference option meets these requirements?",
          options: {
            A: "Real-time inference",
            B: "Serverless inference",
            C: "Asynchronous inference",
            D: "Batch transform",
          },
          correct_answer: "C",
          explanation:
            "Asynchronous inference is suitable for handling large input data and long processing times while still providing responses without blocking other requests. It allows for near real-time latency, whereas the other options are less suitable given the input size and processing time constraints.",
          id: 7,
        },
        {
          number: "7",
          question:
            "A company is using domain-specific models. The company wants to avoid creating new models from the beginning. The company instead wants to adapt pre-trained models to create models for new, related tasks. Which ML strategy meets these requirements?",
          options: {
            A: "Increase the number of epochs.",
            B: "Use transfer learning.",
            C: "Decrease the number of epochs.",
            D: "Use unsupervised learning.",
          },
          correct_answer: "B",
          explanation:
            "Transfer learning allows the company to adapt pre-trained models for new, related tasks, saving time and resources compared to training models from scratch. The other options do not address the goal of reusing existing models.",
          id: 8,
        },
        {
          number: "8",
          question:
            "A company is building a solution to generate images for protective eyewear. The solution must have high accuracy and must minimize the risk of incorrect annotations. Which solution will meet these requirements?",
          options: {
            A: "Human-in-the-loop validation by using Amazon SageMaker Ground Truth Plus",
            B: "Data augmentation by using an Amazon Bedrock knowledge base",
            C: "Image recognition by using Amazon Rekognition",
            D: "Data summarization by using Amazon QuickSight Q",
          },
          correct_answer: "A",
          explanation:
            "Human-in-the-loop validation ensures high accuracy by involving human reviewers to verify and correct annotations, minimizing the risk of errors in the generated images. The other options are not directly relevant for ensuring annotation accuracy in image generation.",
          id: 9,
        },
        {
          number: "9",
          question:
            "A company wants to create a chatbot by using a foundation model (FM) on Amazon Bedrock. The FM needs to access encrypted data that is stored in an Amazon S3 bucket. The data is encrypted with Amazon S3 managed keys (SSE-S3). The FM encounters a failure when attempting to access the S3 bucket data. Which solution will meet these requirements?",
          options: {
            A: "Ensure that the role that Amazon Bedrock assumes has permission to decrypt data with the correct encryption key.",
            B: "Set the access permissions for the S3 buckets to allow public access to enable access over the internet.",
            C: "Use prompt engineering techniques to tell the model to look for information in Amazon S3.",
            D: "Ensure that the S3 data does not contain sensitive information.",
          },
          correct_answer: "A",
          explanation:
            "The foundation model needs the appropriate permissions to decrypt the encrypted data in the S3 bucket. Ensuring that the role used by Amazon Bedrock has permission to access and decrypt the data will resolve the access failure. The other options are not suitable for addressing the encryption and permission issue.",
          id: 10,
        },
        {
          number: "10",
          question:
            "A company wants to use language models to create an application for inference on edge devices. The inference must have the lowest latency possible. Which solution will meet these requirements?",
          options: {
            A: "Deploy optimized small language models (SLMs) on edge devices.",
            B: "Deploy optimized large language models (LLMs) on edge devices.",
            C: "Incorporate a centralized small language model (SLM) API for asynchronous communication with edge",
            D: "Incorporate a centralized large language model (LLM) API for asynchronous communication with edge",
          },
          correct_answer: "A",
          explanation:
            "Deploying optimized small language models (SLMs) directly on edge devices provides low latency for inference since the computation happens locally, avoiding the delays associated with network communication. The other options either increase latency or are less suitable for edge deployment.",
          id: 11,
        },
        {
          number: "11",
          question:
            "A company wants to build an ML model by using Amazon SageMaker. The company needs to share and manage variables for model development across multiple teams. Which SageMaker feature meets these requirements?",
          options: {
            A: "Amazon SageMaker Feature Store",
            B: "Amazon SageMaker Data Wrangler",
            C: "Amazon SageMaker Clarify",
            D: "Amazon SageMaker Model Cards",
          },
          correct_answer: "A",
          explanation:
            "Amazon SageMaker Feature Store is a centralized repository for storing and managing features, allowing multiple teams to share and reuse variables (features) for model development. The other options serve different purposes, such as data preprocessing or model documentation.",
          id: 12,
        },
        {
          number: "12",
          question:
            "A company wants to use generative AI to increase developer productivity and software development. The company wants to use Amazon Q Developer. What can Amazon Q Developer do to help the company meet these requirements?",
          options: {
            A: "Create software snippets, reference tracking, and open source license tracking.",
            B: "Run an application without provisioning or managing servers.",
            C: "Enable voice commands for coding and providing natural language search.",
            D: "Convert audio files to text documents by using ML models.",
          },
          correct_answer: "A",
          explanation:
            "Amazon Q Developer is designed to assist developers by generating software snippets, tracking references, and managing open source licenses, which aligns with the company's goal of increasing productivity in software development. The other options do not match the intended use of Amazon Q Developer.",
          id: 13,
        },
        {
          number: "13",
          question:
            "A financial institution is using Amazon Bedrock to develop an AI application. The application is hosted in a VPC. To meet regulatory compliance standards, the VPC is not allowed access to any internet traffic. Which AWS service or feature will meet these requirements?",
          options: {
            A: "AWS PrivateLink",
            B: "Amazon Macie",
            C: "Amazon CloudFront",
            D: "Internet gateway",
          },
          correct_answer: "A",
          explanation:
            "AWS PrivateLink allows secure, private connectivity between VPCs and AWS services without needing internet access, making it suitable for meeting regulatory compliance standards. The other options either do not provide private connectivity or require internet access.",
          id: 14,
        },
        {
          number: "14",
          question:
            'A company wants to develop an educational game where users answer questions such as the following: "A jar contains six red, four green, and three yellow marbles. What is the probability of choosing a green marble from the jar?" Which solution meets these requirements with the LEAST operational overhead?',
          options: {
            A: "Use supervised learning to create a regression model that will predict probability.",
            B: "Use reinforcement learning to train a model to return the probability.",
            C: "Use code that will calculate probability by using simple rules and computations.",
            D: "Use unsupervised learning to create a model that will estimate probability density.",
          },
          correct_answer: "C",
          explanation:
            "Calculating probability in this scenario involves straightforward arithmetic, making it most efficient to use simple rules and computations. This approach requires the least operational overhead compared to using complex ML models, which are unnecessary for such basic tasks.",
          id: 15,
        },
        {
          number: "15",
          question:
            "Which metric measures the runtime efficiency of operating AI models?",
          options: {
            A: "Customer satisfaction score (CSAT)",
            B: "Training time for each epoch",
            C: "Average response time",
            D: "Number of training instances",
          },
          correct_answer: "C",
          explanation:
            "Average response time measures how quickly an AI model produces an output, which reflects the runtime efficiency of the model. The other options do not directly measure the efficiency of operating AI models.",
          id: 16,
        },
        {
          number: "16",
          question:
            "A company is building a contact center application and wants to gain insights from customer conversations. The company wants to analyze and extract key information from the audio of the customer calls. Which solution meets these requirements?",
          options: {
            A: "Build a conversational chatbot by using Amazon Lex.",
            B: "Transcribe call recordings by using Amazon Transcribe.",
            C: "Extract information from call recordings by using Amazon SageMaker Model Monitor.",
            D: "Create classification labels by using Amazon Comprehend.",
          },
          correct_answer: "B",
          explanation:
            "Amazon Transcribe converts audio recordings into text, which allows for further analysis and extraction of key information from customer conversations. The other options do not directly handle audio transcription or extraction of information from audio.",
          id: 17,
        },
        {
          number: "17",
          question:
            "A company has petabytes of unlabeled customer data to use for an advertisement campaign. The company wants to classify its customers into tiers to advertise and promote the company's products. Which methodology should the company use to meet these requirements?",
          options: {
            A: "Supervised learning",
            B: "Unsupervised learning",
            C: "Reinforcement learning",
            D: "Reinforcement learning from human feedback (RLHF)",
          },
          correct_answer: "B",
          explanation:
            "Unsupervised learning is suitable for analyzing unlabeled data and grouping it into clusters or tiers, which aligns with the company’s goal of classifying customers. The other methods require labeled data or are used for different types of problems.",
          id: 18,
        },
        {
          number: "18",
          question:
            "An AI practitioner wants to use a foundation model (FM) to design a search application. The search application must handle queries that have text and images. Which type of FM should the AI practitioner use to power the search application?",
          options: {
            A: "Multi-modal embedding model",
            B: "Text embedding model",
            C: "Multi-modal generation model",
            D: "Image generation model",
          },
          correct_answer: "A",
          explanation:
            "A multi-modal embedding model can handle both text and image queries by embedding them into a shared space, enabling the search application to process and relate different data types. The other options are not suitable for handling both text and image inputs effectively.",
          id: 19,
        },
        {
          number: "19",
          question:
            "A company uses a foundation model (FM) from Amazon Bedrock for an AI search tool. The company wants to fine-tune the model to be more accurate by using the company's data. Which strategy will successfully fine-tune the model?",
          options: {
            A: "Provide labeled data with the prompt field and the completion field.",
            B: "Prepare the training dataset by creating a .txt file that contains multiple lines in .csv format.",
            C: "Purchase Provisioned Throughput for Amazon Bedrock.",
            D: "Train the model on journals and textbooks.",
          },
          correct_answer: "A",
          explanation:
            "Fine-tuning a foundation model involves training it with labeled data that contains both input prompts and corresponding expected completions to adjust the model’s behavior to fit the company’s needs. The other options are not directly related to the fine-tuning process using specific labeled data.",
          id: 20,
        },
        {
          number: "20",
          question:
            "A company wants to use AI to protect its application from threats. The AI solution needs to check if an IP address is from a suspicious source. Which solution meets these requirements?",
          options: {
            A: "Build a speech recognition system.",
            B: "Create a natural language processing (NLP) named entity recognition system.",
            C: "Develop an anomaly detection system.",
            D: "Create a fraud forecasting system.",
          },
          correct_answer: "C",
          explanation:
            "An anomaly detection system can identify suspicious behavior, such as IP addresses that deviate from expected patterns, which helps in protecting the application from threats. The other options are not designed for detecting suspicious IP addresses.",
          id: 21,
        },
        {
          number: "21",
          question:
            "Which feature of Amazon OpenSearch Service gives companies the ability to build vector database applications?",
          options: {
            A: "Integration with Amazon S3 for object storage",
            B: "Support for geospatial indexing and queries",
            C: "Scalable index management and nearest neighbor search capability",
            D: "Ability to perform real-time analysis on streaming data",
          },
          correct_answer: "C",
          explanation:
            "The scalable index management and nearest neighbor search capability in Amazon OpenSearch Service enables companies to build vector database applications, which are crucial for tasks like similarity search in AI models. The other options do not specifically provide the vector search functionality.",
          id: 22,
        },
        {
          number: "22",
          question: "Which option is a use case for generative AI models?",
          options: {
            A: "Improving network security by using intrusion detection systems",
            B: "Creating photorealistic images from text descriptions for digital marketing",
            C: "Enhancing database performance by using optimized indexing",
            D: "Analyzing financial data to forecast stock market trends",
          },
          correct_answer: "B",
          explanation:
            "Generative AI models are used to create new content, such as photorealistic images from text descriptions, which is useful for digital marketing. The other options involve tasks better suited for analytical or detection systems rather than generative models.",
          id: 23,
        },
        {
          number: "23",
          question:
            "A company wants to build a generative AI application by using Amazon Bedrock and needs to choose a foundation model (FM). The company wants to know how much information can fit into one prompt.   Which consideration will inform the company's decision?  ",
          options: {
            A: "Temperature",
            B: "Context window",
            C: "Batch size",
            D: "Model size",
          },
          correct_answer: "B",
          explanation:
            "The context window determines how much information can fit into a single prompt. It specifies the number of tokens the foundation model can process at once, affecting the length of input that can be provided. The other options do not directly relate to prompt size.",
          id: 24,
        },
        {
          number: "24",
          question:
            "A company wants to make a chatbot to help customers. The chatbot will help solve technical problems without human intervention. The company chose a foundation model (FM) for the chatbot. The chatbot needs to produce responses that adhere to company tone. Which solution meets these requirements?",
          options: {
            A: "Set a low limit on the number of tokens the FM can produce.",
            B: "Use batch inferencing to process detailed responses.",
            C: "Experiment and refine the prompt until the FM produces the desired responses.",
            D: "Define a higher number for the temperature parameter.",
          },
          correct_answer: "C",
          explanation:
            "Experimenting and refining the prompt allows you to guide the FM to produce responses that align with the company's desired tone. This approach helps to shape the behavior of the chatbot. The other options do not directly ensure adherence to company tone.",
          id: 25,
        },
        {
          number: "25",
          question:
            "A company wants to use a large language model (LLM) on Amazon Bedrock for sentiment analysis. The company wants to classify the sentiment of text passages as positive or negative. Which prompt engineering strategy meets these requirements?",
          options: {
            A: "Provide examples of text passages with corresponding positive or negative labels in the prompt followed by the new text passage to be classified.",
            B: "Provide a detailed explanation of sentiment analysis and how LLMs work in the prompt.",
            C: "Provide the new text passage to be classified without any additional context or examples.",
            D: "Provide the new text passage with a few examples of unrelated tasks, such as text summarization or question answering.",
          },
          correct_answer: "A",
          explanation:
            "Providing examples with labels in the prompt helps the LLM understand the context of sentiment analysis, improving its accuracy in classifying the new text passage as positive or negative. The other options do not effectively guide the LLM for sentiment analysis.",
          id: 26,
        },
        {
          number: "26",
          question:
            "A security company is using Amazon Bedrock to run foundation models (FMs). The company wants to ensure that only authorized users invoke the models. The company needs to identify any unauthorized access attempts to set appropriate AWS Identity and Access Management (IAM) policies and roles for future iterations of the FMs.   Which AWS service should the company use to identify unauthorized users that are trying to access Amazon Bedrock?",
          options: {
            A: "AWS Audit Manager",
            B: "AWS CloudTrail",
            C: "Amazon Fraud Detector",
            D: "AWS Trusted Advisor",
          },
          correct_answer: "B",
          explanation:
            "AWS CloudTrail records API activity and provides a log of access attempts, which helps identify unauthorized users trying to access Amazon Bedrock. The other services are not specifically used for tracking unauthorized access attempts in this context.",
          id: 27,
        },
        {
          number: "27",
          question:
            "A company has developed an ML model for image classification. The company wants to deploy the model to production so that a web application can use the model. The company needs to implement a solution to host the model and serve predictions without managing any of the underlying infrastructure. Which solution will meet these requirements?",
          options: {
            A: "Use Amazon SageMaker Serverless Inference to deploy the model.",
            B: "Use Amazon CloudFront to deploy the model.",
            C: "Use Amazon API Gateway to host the model and serve predictions.",
            D: "Use AWS Batch to host the model and serve predictions.",
          },
          correct_answer: "A",
          explanation:
            "Amazon SageMaker Serverless Inference allows the company to deploy the ML model without managing any underlying infrastructure, making it suitable for hosting the model and serving predictions. The other options do not directly provide serverless model deployment capabilities.",
          id: 28,
        },
        {
          number: "28",
          question:
            "An AI company periodically evaluates its systems and processes with the help of independent software vendors (ISVs). The company needs to receive email message notifications when an ISV's compliance reports become available. Which AWS service can the company use to meet this requirement?",
          options: {
            A: "AWS Audit Manager",
            B: "AWS Artifact",
            C: "AWS Trusted Advisor",
            D: "AWS Data Exchange",
          },
          correct_answer: "B",
          explanation:
            "AWS Artifact provides access to compliance reports, including those from independent software vendors (ISVs). The company can use AWS Artifact to receive notifications when new compliance reports are available. The other services are not used for accessing and notifying about compliance reports.",
          id: 29,
        },
        {
          number: "29",
          question:
            "A company wants to use a large language model (LLM) to develop a conversational agent. The company needs to prevent the LLM from being manipulated with common prompt engineering techniques to perform undesirable actions or expose sensitive information. Which action will reduce these risks?",
          options: {
            A: "Create a prompt template that teaches the LLM to detect attack patterns.",
            B: "Increase the temperature parameter on invocation requests to the LLM.",
            C: "Avoid using LLMs that are not listed in Amazon SageMaker.",
            D: "Decrease the number of input tokens on invocations of the LLM.",
          },
          correct_answer: "A",
          explanation:
            "Creating a prompt template that helps the LLM detect common attack patterns can reduce the risk of prompt injection and other undesirable manipulations. The other options do not effectively address the risk of prompt manipulation or unauthorized use.",
          id: 30,
        },
        {
          number: "30",
          question:
            "A company is using the Generative AI Security Scoping Matrix to assess security responsibilities for its solutions. The company has identified four different solution scopes based on the matrix. Which solution scope gives the company the MOST ownership of security responsibilities?",
          options: {
            A: "Using a third-party enterprise application that has embedded generative AI features.",
            B: "Building an application by using an existing third-party generative AI foundation model (FM).",
            C: "Refining an existing third-party generative AI foundation model (FM) by fine-tuning the model by using data specific to the business.",
            D: "Building and training a generative AI model from scratch by using specific data that a customer owns.",
          },
          correct_answer: "D",
          explanation:
            "Building and training a generative AI model from scratch gives the company the most ownership of security responsibilities, as it involves full control over data, training, deployment, and security measures. The other options involve varying levels of dependency on third-party tools and services, which reduces the company's ownership of security.",
          id: 31,
        },
        {
          number: "31",
          question:
            "An AI practitioner has a database of animal photos. The AI practitioner wants to automatically identify and categorize the animals in the photos without manual human effort. Which strategy meets these requirements?",
          options: {
            A: "Object detection",
            B: "Anomaly detection",
            C: "Named entity recognition",
            D: "Inpainting",
          },
          correct_answer: "A",
          explanation:
            "Object detection is used to automatically identify and categorize objects (in this case, animals) in photos. It can detect the presence of animals and classify them accordingly. The other strategies are not suitable for identifying and categorizing animals in images.",
          id: 32,
        },
        {
          number: "32",
          question:
            "A company wants to create an application by using Amazon Bedrock. The company has a limited budget and prefers flexibility without long-term commitment. Which Amazon Bedrock pricing model meets these requirements?",
          options: {
            A: "On-Demand",
            B: "Model customization",
            C: "Provisioned Throughput",
            D: "Spot Instance",
          },
          correct_answer: "A",
          explanation:
            "The On-Demand pricing model provides flexibility without requiring a long-term commitment, allowing the company to pay only for the resources used, which fits well with a limited budget. The other options are either not relevant to pricing flexibility or involve specific resource commitments.",
          id: 33,
        },
        {
          number: "33",
          question:
            "Which AWS service or feature can help an AI development team quickly deploy and consume a foundation model (FM) within the team's VPC?",
          options: {
            A: "Amazon Personalize",
            B: "Amazon SageMaker JumpStart",
            C: "PartyRock, an Amazon Bedrock Playground",
            D: "Amazon SageMaker endpoints",
          },
          correct_answer: "B",
          explanation:
            "Amazon SageMaker JumpStart provides pre-built models, including foundation models, that can be quickly deployed and consumed within a VPC, helping teams get started faster. The other options are not designed for deploying foundation models in this context.",
          id: 34,
        },
        {
          number: "34",
          question:
            "How can companies use large language models (LLMs) securely on Amazon Bedrock?",
          options: {
            A: "Configure AWS Identity and Access Management (IAM) roles and policies by using least privilege",
            B: "Enable AWS Audit Manager for automatic model evaluation jobs.",
            C: "Enable Amazon Bedrock automatic model evaluation jobs.",
            D: "Use Amazon CloudWatch Logs to make models explainable and to monitor for bias.",
          },
          correct_answer: "A",
          explanation:
            "Designing clear prompts using IAM roles with least privilege access ensures secure LLMs on Amazon Bedrock by minimizing directly address securing the use of LLMs.",
          id: 35,
        },
        {
          number: "35",
          question:
            "A company has terabytes of data in a database that the company can use for business analysis. The company wants to build an AI-based application that can build a SQL query from input text that employees provide. The employees have minimal experience with technology. Which solution meets these requirements?",
          options: {
            A: "Generative pre-trained transformers (GPT)",
            B: "Residual neural network",
            C: "Support vector machine",
            D: "WaveNet",
          },
          correct_answer: "A",
          explanation:
            "GPT models are well-suited for converting natural language input into structured queries like SQL, making them ideal for building an AI-based application that translates employee-provided text into SQL queries. The other options are not designed for natural language understanding and query generation tasks.",
          id: 36,
        },
        {
          number: "36",
          question:
            "A company built a deep learning model for object detection and deployed the model to production. Which AI process occurs when the model analyzes a new image to identify objects?",
          options: {
            A: "Training",
            B: "Inference",
            C: "Model deployment",
            D: "Bias correction",
          },
          correct_answer: "B",
          explanation:
            "Inference is the process where the model analyzes new data (in this case, a new image) to make predictions or identify objects. The other options are related to different stages of the AI lifecycle, such as building or preparing the model.",
          id: 37,
        },
        {
          number: "37",
          question:
            "An AI practitioner is building a model to generate images of humans in various professions. The AI practitioner discovered that the input data is biased and that specific attributes affect the image generation and create bias in the model. Which technique will solve the problem?",
          options: {
            A: "Data augmentation for imbalanced classes",
            B: "Model monitoring for class distribution",
            C: "Retrieval Augmented Generation (RAG)",
            D: "Watermark detection for images",
          },
          correct_answer: "A",
          explanation:
            "Data augmentation forimbalanced classes helps address bias by creating a more balanced dataset, ensuring that different attributes The other options do not directly address data",
          id: 38,
        },
        {
          number: "38",
          question:
            "A company is using an Amazon Titan foundation model (FM) in Amazon Bedrock. The company needs to supplement the model by using relevant data from the company's private data sources. Which solution will meet this requirement?",
          options: {
            A: "Use a different FM.",
            B: "Choose a lower temperature value.",
            C: "Create an Amazon Bedrock knowledge base.",
            D: "Enable model invocation logging.",
          },
          correct_answer: "C",
          explanation:
            "Creating an Amazon Bedrock knowledge base allows the company to supplement the foundation model with relevant data from their private data sources. This ensures that the model has access to the additional, context-specific information needed. The other options do not directly address supplementing the model with private data.",
          id: 39,
        },
        {
          number: "39",
          question:
            "A medical company is customizing a foundation model (FM) for diagnostic purposes. The company needs the model to be transparent and explainable to meet regulatory requirements. Which solution will meet these requirements?",
          options: {
            A: "Configure the security and compliance by using Amazon Inspector.",
            B: "Generate simple metrics, reports, and examples by using Amazon SageMaker Clarify.",
            C: "Encrypt and secure training data by using Amazon Macie.",
            D: "Gather more data. Use Amazon Rekognition to add custom labels to the data.",
          },
          correct_answer: "B",
          explanation:
            "Amazon SageMaker Clarify helps with transparency and explainability by generating metrics, reports, and examples that show how the model makes decisions, which is essential for meeting regulatory requirements. The other options are not directly related to improving the model's transparency or explainability.",
          id: 40,
        },
        {
          number: "40",
          question:
            "A company wants to deploy a conversational chatbot to answer customer questions. The chatbot is based on a fine-tuned Amazon SageMaker JumpStart model. The application must comply with multiple regulatory frameworks. Which capabilities can the company show compliance for? (Choose two.)",
          options: {
            A: "Auto scaling inference endpoints",
            B: "Threat detection",
            C: "Data protection",
            D: "Cost optimization",
            E: "Loosely coupled microservices",
          },
          correct_answer: "BC",
          explanation:
            "Threat detection: Ensuring security measures are in place to detect threats is important for compliance with regulatory frameworks. Data protection: Proper data handling and protection measures are key compliance aspects, especially in applications dealing with sensitive customer information. The other options (auto scaling, cost optimization, and loosely coupled microservices) are more related to performance and architecture rather than regulatory compliance.",
          id: 41,
        },
        {
          number: "41",
          question:
            "A company is training a foundation model (FM). The company wants to increase the accuracy of the model up to a specific acceptance level. Which solution will meet these requirements?",
          options: {
            A: "Decrease the batch size.",
            B: "Increase the epochs.",
            C: "Decrease the epochs.",
            D: "Increase the temperature parameter.",
          },
          correct_answer: "B",
          explanation:
            "Increasing the number of epochs allows the model to train for more iterations, improving its accuracy until the model reaches an optimal level. The other options are either less effective or unrelated to improving accuracy.",
          id: 42,
        },
        {
          number: "42",
          question:
            "A company is building a large language model (LLM) question answering chatbot. The company wants to decrease the number of actions call center employees need to take to respond to customer questions. Which business objective should the company use to evaluate the effect of the LLM chatbot?",
          options: {
            A: "Website engagement rate",
            B: "Average call duration",
            C: "Corporate social responsibility",
            D: "Regulatory compliance",
          },
          correct_answer: "B",
          explanation:
            "Reducing the average call duration directly indicates how effectively the LLM chatbot is helping call center employees answer customer questions, thus reducing the number of actions needed. The other options are not directly related to the performance of a call center chatbot.",
          id: 43,
        },
        {
          number: "43",
          question:
            "Which functionality does Amazon SageMaker Clarify provide?",
          options: {
            A: "Integrates a Retrieval Augmented Generation (RAG) workflow",
            B: "Monitors the quality of ML models in production",
            C: "Documents critical details about ML models",
            D: "Identifies potential bias during data preparation",
          },
          correct_answer: "D",
          explanation:
            "Amazon SageMaker Clarify helps detect potential bias in datasets and models during data preparation, training, and deployment. It also provides tools for explainability. The other options are functionalities that do not directly match SageMaker Clarify's core features.",
          id: 44,
        },
        {
          number: "44",
          question:
            "A company is developing a new model to predict the prices of specific items. The model performed well on the training dataset. When the company deployed the model to production, the model's performance decreased significantly. What should the company do to mitigate this problem?",
          options: {
            A: "Reduce the volume of data that is used in training.",
            B: "Add hyperparameters to the model.",
            C: "Increase the volume of data that is used in training.",
            D: "Increase the model training time.",
          },
          correct_answer: "C",
          explanation:
            "Increasing the volume of data used in training helps the model generalize better to new, unseen data, reducing overfitting and improving performance in production. The other options either do not address the issue of model generalization or are unlikely to effectively solve the problem.",
          id: 45,
        },
        {
          number: "45",
          question:
            "An ecommerce company wants to build a solution to determine customer sentiments based on written customer reviews of products. Which AWS services meet these requirements? (Choose two.)",
          options: {
            A: "Amazon Lex",
            B: "Amazon Comprehend",
            C: "Amazon Polly",
            D: "Amazon Bedrock",
            E: "Amazon Rekognition",
          },
          correct_answer: "BD",
          explanation:
            "Amazon Comprehend: This service is specifically designed for natural language processing (NLP) tasks, including sentiment analysis, making it ideal for analyzing customer reviews. Amazon Bedrock: Bedrock can be used to leverage foundation models, which can also be employed for sentiment analysis tasks. The other options are not suitable for sentiment analysis of written customer reviews.",
          id: 46,
        },
        {
          number: "46",
          question:
            "A company wants to use large language models (LLMs) with Amazon Bedrock to develop a chat interface for the company's product manuals. The manuals are stored as PDF files.  Which solution meets these requirements MOST cost-effectively? ",
          options: {
            A: "Use prompt engineering to add one PDF file as context to the user prompt when the prompt is submitted to Amazon Bedrock.",
            B: "Use prompt engineering to add all the PDF files as context to the user prompt when the prompt is submitted to Amazon Bedrock.",
            C: "Use all the PDF documents to fine-tune a model with Amazon Bedrock. Use the fine-tuned model to process user prompts.",
            D: "Upload PDF documents to an Amazon Bedrock knowledge base. Use the knowledge base to provide context when users submit prompts to Amazon Bedrock",
          },
          correct_answer: "D",
          explanation:
            "Using an Amazon Bedrock knowledge base allows the model to efficiently access relevant information from the PDF manuals when needed, reducing the cost compared to continuously fine-tuning a model or providing all PDFs as context in each prompt. This approach ensures that only necessary context is provided, making it cost-effective.",
          id: 47,
        },
        {
          number: "47",
          question:
            "A social media company wants to use a large language model (LLM) for content moderation. The company wants to evaluate the LLM outputs for bias and potential discrimination against specific groups or individuals. Which data source should the company use to evaluate the LLM outputs with the LEAST administrative effort?",
          options: {
            A: "User-generated content",
            B: "Moderation logs",
            C: "Content moderation guidelines",
            D: "Benchmark datasets",
          },
          correct_answer: "D",
          explanation:
            "Benchmark datasets are standardized datasets specifically designed for evaluating models for bias and fairness, allowing for efficient assessment with minimal administrative effort. The other options would require more manual processing and might not provide a consistent basis for evaluating bias and discrimination.",
          id: 48,
        },
        {
          number: "48",
          question:
            "A company wants to use a pre-trained generative AI model to generate content for its marketing campaigns. The company needs to ensure that the generated content aligns with the company's brand voice and messaging requirements. Which solution meets these requirements?",
          options: {
            A: "Optimize the model's architecture and hyperparameters to improve the model's overall performance.",
            B: "Increase the model's complexity by adding more layers to the model's architecture.",
            C: "Create effective prompts that provide clear instructions and context to guide the model's generation.",
            D: "Select a large, diverse dataset to pre-train a new generative model.",
          },
          correct_answer: "C",
          explanation:
            "Creating effective prompts helps guide the pre-trained generative AI model to produce content that aligns with the company's brand voice and messaging. The other options either involve model architecture changes or require extensive training, which are not necessary for aligning content generation.",
          id: 49,
        },
        {
          number: "49",
          question:
            "A loan company is building a generative AI-based solution to offer new applicants discounts based on specific business criteria. The company wants to build and use an AI model responsibly to minimize bias that could negatively affect some customers.     Which actions should the company take to meet these requirements? (Choose two.)",
          options: {
            A: "Detect imbalances or disparities in the data.",
            B: "Ensure that the model runs frequently.",
            C: "Evaluate the model's behavior so that the company can provide transparency to stakeholders.",
            D: "Use the Recall-Oriented Understudy for Gisting Evaluation (ROUGE) technique to ensure that the model",
            E: "Ensure that the model's inference time is within the accepted limits.",
          },
          correct_answer: "AC",
          explanation:
            "Detect imbalances or disparities in the data: Identifying and addressing data imbalances helps minimize biases that could negatively affect customers. Evaluate the model's behavior so that the company can provide transparency to stakeholders: Evaluating the model and ensuring transparency is important for responsible AI usage, as it helps stakeholders understand how decisions are made. The other options are either not directly related to minimizing bias or do not address responsible AI development.",
          id: 50,
        },
        {
          number: "50",
          question:
            "A company is using an Amazon Bedrock base model to summarize documents for an internal use case. The company trained a custom model to improve the summarization quality. Which action must the company take to use the custom model through Amazon Bedrock?",
          options: {
            A: "Purchase Provisioned Throughput for the custom model.",
            B: "Deploy the custom model in an Amazon SageMaker endpoint for real-time inference.",
            C: "Register the model with the Amazon SageMaker Model Registry.",
            D: "Grant access to the custom model in Amazon Bedrock.",
          },
          correct_answer: "A",
          explanation: "",
          id: 51,
        },
        {
          number: "51",
          question:
            "A company needs to choose a model from Amazon Bedrock to use internally. The company must identify a model that generates responses in a style that the company's employees prefer. What should the company do to meet these requirements?",
          options: {
            A: "Evaluate the models by using built-in prompt datasets.",
            B: "Evaluate the models by using a human workforce and custom prompt datasets.",
            C: "Use public model leaderboards to identify the model.",
            D: "Use the model InvocationLatency runtime metrics in Amazon CloudWatch when trying models.",
          },
          correct_answer: "B",
          explanation:
            "Evaluating models using a human workforce and custom prompt datasets ensures that the model generates responses in the style that aligns with the company’s preferences. The other options either do not provide direct feedback on style preferences or are not specific enough for determining suitability based on employee preferences.",
          id: 52,
        },
        {
          number: "52",
          question:
            "A student at a university is copying content from generative AI to write essays. Which challenge of responsible generative AI does this scenario represent?",
          options: {
            A: "Toxicity",
            B: "Hallucinations",
            C: "Plagiarism",
            D: "Privacy",
          },
          correct_answer: "C",
          explanation:
            "Copying content from generative AI to write essays without proper attribution constitutes plagiarism, which is a key challenge of responsible generative AI. The other options are unrelated to this specific issue.",
          id: 53,
        },
        {
          number: "53",
          question:
            "A company needs to build its own large language model (LLM) based on only the company's private data. The company is concerned about the environmental effect of the training process. Which Amazon EC2 instance type has the LEAST environmental effect when training LLMs?",
          options: {
            A: "Amazon EC2 C series",
            B: "Amazon EC2 G series",
            C: "Amazon EC2 P series",
            D: "Amazon EC2 Trn series",
          },
          correct_answer: "D",
          explanation:
            "Amazon EC2 Trn series instances (powered by AWS Trainium chips) are designed to provide efficient and environmentally friendly training of large machine learning models. They are optimized for energy efficiency, which reduces the environmental impact of the training process. The other instance types are not specifically optimized for minimizing environmental effects during training.",
          id: 54,
        },
        {
          number: "54",
          question:
            "A company wants to build an interactive application for children that generates new stories based on classic stories. The company wants to use Amazon Bedrock and needs to ensure that the results and topics are appropriate for children. Which AWS service or feature will meet these requirements?",
          options: {
            A: "Amazon Rekognition",
            B: "Amazon Bedrock playgrounds",
            C: "Guardrails for Amazon Bedrock",
            D: "Agents for Amazon Bedrock",
          },
          correct_answer: "C",
          explanation:
            "Guardrails for Amazon Bedrock can help ensure that the output generated by Amazon Bedrock is appropriate for children. Guardrails are used to apply content moderation, guidelines, and ensure safety by filtering potentially harmful or inappropriate content, which is essential when building an interactive application for children.",
          id: 55,
        },
        {
          number: "55",
          question:
            "A company is building an application that needs to generate synthetic data that is based on existing data. Which type of model can the company use to meet this requirement?",
          options: {
            A: "Generative adversarial network (GAN)",
            B: "XGBoost",
            C: "Residual neural network",
            D: "WaveNet",
          },
          correct_answer: "A",
          explanation: "",
          id: 56,
        },
        {
          number: "56",
          question:
            "A digital devices company wants to predict customer demand for memory hardware. The company does not have coding experience or knowledge of ML algorithms and needs to develop a data-driven predictive model. The company needs to perform analysis on internal data and external data. Which solution will meet these requirements?",
          options: {
            A: "Store the data in Amazon S3. Create ML models and demand forecast predictions by using Amazon",
            B: "Import the data into Amazon SageMaker Data Wrangler. Create ML models and demand forecast",
            C: "Import the data into Amazon SageMaker Data Wrangler. Build ML models and demand forecast",
            D: "Import the data into Amazon SageMaker Canvas. Build ML models and demand forecast predictions by",
          },
          correct_answer: "D",
          explanation:
            "Amazon SageMaker Canvas is a no-code tool that allows users to build ML models and make predictions without requiring programming knowledge. It is ideal for users with no coding experience, providing an easy interface for importing data and generating predictive models. The other options require more technical expertise or are not designed for no-code model building.",
          id: 57,
        },
        {
          number: "57",
          question:
            "A company has installed a security camera. The company uses an ML model to evaluate the security camera footage for potential thefts. The company has discovered that the model disproportionately flags people who are members of a specific ethnic group. Which type of bias is affecting the model output?",
          options: {
            A: "Measurement bias",
            B: "Sampling bias",
            C: "Observer bias",
            D: "Confirmation bias",
          },
          correct_answer: "B",
          explanation:
            "Sampling bias occurs when the training data is not representative of the overall population, leading to disproportionate flagging of specific groups. In this case, the model may have been trained on biased data that did not adequately represent all ethnic groups, resulting in skewed predictions. The other types of bias do not directly apply to the selection of training data or its representativeness.",
          id: 58,
        },
        {
          number: "58",
          question:
            "A company is building a customer service chatbot. The company wants the chatbot to improve its responses by learning from past interactions and online resources. Which AI learning strategy provides this self-improvement capability?",
          options: {
            A: "Supervised learning with a manually curated dataset of good responses and bad responses",
            B: "Reinforcement learning with rewards for positive customer feedback",
            C: "Unsupervised learning to find clusters of similar customer inquiries",
            D: "Supervised learning with a continuously updated FAQ database",
          },
          correct_answer: "B",
          explanation:
            "Reinforcement learning allows the chatbot to learn from interactions by receiving rewards for positive customer feedback, which helps the model self-improve over time. The other options do not directly provide a mechanism for continuous self-improvement based on interactions.",
          id: 59,
        },
        {
          number: "59",
          question:
            "An AI practitioner has built a deep learning model to classify the types of materials in images. The AI practitioner now wants to measure the model performance. Which metric will help the AI practitioner evaluate the performance of the model?",
          options: {
            A: "Confusion matrix",
            B: "Correlation matrix",
            C: "R2 score",
            D: "Mean squared error (MSE)",
          },
          correct_answer: "A",
          explanation:
            "A confusion matrix provides detailed insights into the performance of a classification model by showing the true positives, false positives, true negatives, and false negatives. This metric helps evaluate how well the model classifies the different types of materials in images. The other metrics are not as suitable for evaluating a classification model.",
          id: 60,
        },
        {
          number: "60",
          question:
            "A company has built a chatbot that can respond to natural language questions with images. The company wants to ensure that the chatbot does not return inappropriate or unwanted images. Which solution will meet these requirements?",
          options: {
            A: "Implement moderation APIs.",
            B: "Retrain the model with a general public dataset.",
            C: "Perform model validation.",
            D: "Automate user feedback integration.",
          },
          correct_answer: "A",
          explanation:
            "Implementing moderation APIs can help filter and block inappropriate or unwanted images before they are returned by the chatbot. The other options do not directly address ensuring that the chatbot avoids returning inappropriate images.",
          id: 61,
        },
        {
          number: "61",
          question:
            "An AI practitioner is using an Amazon Bedrock base model to summarize session chats from the customer service department. The AI practitioner wants to store invocation logs to monitor model input and output data. Which strategy should the AI practitioner use?",
          options: {
            A: "Configure AWS CloudTrail as the logs destination for the model.",
            B: "Enable model invocation logging in Amazon Bedrock.",
            C: "Configure AWS Audit Manager as the logs destination for the model.",
            D: "Configure model invocation logging in Amazon EventBridge.",
          },
          correct_answer: "B",
          explanation:
            "Enabling invocation logging in Amazon Bedrock allows the AI practitioner to monitor and store the input and output data for model invocations. The other options are not directly used for logging model invocations in Amazon Bedrock.",
          id: 62,
        },
        {
          number: "62",
          question:
            "A company is building an ML model to analyze archived data. The company must perform inference on large datasets that are multiple GBs in size. The company does not need to access the model predictions immediately. Which Amazon SageMaker inference option will meet these requirements?",
          options: {
            A: "Batch transform",
            B: "Real-time inference",
            C: "Serverless inference",
            D: "Asynchronous inference",
          },
          correct_answer: "A",
          explanation:
            "Batch transform is ideal for processing large datasets that do not require real-time predictions. It allows the company to perform inference on multiple GBs of data efficiently without needing immediate results. The other options are more suitable for scenarios requiring real-time or near real-time access.",
          id: 63,
        },
        {
          number: "63",
          question:
            "Which term describes the numerical representations of real-world objects and concepts that AI and natural language processing (NLP) models use to improve understanding of textual information?",
          options: {
            A: "Embeddings",
            B: "Tokens",
            C: "Models",
            D: "Binaries",
          },
          correct_answer: "A",
          explanation:
            "Embeddings are numerical representations of real-world objects and concepts that help AI and NLP models understand and work with textual information more effectively by capturing relationships and similarities between words or phrases. The other options do not describe this concept.",
          id: 64,
        },
        {
          number: "64",
          question:
            "A research company implemented a chatbot by using a foundation model (FM) from Amazon Bedrock. The chatbot searches for answers to questions from a large database of research papers. After multiple prompt engineering attempts, the company notices that the FM is performing poorly because of the complex scientific terms in the research papers. How can the company improve the performance of the chatbot?",
          options: {
            A: "Use few-shot prompting to define how the FM can answer the questions.",
            B: "Use domain adaptation fine-tuning to adapt the FM to complex scientific terms.",
            C: "Change the FM inference parameters.",
            D: "Clean the research paper data to remove complex scientific terms.",
          },
          correct_answer: "B",
          explanation:
            "Domain adaptation fine-tuning allows the FM to better understand the complex scientific terms by training it with domain-specific data, improving its performance on such specialized content. The other options are either insufficient or not directly related to handling complex terminology effectively.",
          id: 65,
        },
        {
          number: "65",
          question:
            "A company wants to use a large language model (LLM) on Amazon Bedrock for sentiment analysis. The company needs the LLM to produce more consistent responses to the same input prompt. Which adjustment to an inference parameter should the company make to meet these requirements?",
          options: {
            A: "Decrease the temperature value.",
            B: "Increase the temperature value.",
            C: "Decrease the length of output tokens.",
            D: "Increase the maximum generation length.",
          },
          correct_answer: "A",
          explanation:
            "Decreasing the temperature value makes the model's output more deterministic and consistent by reducing randomness in response generation. The other adjustments do not directly ensure consistent responses.",
          id: 66,
        },
        {
          number: "66",
          question:
            "A company wants to develop a large language model (LLM) application by using Amazon Bedrock and customer data that is uploaded to Amazon S3. The company's security policy states that each team can access data for only the team's own customers. Which solution will meet these requirements?",
          options: {
            A: "Create an Amazon Bedrock custom service role for each team that has access to only the team's customer data.",
            B: "Create a custom service role that has Amazon S3 access. Ask teams to specify the customer name on each Amazon Bedrock request.",
            C: "Redact personal data in Amazon S3. Update the S3 bucket policy to allow team access to customer data.",
            D: "Create one Amazon Bedrock role that has full Amazon S3 access. Create IAM roles for each team that have access to only each team's customer folders.",
          },
          correct_answer: "A",
          explanation:
            "Creating a custom Amazon Bedrock service role for each team with restricted access to only the team's customer data ensures compliance with the security policy, providing the necessary data segregation and access control. The other options do not effectively enforce team-specific access or may pose risks of broader access than allowed by the policy.",
          id: 67,
        },
        {
          number: "67",
          question:
            "A medical company deployed a disease detection model on Amazon Bedrock. To comply with privacy policies, the company wants to prevent the model from including personal patient information in its responses. The company also wants to receive notification when policy violations occur. Which solution meets these requirements?",
          options: {
            A: "Use Amazon Macie to scan the model's output for sensitive data and set up alerts for potential",
            B: "Configure AWS CloudTrail to monitor the model's responses and create alerts for any detected personal",
            C: "Use Guardrails for Amazon Bedrock to filter content. Set up Amazon CloudWatch alarms for notification",
            D: "Implement Amazon SageMaker Model Monitor to detect data drift and receive alerts when model quality",
          },
          correct_answer: "C",
          explanation:
            "Guardrails for Amazon Bedrock can be used to filter content and ensure that personal patient information is not included in model responses. Setting up Amazon CloudWatch alarms allows the company to receive notifications when policy violations occur. The other options are not specifically designed for filtering model output and monitoring policy compliance.",
          id: 68,
        },
        {
          number: "68",
          question:
            "A company manually reviews all submitted resumes in PDF format. As the company grows, the company expects the volume of resumes to exceed the company's review capacity. The company needs an automated system to convert the PDF resumes into plain text format for additional processing. Which AWS service meets this requirement?",
          options: {
            A: "Amazon Textract",
            B: "Amazon Personalize",
            C: "Amazon Lex",
            D: "Amazon Transcribe",
          },
          correct_answer: "A",
          explanation:
            "Amazon Textract can extract text from PDF documents, making it suitable for converting resumes into plain text for further processing. The other services do not provide functionality to extract text from PDFs.",
          id: 69,
        },
        {
          number: "69",
          question:
            "An education provider is building a question and answer application that uses a generative AI model to explain complex concepts. The education provider wants to automatically change the style of the model response depending on who is asking the question. The education provider will give the model the age range of the user who has asked the question.     Which solution meets these requirements with the LEAST implementation effort?",
          options: {
            A: "Fine-tune the model by using additional training data that is representative of the various age ranges",
            B: "Add a role description to the prompt context that instructs the model of the age range that the response",
            C: "Use chain-of-thought reasoning to deduce the correct style and complexity for a response suitable for",
            D: "Summarize the response text depending on the age of the user so that younger users receive shorter",
          },
          correct_answer: "B",
          explanation:
            "Adding a role description to the prompt is the simplest and most effective way to adjust the model's response style based on the user's age range. It requires minimal implementation effort and effectively tailors the output. The other options involve more complex processes, such as fine-tuning or additional reasoning steps.",
          id: 70,
        },
        {
          number: "70",
          question:
            "Which strategy evaluates the accuracy of a foundation model (FM) that is used in image classification tasks?",
          options: {
            A: "Calculate the total cost of resources used by the model.",
            B: "Measure the model's accuracy against a predefined benchmark dataset.",
            C: "Count the number of layers in the neural network.",
            D: "Assess the color accuracy of images processed by the model.",
          },
          correct_answer: "B",
          explanation:
            "Evaluating a foundation model's accuracy by measuring its performance against a predefined benchmark dataset is the standard approach for assessing accuracy in image classification tasks. The other options do not provide an appropriate measure of classification accuracy.",
          id: 71,
        },
        {
          number: "71",
          question:
            "An accounting firm wants to implement a large language model (LLM) to automate document processing. The firm must proceed responsibly to avoid potential harms. What should the firm do when developing and deploying the LLM? (Choose two.)",
          options: {
            A: "Include fairness metrics for model evaluation.",
            B: "Adjust the temperature parameter of the model.",
            C: "Modify the training data to mitigate bias.",
            D: "Avoid overfitting on the training data.",
            E: "Apply prompt engineering techniques.",
          },
          correct_answer: "AC",
          explanation:
            "Include fairness metrics for model evaluation: Fairness metrics help ensure that the LLM is unbiased and treats all cases equitably, which is essential for responsible AI use. Modify the training data to mitigate bias: Adjusting the training data helps reduce any inherent bias that might exist, contributing to a more fair and responsible LLM. The other options are related to general model optimization but do not directly address responsible AI practices regarding potential harms like bias and fairness.",
          id: 72,
        },
        {
          number: "72",
          question:
            "A company is building an ML model. The company collected new data and analyzed the data by creating a correlation matrix, calculating statistics, and visualizing the data. Which stage of the ML pipeline is the company currently in?",
          options: {
            A: "Data pre-processing",
            B: "Feature engineering",
            C: "Exploratory data analysis",
            D: "Hyperparameter tuning",
          },
          correct_answer: "C",
          explanation:
            "The company is currently in the exploratory data analysis (EDA) stage, which involves summarizing data through statistics, visualizations, and correlation matrices to understand the dataset before moving on to modeling. The other options are subsequent steps in the ML pipeline.",
          id: 73,
        },
        {
          number: "73",
          question:
            "A company has documents that are missing some words because of a database error. The company wants to build an ML model that can suggest potential words to fill in the missing text. Which type of model meets this requirement?",
          options: {
            A: "Topic modeling",
            B: "Clustering models",
            C: "Prescriptive ML models",
            D: "BERT-based models",
          },
          correct_answer: "D",
          explanation:
            "BERT-based models are well-suited for natural language understanding tasks, including filling in missing words, because they use contextual information to predict missing tokens in a text. The other types of models are not designed for this type of text completion task.",
          id: 74,
        },
        {
          number: "74",
          question:
            "A company wants to display the total sales for its top-selling products across various retail locations in the past 12 months. Which AWS solution should the company use to automate the generation of graphs?",
          options: {
            A: "Amazon Q in Amazon EC2",
            B: "Amazon Q Developer",
            C: "Amazon Q in Amazon QuickSight",
            D: "Amazon Q in AWS Chatbot",
          },
          correct_answer: "C",
          explanation:
            "Amazon Q in Amazon QuickSight allows users to ask questions in natural language and automatically generate graphs and visualizations to display insights, such as total sales for top-selling products. The other options do not provide the same functionality for generating visual analytics.",
          id: 75,
        },
        {
          number: "75",
          question:
            "A company is building a chatbot to improve user experience. The company is using a large language model (LLM) from Amazon Bedrock for intent detection. The company wants to use few-shot learning to improve intent detection accuracy. Which additional data does the company need to meet these requirements?",
          options: {
            A: "Pairs of chatbot responses and correct user intents",
            B: "Pairs of user messages and correct chatbot responses",
            C: "Pairs of user messages and correct user intents",
            D: "Pairs of user intents and correct chatbot responses",
          },
          correct_answer: "C",
          explanation:
            "Few-shot learning involves providing the model with a few examples to help it understand how to perform the task. For intent detection, the company needs pairs of user messages and the correct user intents, which will help the LLM improve its accuracy in detecting user intents. The other options do not provide the necessary pairing for improving intent detection.",
          id: 76,
        },
        {
          number: "76",
          question:
            "A company is using few-shot prompting on a base model that is hosted on Amazon Bedrock. The model currently uses 10 examples in the prompt. The model is invoked once daily and is performing well. The company wants to lower the monthly cost. Which solution will meet these requirements?",
          options: {
            A: "Customize the model by using fine-tuning.",
            B: "Decrease the number of tokens in the prompt.",
            C: "Increase the number of tokens in the prompt.",
            D: "Use Provisioned Throughput.",
          },
          correct_answer: "B",
          explanation:
            "Decreasing the number of tokens in the prompt reduces the amount of data being processed, thereby lowering the cost of using the model. Since the model is performing well, reducing the prompt size is a costeffective way to maintain performance while lowering expenses. The other options either increase costs or are unrelated to prompt size.",
          id: 77,
        },
        {
          number: "77",
          question:
            "An AI practitioner is using a large language model (LLM) to create content for marketing campaigns. The generated content sounds plausible and factual but is incorrect. Which problem is the LLM having?",
          options: {
            A: "Data leakage",
            B: "Hallucination",
            C: "Overfitting",
            D: "Underfitting",
          },
          correct_answer: "B",
          explanation:
            "Hallucination occurs when a large language model generates content that appears plausible and factual but is incorrect or fabricated. This is a common issue with LLMs. The other options do not describe this particular behavior.",
          id: 78,
        },
        {
          number: "78",
          question:
            "An AI practitioner trained a custom model on Amazon Bedrock by using a training dataset that contains confidential data. The AI practitioner wants to ensure that the custom model does not generate inference responses based on confidential data. How should the AI practitioner prevent responses based on confidential data?",
          options: {
            A: "Delete the custom model. Remove the confidential data from the training dataset. Retrain the custom",
            B: "Mask the confidential data in the inference responses by using dynamic data masking.",
            C: "Encrypt the confidential data in the inference responses by using Amazon SageMaker.",
            D: "Encrypt the confidential data in the custom model by using AWS Key Management Service (AWS KMS).",
          },
          correct_answer: "A",
          explanation:
            "To ensure that the custom model does not generate responses based on confidential data, the best approach is to retrain the model without including the confidential data. This prevents the model from learning patterns associated with that sensitive information, thereby avoiding its use in inference. The other options do not address the root cause of the issue—removing confidential data from the training process.",
          id: 79,
        },
        {
          number: "79",
          question:
            "A company has built a solution by using generative AI. The solution uses large language models (LLMs) to translate training manuals from English into other languages. The company wants to evaluate the accuracy of the solution by examining the text generated for the manuals. Which model evaluation strategy meets these requirements?",
          options: {
            A: "Bilingual Evaluation Understudy (BLEU)",
            B: "Root mean squared error (RMSE)",
            C: "Recall-Oriented Understudy for Gisting Evaluation (ROUGE)",
            D: "F1 score",
          },
          correct_answer: "A",
          explanation:
            "The BLEU (Bilingual Evaluation Understudy) score is a common metric used to evaluate the accuracy of machine translation by comparing the generated translation with reference translations. It is specifically designed for translation tasks, whereas the other metrics are not suitable for evaluating translation quality.",
          id: 80,
        },
        {
          number: "80",
          question:
            "A large retailer receives thousands of customer support inquiries about products every day. The customer support inquiries need to be processed quickly. The company wants to implement Agents for Amazon Bedrock. What are the key benefits of using Amazon Bedrock agents that could help this retailer?",
          options: {
            A: "Generation of custom foundation models (FMs) to predict customer needs",
            B: "Automation of repetitive tasks and orchestration of complex workflows",
            C: "Automatically calling multiple foundation models (FMs) and consolidating the results",
            D: "Selecting the foundation model (FM) based on predefined criteria and metrics",
          },
          correct_answer: "B",
          explanation:
            "Amazon Bedrock agents help automate repetitive tasks and orchestrate complex workflows, which is ideal for handling thousands of customer support inquiries efficiently. This helps reduce response times and improves productivity. The other options do not directly address automation and orchestration of tasks for customer support.",
          id: 81,
        },
        {
          number: "81",
          question:
            "Which option is a benefit of ongoing pre-training when fine-tuning a foundation model (FM)?",
          options: {
            A: "Helps decrease the model's complexity",
            B: "Improves model performance over time",
            C: "Decreases the training time requirement",
            D: "Optimizes model inference time",
          },
          correct_answer: "B",
          explanation:
            "Ongoing pre-training helps enhance a foundation model's performance by continuously updating it with new data, thereby improving its ability to generalize and perform well on different tasks. The other options do not directly relate to the benefits of ongoing pre-training.",
          id: 82,
        },
        {
          number: "82",
          question: "What are tokens in the context of generative AI models?",
          options: {
            A: "Tokens are the basic units of input and output that a generative AI model operates on, representing words, subwords, or other linguistic units.",
            B: "Tokens are the mathematical representations of words or concepts used in generative AI models.",
            C: "Tokens are the pre-trained weights of a generative AI model that are fine-tuned for specific tasks.",
            D: "Tokens are the specific prompts or instructions given to a generative AI model to generate output.",
          },
          correct_answer: "A",
          explanation:
            "Tokens are the smallest units (e.g., words, subwords, or characters) that generative AI models use to process text. They form the basis of both the input given to and the output generated by the model. The other options do not accurately describe tokens in this context.",
          id: 83,
        },
        {
          number: "83",
          question:
            "A company wants to assess the costs that are associated with using a large language model (LLM) to generate inferences. The company wants to use Amazon Bedrock to build generative AI applications. Which factor will drive the inference costs?",
          options: {
            A: "Number of tokens consumed",
            B: "Temperature value",
            C: "Amount of data used to train the LLM",
            D: "Total training time",
          },
          correct_answer: "A",
          explanation:
            "Inference costs for large language models are typically driven by the number of tokens processed during input and output, as each token incurs computational resources. The other factors (temperature value, training data, and training time) do not directly impact inference costs.",
          id: 84,
        },
        {
          number: "84",
          question:
            "A company is using Amazon SageMaker Studio notebooks to build and train ML models. The company stores the data in an Amazon S3 bucket. The company needs to manage the flow of data from Amazon S3 to SageMaker Studio notebooks. Which solution will meet this requirement?",
          options: {
            A: "Use Amazon Inspector to monitor SageMaker Studio.",
            B: "Use Amazon Macie to monitor SageMaker Studio.",
            C: "Configure SageMaker to use a VPC with an S3 endpoint.",
            D: "Configure SageMaker to use S3 Glacier Deep Archive.",
          },
          correct_answer: "C",
          explanation:
            "Configuring Amazon SageMaker to use a VPC with an S3 endpoint ensures secure, direct, and managed data flow between Amazon S3 and SageMaker Studio notebooks. This setup avoids public internet exposure and maintains data integrity during transfers. The other options do not provide a solution for managing the data flow in this context.",
          id: 85,
        },
        {
          number: "85",
          question:
            "A company has a foundation model (FM) that was customized by using Amazon Bedrock to answer customer queries about products. The company wants to validate the model's responses to new types of queries. The company needs to upload a new dataset that Amazon Bedrock can use for validation. Which AWS service meets these requirements?",
          options: {
            A: "Amazon S3",
            B: "Amazon Elastic Block Store (Amazon EBS)",
            C: "Amazon Elastic File System (Amazon EFS)",
            D: "AWS Snowcone",
          },
          correct_answer: "A",
          explanation:
            "Amazon S3 is the most suitable AWS service for uploading and storing datasets used for validation purposes. It is highly scalable and integrated with Amazon Bedrock, allowing easy access to data for model validation. The other options do not provide the same level of integration or suitability for managing datasets in this context.",
          id: 86,
        },
        {
          number: "86",
          question:
            "Which prompting attack directly exposes the configured behavior of a large language model (LLM)?",
          options: {
            A: "Prompted persona switches",
            B: "Exploiting friendliness and trust",
            C: "Ignoring the prompt template",
            D: "Extracting the prompt template",
          },
          correct_answer: "D",
          explanation:
            '"Extracting the prompt template" is a type of prompting attack that involves directly exposing the configured behavior or the underlying system prompt of a large language model (LLM). This attack can reveal sensitive details about how the model operates, including its internal instructions or restrictions, which are typically not intended to be disclosed to the user. Such an exposure can compromise the security and reliability of the LLM by making it vulnerable to further exploitation or misuse.',
          id: 87,
        },
        {
          number: "87",
          question:
            "A company wants to use Amazon Bedrock. The company needs to review which security aspects the company is responsible for when using Amazon Bedrock. Which security aspect will the company be responsible for?",
          options: {
            A: "Patching and updating the versions of Amazon Bedrock",
            B: "Protecting the infrastructure that hosts Amazon Bedrock",
            C: "Securing the company's data in transit and at rest",
            D: "Provisioning Amazon Bedrock within the company network",
          },
          correct_answer: "C",
          explanation:
            "According to the AWS Shared Responsibility Model, AWS manages the security of the cloud, including the infrastructure and services like Amazon Bedrock. Customers are responsible for security in the cloud, which encompasses protecting their data, managing access controls, and configuring security settings for their applications. Reference: https://aws.amazon.com/compliance/shared-responsibility-model/?nc1=h_ls",
          id: 88,
        },
        {
          number: "88",
          question:
            "A social media company wants to use a large language model (LLM) to summarize messages. The company has chosen a few LLMs that are available on Amazon SageMaker JumpStart. The company wants to compare the generated output toxicity of these models. Which strategy gives the company the ability to evaluate the LLMs with the LEAST operational overhead?",
          options: {
            A: "Crowd-sourced evaluation",
            B: "Automatic model evaluation",
            C: "Model evaluation with human workers",
            D: "Reinforcement learning from human feedback (RLHF)",
          },
          correct_answer: "B",
          explanation:
            "Automatic model evaluation is the strategy that allows the company to evaluate the LLMs with the least operational overhead. This method leverages automated tools and processes to assess the toxicity or quality of the generated output without the need for manual intervention or crowd-sourced input. By using pre-built evaluation metrics or toxicity detection models, the company can quickly and efficiently evaluate multiple models without the complexity and time required for human evaluations.",
          id: 89,
        },
        {
          number: "89",
          question:
            "A company is testing the security of a foundation model (FM). During testing, the company wants to get around the safety features and make harmful content. Which security technique is this an example of? ",
          options: {
            A: "Fuzzing training data to find vulnerabilities",
            B: "Denial of service (DoS)",
            C: "Penetration testing with authorization",
            D: "Jailbreak",
          },
          correct_answer: "D",
          explanation:
            '"Jailbreaking" refers to attempts to bypass or disable the built-in safety features and restrictions of a system, in this case, a foundation model (FM). This technique involves trying to circumvent the safeguards that prevent the model from generating harmful or unsafe content. Jailbreaking is often performed to exploit vulnerabilities in a model’s filtering or safety protocols, making it a direct attempt to undermine its protections.',
          id: 90,
        },
        {
          number: "90",
          question:
            "A company needs to use Amazon SageMaker for model training and inference. The company must comply with regulatory requirements to run SageMaker jobs in an isolated environment without internet access. Which solution will meet these requirements?",
          options: {
            A: "Run SageMaker training and inference by using SageMaker Experiments.",
            B: "Run SageMaker training and inference by using network isolation.",
            C: "Encrypt the data at rest by using encryption for SageMaker geospatial capabilities.",
            D: "Associate appropriate AWS Identity and Access Management (IAM) roles with the SageMaker jobs.",
          },
          correct_answer: "B",
          explanation:
            "Network isolation in Amazon SageMaker allows you to run training and inference jobs in an environment that does not have access to the internet. This helps ensure that the data and the model do not inadvertently access external resources, meeting regulatory compliance requirements for isolated environments.",
          id: 91,
        },
        {
          number: "91",
          question:
            "An ML research team develops custom ML models. The model artifacts are shared with other teams for integration into products and services. The ML team retains the model training code and data. The ML team wants to build a mechanism that the ML team can use to audit models. Which solution should the ML team use when publishing the custom ML models?",
          options: {
            A: "Create documents with the relevant information. Store the documents in Amazon S3.",
            B: "Use AWS AI Service Cards for transparency and understanding models.",
            C: "Create Amazon SageMaker Model Cards with intended uses and training and inference details.",
            D: "Create model training scripts. Commit the model training scripts to a Git repository.",
          },
          correct_answer: "C",
          explanation:
            "Amazon SageMaker Model Cards are designed to document the essential details about machine learning models, including their intended uses, training datasets, training parameters, evaluation metrics, and inference environment. This provides a centralized mechanism to store and audit the metadata of the models, which is ideal for the ML team's need to share and audit models effectively. T Key benefits of Amazon Model Cards:  SageMaker Standardized documentation of models' characteristics and intended use cases.  for auditing purposes. Transparency and traceability Integration with other AWS services for lifecycle management.",
          id: 92,
        },
        {
          number: "92",
          question:
            "A software company builds tools for customers. The company wants to use AI to increase software development productivity. Which solution will meet these requirements?",
          options: {
            A: "Use a binary classification model to generate code reviews.",
            B: "Install code recommendation software in the company's developer tools.",
            C: "Install a code forecasting tool to predict potential code issues.",
            D: "Use a natural language processing (NLP) tool to generate code.",
          },
          correct_answer: "D",
          explanation:
            "Natural language processing (NLP) tools can be used to generate code from high-level descriptions or suggestions, which can greatly enhance software development productivity. By leveraging NLP models, developers can automate repetitive coding tasks, generate code snippets, or even complete blocks of code based on natural language inputs. This can speed up development and reduce errors.",
          id: 93,
        },
        {
          number: "93",
          question:
            "A retail store wants to predict the demand for a specific product for the next few weeks by using the Amazon SageMaker DeepAR forecasting algorithm. Which type of data will meet this requirement?",
          options: {
            A: "Text data",
            B: "Image data",
            C: "Time series data",
            D: "Binary data",
          },
          correct_answer: "C",
          explanation:
            "The Amazon SageMaker DeepAR forecasting algorithm is specifically designed for forecasting scalar (onedimensional) time series data using recurrent neural networks (RNNs). It excels when trained on datasets containing hundreds of related time series, enabling it to learn patterns across multiple series and provide accurate forecasts. Reference:",
          id: 94,
        },
        {
          number: "94",
          question:
            "A large retail bank wants to develop an ML system to help the risk management team decide on loan allocations for different demographics. What must the bank do to develop an unbiased ML model?",
          options: {
            A: "Reduce the size of the training dataset.",
            B: "Ensure that the ML model predictions are consistent with historical results.",
            C: "Create a different ML model for each demographic group.",
            D: "Measure class imbalance on the training dataset. Adapt the training process accordingly.",
          },
          correct_answer: "D",
          explanation:
            "In machine learning, class imbalance occurs when certain classes are underrepresented in the training dataset, leading to biased model predictions. To develop an unbiased model, it's crucial to assess the class distribution and adjust the training process to address any imbalances. This can be achieved through techniques such as oversampling the minority class, undersampling the majority class, or applying class weights to ensure the model treats all classes equitably.",
          id: 95,
        },
        {
          number: "95",
          question:
            "Which prompting technique can protect against prompt injection attacks?",
          options: {
            A: "Adversarial prompting",
            B: "Zero-shot prompting",
            C: "Least-to-most prompting",
            D: "Chain-of-thought prompting",
          },
          correct_answer: "A",
          explanation:
            "Adversarial prompting is a technique used to defend against prompt injection attacks by crafting inputs that are specifically designed to identify and neutralize malicious prompts. This approach involves generating prompts that can detect and mitigate adversarial inputs, thereby enhancing the robustness of language models against such attacks.",
          id: 96,
        },
        {
          number: "96",
          question:
            "A company has fine-tuned a large language model (LLM) to answer questions for a help desk. The company wants to determine if the fine-tuning has enhanced the model's accuracy. Which metric should the company use for the evaluation?",
          options: {
            A: "Precision",
            B: "Time to first token",
            C: "F1 score",
            D: "Word error rate",
          },
          correct_answer: "C",
          explanation:
            "The F1 score is a metric that combines precision and recall into a single value, providing a balance between the two. It is particularly useful in evaluating models where there is an uneven class distribution, as it considers both false positives and false negatives. The F1 score is calculated as the harmonic mean of precision and recall: This metric ranges from 0 to 1, with 1 indicating perfect precision and recall. In the context of evaluating a fine-tuned large language model (LLM) for a help desk application, the F1 score is appropriate because it assesses the model's ability to provide accurate and relevant responses, balancing the trade-off between precision (correctness of responses) and recall (completeness of relevant responses). Reference:",
          id: 97,
        },
        {
          number: "97",
          question:
            "A company is using Retrieval Augmented Generation (RAG) with Amazon Bedrock and Stable Diffusion to generate product images based on text descriptions. The results are often random and lack specific details. The company wants to increase the specificity of the generated images.     Which solution meets these requirements?",
          options: {
            A: "Increase the number of generation steps.",
            B: "Use the MASK_IMAGE_BLACK mask source option.",
            C: "Increase the classifier-free guidance (CFG) scale.",
            D: "Increase the prompt strength.",
          },
          correct_answer: "C",
          explanation:
            "In Stable Diffusion, the classifier-free guidance (CFG) scale parameter controls how closely the generated image adheres to the provided text prompt. By increasing the CFG scale, the model places more emphasis on the prompt, leading to images that more accurately reflect the specified details. However, it's important to balance this setting, as excessively high values can result in less diverse and potentially lowerquality images.",
          id: 98,
        },
        {
          number: "98",
          question:
            "A company wants to implement a large language model (LLM) based chatbot to provide customer service agents with real-time contextual responses to customers' inquiries. The company will use the company's policies as the knowledge base. Which solution will meet these requirements MOST cost-effectively?",
          options: {
            A: "Retrain the LLM on the company policy data.",
            B: "Fine-tune the LLM on the company policy data.",
            C: "Implement Retrieval Augmented Generation (RAG) for in-context responses.",
            D: "Use pre-training and data augmentation on the company policy data.",
          },
          correct_answer: "C",
          explanation:
            "Retrieval Augmented Generation (RAG) integrates external data sources with LLMs to produce accurate and contextually relevant outputs without the need for extensive retraining. By connecting the chatbot to the company's policy documents, RAG enables the model to retrieve pertinent information in real-time, ensuring responses are both accurate and up-to-date. This approach is cost-effective as it leverages existing data without the computational expenses associated with retraining or fine-tuning large models. Reference:",
          id: 99,
        },
        {
          number: "99",
          question:
            "A company wants to create a new solution by using AWS Glue. The company has minimal programming experience with AWS Glue. Which AWS service can help the company use AWS Glue?",
          options: {
            A: "Amazon Q Developer",
            B: "AWS Config",
            C: "Amazon Personalize",
            D: "Amazon Comprehend",
          },
          correct_answer: "A",
          explanation:
            "Amazon Q Developer is a tool designed to help users with minimal programming experience to work with AWS Glue. It provides a graphical user interface and simplifies the creation of data transformation and extraction workflows, allowing users to perform tasks like querying and working with data without needing deep coding skills.",
          id: 100,
        },
        {
          number: "100",
          question:
            "A company is developing a mobile ML app that uses a phone's camera to diagnose and treat insect bites. The company wants to train an image classification model by using a diverse dataset of insect bite photos from different genders, ethnicities, and geographic locations around the world. Which principle of responsible AI does the company demonstrate in this scenario?",
          options: {
            A: "Fairness",
            B: "Explainability",
            C: "Governance",
            D: "Transparency",
          },
          correct_answer: "A",
          explanation:
            "The company is actively seeking to ensure that the image classification model is trained on a diverse dataset that includes insect bite photos from various genders, ethnicities, and geographic locations. This reflects the fairness principle of responsible AI, which emphasizes creating models that make unbiased decisions across all demographic groups. By including a diverse range of data, the company is aiming to prevent biases that could lead to inaccurate diagnoses or treatments for certain groups of people. Fairness ensures that AI systems do not discriminate based on race, gender, geography, or other characteristics.",
          id: 101,
        },
        {
          number: "101",
          question:
            "A company is developing an ML model to make loan approvals. The company must implement a solution to detect bias in the model. The company must also be able to explain the model's predictions. Which solution will meet these requirements?",
          options: {
            A: "Amazon SageMaker Clarify",
            B: "Amazon SageMaker Data Wrangler",
            C: "Amazon SageMaker Model Cards",
            D: "AWS AI Service Cards",
          },
          correct_answer: "A",
          explanation:
            "Amazon SageMaker Clarify is a tool that helps detect bias in machine learning models and provides explainability for model predictions. It allows users to understand the factors influencing a model's predictions and assess whether the model is fair across different demographic groups. This is exactly what the company needs for detecting bias and explaining model decisions, especially for high-stakes applications like loan approvals. Bias detection: SageMaker Clarify can analyze the training data and the model's predictions to identify and mitigate bias. Explainability: It provides features for explaining the predictions made by the model, which helps users understand the reasons behind the model's decisions.",
          id: 102,
        },
        {
          number: "102",
          question:
            "A company has developed a generative text summarization model by using Amazon Bedrock. The company will use Amazon Bedrock automatic model evaluation capabilities.   Which metric should the company use to evaluate the accuracy of the model?  ",
          options: {
            A: "Area Under the ROC Curve (AUC) score",
            B: "F1 score",
            C: "BERTScore",
            D: "Real world knowledge (RWK) score",
          },
          correct_answer: "C",
          explanation:
            "BERTScore is a metric specifically designed for evaluating the quality of text generated by models, particularly in tasks like text summarization and machine translation. It uses contextual embeddings from a pre-trained BERT model to compare generated text with reference text at the word level, making it wellsuited for evaluating the accuracy of generative text summarization models. BERTScore assesses the semantic similarity between the generated text and the reference text, providing a more nuanced evaluation compared to traditional methods that focus on exact matches.",
          id: 103,
        },
        {
          number: "103",
          question:
            "An AI practitioner wants to predict the classification of flowers based on petal length, petal width, sepal length, and sepal width. Which algorithm meets these requirements?",
          options: {
            A: "K-nearest neighbors (k-NN)",
            B: "K-mean",
            C: "Autoregressive Integrated Moving Average (ARIMA)",
            D: "Linear regression",
          },
          correct_answer: "A",
          explanation:
            "K-nearest neighbors (k-NN) is a supervised learning algorithm used for classification tasks. It works by classifying a data point based on how its features (such as petal length, petal width, sepal length, and sepal width in this case) are similar to the data points in the training set. For this problem, where the task is to classify flowers based on their features, k-NN is an appropriate choice. k-NN is simple and effective for classification problems where the decision boundary is not necessarily linear, as it looks at the closest neighbors to make predictions.",
          id: 104,
        },
        {
          number: "104",
          question:
            "A company is using custom models in Amazon Bedrock for a generative AI application. The company wants to use a company managed encryption key to encrypt the model artifacts that the model customization jobs create. Which AWS service meets these requirements?",
          options: {
            A: "AWS Key Management Service (AWS KMS)",
            B: "Amazon Inspector",
            C: "Amazon Macie",
            D: "AWS Secrets Manager",
          },
          correct_answer: "A",
          explanation:
            "AWS Key Management Service (AWS KMS) is a fully managed service that allows you to create and control encryption keys used to encrypt your data. It is ideal for scenarios like this, where a company wants to use a custom, company-managed encryption key to protect model artifacts. AWS KMS allows you to securely manage keys for encrypting and decrypting data, and it integrates with various AWS services, including Amazon Bedrock. AWS KMS provides centralized key management, and it is designed for use cases that involve encrypting both data and artifacts, such as model customization jobs.",
          id: 105,
        },
        {
          number: "105",
          question:
            "A company wants to use large language models (LLMs) to produce code from natural language code comments. Which LLM feature meets these requirements?",
          options: {
            A: "Text summarization",
            B: "Text generation",
            C: "Text completion",
            D: "Text classification",
          },
          correct_answer: "B",
          explanation:
            "Text generation is the feature of large language models (LLMs) that enables them to produce text based on a given prompt or input. In this scenario, the input would be natural language code comments, and the LLM would generate code based on those comments. This process involves understanding the context of the comment and generating corresponding code, which is the core functionality of text generation.",
          id: 106,
        },
        {
          number: "106",
          question:
            "A company is introducing a mobile app that helps users learn foreign languages. The app makes text more coherent by calling a large language model (LLM). The company collected a diverse dataset of text and supplemented the dataset with examples of more readable versions. The company wants the LLM output to resemble the provided examples. Which metric should the company use to assess whether the LLM meets these requirements?",
          options: {
            A: "Value of the loss function",
            B: "Semantic robustness",
            C: "Recall-Oriented Understudy for Gisting Evaluation (ROUGE) score",
            D: "Latency of the text generation",
          },
          correct_answer: "C",
          explanation:
            "ROUGE is a set of metrics that evaluates the quality of summaries by comparing the overlap of n-grams, word sequences, and word pairs between the model output and reference examples. Since the company is working on a language model that improves the coherence of text and wants the output to resemble the provided examples (which are more readable versions of the original text), ROUGE is the most appropriate metric. It assesses how closely the generated text matches the reference text in terms of content and readability. ROUGE score is commonly used to evaluate the performance of models in tasks like summarization, where the goal is to ensure the generated text aligns closely with human-provided examples.",
          id: 107,
        },
        {
          number: "107",
          question:
            "A company notices that its foundation model (FM) generates images that are unrelated to the prompts. The company wants to modify the prompt techniques to decrease unrelated images.     Which solution meets these requirements?",
          options: {
            A: "Use zero-shot prompts.",
            B: "Use negative prompts.",
            C: "Use positive prompts.",
            D: "Use ambiguous prompts.",
          },
          correct_answer: "B",
          explanation:
            "Negative prompts are used to explicitly instruct the model about what to avoid or not generate. By providing the model with specific guidance on what is not desired (e.g., by including terms or concepts that should not appear in the image), the model can better focus on generating relevant and related content. This technique can help reduce unrelated or irrelevant images by constraining the model's creative generation process.",
          id: 108,
        },
        {
          number: "108",
          question:
            "A company wants to use a large language model (LLM) to generate concise, feature-specific descriptions for the company’s products. Which prompt engineering technique meets these requirements?",
          options: {
            A: "Create one prompt that covers all products. Edit the responses to make the responses more specific, concise, and tailored to each product.",
            B: "Create prompts for each product category that highlight the key features. Include the desired output format and length for each prompt response.",
            C: "Include a diverse range of product features in each prompt to generate creative and unique descriptions.",
            D: "Provide detailed, product-specific prompts to ensure precise and customized descriptions.",
          },
          correct_answer: "B",
          explanation:
            "To generate concise, feature-specific descriptions for each product, the company should create prompts tailored to specific product categories. By highlighting the key features of each product category in the prompt, the model can focus on generating descriptions that are relevant and aligned with the unique attributes of each product. Additionally, specifying the desired output format and length ensures that the responses meet the company's requirements for conciseness and clarity. Tailored prompts help ensure the model generates relevant and accurate descriptions by focusing on the most important features of each product category. Desired output format and length ensure the responses are consistent and concise, as required.",
          id: 109,
        },
        {
          number: "109",
          question:
            "A company is developing an ML model to predict customer churn. The model performs well on the training dataset but does not accurately predict churn for new data. Which solution will resolve this issue?  ",
          options: {
            A: "Decrease the regularization parameter to increase model complexity.",
            B: "Increase the regularization parameter to decrease model complexity.",
            C: "Add more features to the input data.",
            D: "Train the model for more epochs.",
          },
          correct_answer: "B",
          explanation:
            "The issue described is a common case of overfitting, where the model performs well on the training data but fails to generalize to new, unseen data. This suggests that the model is too complex and has learned to memorize the training data rather than identifying generalizable patterns. Increasing the regularization parameter helps to reduce model complexity by penalizing large weights, thereby encouraging simpler models that are less likely to overfit. This will improve the model's ability to generalize to new data, potentially improving performance on unseen customer data.",
          id: 110,
        },
        {
          number: "110",
          question:
            "A company is implementing intelligent agents to provide conversational search experiences for its customers. The company needs a database service that will support storage and queries of embeddings from a generative AI model as vectors in the database. Which AWS service will meet these requirements?",
          options: {
            A: "Amazon Athena",
            B: "Amazon Aurora PostgreSQL",
            C: "Amazon Redshift",
            D: "Amazon EMR",
          },
          correct_answer: "B",
          explanation:
            "Amazon Aurora PostgreSQL is a relational database service that supports a wide range of applications, including those that require vector-based operations. Specifically, Aurora PostgreSQL can be extended with vector search capabilities using extensions like pgvector. This extension allows you to store, index, and query vector embeddings from generative AI models, making it well-suited for the company's needs to store and query embeddings as vectors. pgvector is an extension for PostgreSQL that provides efficient similarity search for vector data, which is ideal for storing and querying embeddings.",
          id: 111,
        },
        {
          number: "111",
          question:
            "A financial institution is building an AI solution to make loan approval decisions by using a foundation model (FM). For security and audit purposes, the company needs the AI solution's decisions to be explainable. Which factor relates to the explainability of the AI solution's decisions?",
          options: {
            A: "Model complexity",
            B: "Training time",
            C: "Number of hyperparameters",
            D: "Deployment time",
          },
          correct_answer: "A",
          explanation:
            'Model complexity plays a significant role in the explainability of an AI solution. Simpler models tend to be more explainable because it is easier to understand how they make decisions. For example, linear regression or decision trees are generally easier to interpret compared to more complex models like deep neural networks. In contrast, highly complex models, such as deep learning models, may provide very accurate results but are often considered "black boxes," meaning their decision-making process is not easily interpretable. This lack of transparency makes it difficult to explain their decisions in a way that satisfies regulatory requirements or customer understanding.',
          id: 112,
        },
        {
          number: "112",
          question:
            "A pharmaceutical company wants to analyze user reviews of new medications and provide a concise overview for each medication. Which solution meets these requirements?",
          options: {
            A: "Create a time-series forecasting model to analyze the medication reviews by using Amazon Personalize.",
            B: "Create medication review summaries by using Amazon Bedrock large language models (LLMs).",
            C: "Create a classification model that categorizes medications into different groups by using Amazon",
            D: "Create medication review summaries by using Amazon Rekognition.",
          },
          correct_answer: "B",
          explanation:
            "Amazon Bedrock provides access to a variety of large language models (LLMs), which are well-suited for tasks such as text summarization. By using LLMs, the pharmaceutical company can automatically generate concise and coherent summaries of user reviews for each medication. These models are designed to understand and process large amounts of text, making them ideal for summarizing user reviews in a clear and efficient manner. Amazon Bedrock allows the company to utilize LLMs that can generate summaries, which is exactly what is needed in this case.",
          id: 113,
        },
        {
          number: "113",
          question:
            "A company wants to build a lead prioritization application for its employees to contact potential customers. The application must give employees the ability to view and adjust the weights assigned to different variables in the model based on domain knowledge and expertise. Which ML model type meets these requirements?",
          options: {
            A: "Logistic regression model",
            B: "Deep learning model built on principal components",
            C: "K-nearest neighbors (k-NN) model",
            D: "Neural network",
          },
          correct_answer: "A",
          explanation:
            "A logistic regression model is a simple, interpretable machine learning model that allows users to adjust the weights of different variables based on domain knowledge and expertise. Since logistic regression is based on a linear relationship between the input features and the predicted outcome, each feature has a corresponding weight (coefficient) that can be easily viewed and adjusted. This makes it a suitable choice for a lead prioritization application where domain experts need the ability to modify the model based on their knowledge.",
          id: 114,
        },
        {
          number: "114",
          type: "dropdown_multiple",
          question:
            "A company wants to build an ML application. Select and order the correct steps from the following list to develop a well-architected ML workload. Each step should be selected one time.",
          options: [
            "Deploy model",
            "Develop model",
            "Monitor model",
            "Define business goal and frame ML problem",
          ],
          scenarios: [
            {
              id: "scenario_1",
              text: "Step 1:",
            },
            {
              id: "scenario_2",
              text: "Step 2:",
            },
            {
              id: "scenario_3",
              text: "Step 3:",
            },
            {
              id: "scenario_4",
              text: "Step 4:",
            },
          ],
          correct_answer: {
            scenario_1: "Define business goal and frame ML problem",
            scenario_2: "Develop model",
            scenario_3: "Deploy model",
            scenario_4: "Monitor model",
          },
          explanation:
            "The typical sequence of steps in building an ML application involves: 1. Defining the business goal and framing the ML problem - This step involves understanding the business need and determining how ML can address it. 2. Developing the model - This involves selecting the appropriate algorithm, training the model, and evaluating it. 3. Deploying the model - Once the model is trained, it is deployed to production to serve predictions. 4. Monitoring the model - After deployment, the model is monitored to ensure it performs well over time and to detect any issues like model drift.",
          id: 115,
        },
        {
          number: "115",
          question:
            "Which strategy will determine if a foundation model (FM) effectively meets business objectives?",
          options: {
            A: "Evaluate the model's performance on benchmark datasets.",
            B: "Analyze the model's architecture and hyperparameters.",
            C: "Assess the model's alignment with specific use cases.",
            D: "Measure the computational resources required for model deployment.",
          },
          correct_answer: "C",
          explanation:
            "To determine if a foundation model (FM) effectively meets business objectives, the key is to assess how well the model aligns with the specific use cases or business goals it is intended to address. This involves evaluating whether the model can provide meaningful and accurate outputs relevant to the business’s needs, ensuring that it solves the real-world problems the company aims to tackle. Aligning with specific use cases ensures that the model’s capabilities are tailored to the tasks at hand, such as improving customer support, enhancing decision-making, or automating certain processes.",
          id: 116,
        },
        {
          number: "116",
          question:
            "A company needs to train an ML model to classify images of different types of animals. The company has a large dataset of labeled images and will not label more data. Which type of learning should the company use to train the model?",
          options: {
            A: "Supervised learning",
            B: "Unsupervised learning",
            C: "Reinforcement learning",
            D: "Active learning",
          },
          correct_answer: "A",
          explanation:
            "In supervised learning, the model is trained using a labeled dataset, where each image (input) has a corresponding label (the type of animal, in this case). Since the company already has a large dataset of labeled images, supervised learning is the most appropriate approach. The model learns to classify the images based on the features and labels in the training data.",
          id: 117,
        },
        {
          number: "117",
          question:
            "Which phase of the ML lifecycle determines compliance and regulatory requirements? ",
          options: {
            A: "Feature engineering",
            B: "Model training",
            C: "Data collection",
            D: "Business goal identification",
          },
          correct_answer: "C",
          explanation:
            "The data collection phase of the ML lifecycle is the most relevant for determining compliance and regulatory requirements. During this phase, organizations must ensure that the data being collected and used for training the model complies with legal and regulatory standards, such as data privacy laws (e.g., GDPR, HIPAA), industry-specific regulations, and ethical considerations. The organization must also verify that they have the proper consent to use the data and that the data does not contain any biases or violate any regulations.",
          id: 118,
        },
        {
          number: "118",
          question:
            "A food service company wants to develop an ML model to help decrease daily food waste and increase sales revenue. The company needs to continuously improve the model's accuracy. Which solution meets these requirements?",
          options: {
            A: "Use Amazon SageMaker and iterate with newer data.",
            B: "Use Amazon Personalize and iterate with historical data.",
            C: "Use Amazon CloudWatch to analyze customer orders.",
            D: "Use Amazon Rekognition to optimize the model.",
          },
          correct_answer: "A",
          explanation:
            "To continuously improve the accuracy of the machine learning model, Amazon SageMaker is the appropriate solution. SageMaker allows for efficient model training, deployment, and iteration. The company can use SageMaker to retrain the model regularly with newer data to account for changes in customer behavior, market trends, and other dynamic factors that may affect food waste and sales revenue. Iterating with newer data helps improve the model's performance over time, ensuring it remains accurate and relevant.",
          id: 119,
        },
        {
          number: "119",
          question:
            "A company has developed an ML model to predict real estate sale prices. The company wants to deploy the model to make predictions without managing servers or infrastructure. Which solution meets these requirements?",
          options: {
            A: "Deploy the model on an Amazon EC2 instance.",
            B: "Deploy the model on an Amazon Elastic Kubernetes Service (Amazon EKS) cluster.",
            C: "Deploy the model by using Amazon CloudFront with an Amazon S3 integration.",
            D: "Deploy the model by using an Amazon SageMaker endpoint.",
          },
          correct_answer: "D",
          explanation:
            "Amazon SageMaker provides a fully managed service for deploying machine learning models at scale without needing to manage servers or infrastructure. When you deploy a model using Amazon SageMaker endpoints, it handles all the infrastructure, scaling, and maintenance aspects. This allows the company to focus solely on making predictions and integrating the model into their application, without worrying about managing servers or clusters. Amazon SageMaker simplifies the deployment of ML models by providing scalable, secure, and serverless endpoints, which are ideal for serving predictions in real-time.",
          id: 120,
        },
        {
          number: "120",
          question:
            "A company wants to develop an AI application to help its employees check open customer claims, identify details for a specific claim, and access documents for a claim. Which solution meets these requirements?",
          options: {
            A: "Use Agents for Amazon Bedrock with Amazon Fraud Detector to build the application.",
            B: "Use Agents for Amazon Bedrock with Amazon Bedrock knowledge bases to build the application.",
            C: "Use Amazon Personalize with Amazon Bedrock knowledge bases to build the application.",
            D: "Use Amazon SageMaker to build the application by training a new ML model.",
          },
          correct_answer: "B",
          explanation:
            "Amazon Bedrock is a service that allows you to build AI-powered applications using foundation models (FMs) without managing infrastructure. Agents for Amazon Bedrock allows you to create intelligent agents that can help users interact with data and perform tasks such as checking open customer claims, identifying claim details, and accessing relevant documents. By using Amazon Bedrock knowledge bases, the AI agent can access relevant information (e.g., details about claims, documents, or customer data) in real time, and provide precise, context-driven answers based on the claim-related data stored in the knowledge base. Agents for Amazon Bedrock can help automate interactions by understanding and responding to natural language queries, making it a good fit for this use case. Amazon Bedrock knowledge bases enable the system to access stored information, documents, and details for specific claims efficiently.",
          id: 121,
        },
        {
          number: "121",
          question:
            "A manufacturing company uses AI to inspect products and find any damages or defects. Which type of AI application is the company using?",
          options: {
            A: "Recommendation system",
            B: "Natural language processing (NLP)",
            C: "Computer vision",
            D: "Image processing",
          },
          correct_answer: "C",
          explanation:
            "Computer vision is a field of AI that enables machines to interpret and understand visual information from the world, such as images and videos. In the case of the manufacturing company, the AI is used to inspect products for damages or defects, which involves analyzing visual data (e.g., product images or videos). This is a classic application of computer vision, where the AI system identifies and classifies objects or defects within images.",
          id: 122,
        },
        {
          number: "122",
          question:
            "A company wants to create an ML model to predict customer satisfaction. The company needs fully automated model tuning. Which AWS service meets these requirements?",
          options: {
            A: "Amazon Personalize",
            B: "Amazon SageMaker",
            C: "Amazon Athena",
            D: "Amazon Comprehend",
          },
          correct_answer: "B",
          explanation:
            "Amazon SageMaker provides a fully managed environment for building, training, and deploying machine learning models, including automatic model tuning. Specifically, SageMaker includes a feature called Automatic Model Tuning (or Hyperparameter Optimization), which automates the process of finding the best hyperparameters for your machine learning model. This is essential when you want to optimize the model’s performance without manual intervention. Amazon SageMaker allows you to automate the training process and hyperparameter tuning, which aligns perfectly with the company's need for fully automated model tuning.",
          id: 123,
        },
        {
          number: "123",
          question:
            "Which technique can a company use to lower bias and toxicity in generative AI applications during the postprocessing ML lifecycle?",
          options: {
            A: "Human-in-the-loop",
            B: "Data augmentation",
            C: "Feature engineering",
            D: "Adversarial training",
          },
          correct_answer: "A",
          explanation:
            "Human-in-the-loop (HITL) is a technique where human oversight is involved in the decision-making process of AI systems. In the context of generative AI applications, HITL can be used during the postprocessing phase to identify and mitigate biases or toxic outputs. Humans can review and intervene when the model generates inappropriate or biased content, providing corrections or adjustments that help reduce the likelihood of toxicity and bias. This feedback loop helps refine and improve the model’s outputs over time.",
          id: 124,
        },
        {
          number: "124",
          question:
            "A bank has fine-tuned a large language model (LLM) to expedite the loan approval process. During an external audit of the model, the company discovered that the model was approving loans at a faster pace for a specific demographic than for other demographics. How should the bank fix this issue MOST cost-effectively?",
          options: {
            A: "Include more diverse training data. Fine-tune the model again by using the new data.",
            B: "Use Retrieval Augmented Generation (RAG) with the fine-tuned model.",
            C: "Use AWS Trusted Advisor checks to eliminate bias.",
            D: "Pre-train a new LLM with more diverse training data.",
          },
          correct_answer: "A",
          explanation:
            "The issue described is a case of bias in the model’s decision-making process, where the model is showing a preference for a specific demographic. The most cost-effective way to address this is to include more diverse training data that better represents all demographics. By fine-tuning the model with this more diverse data, you can help ensure that the model treats all demographic groups fairly and does not exhibit biased behavior. Fine-tuning the model with updated data ensures that the model learns from a more representative sample, improving fairness in its predictions without needing to completely retrain the model from scratch.",
          id: 125,
        },
        {
          number: "125",
          type: "dropdown_multiple",
          question:
            "A company has developed a large language model (LLM) and wants to make the LLM available to multiple internal teams. The company needs to select the appropriate inference mode for each team. Select the correct inference mode from the following list for each use case. Each inference mode should be selected one or more times. ",
          options: ["Batch transform", "Real-time inference"],
          scenarios: [
            {
              id: "scenario_1",
              text: "The company's chatbot needs predictions from the LLM to understand users' intent with minimal latency.",
            },
            {
              id: "scenario_2",
              text: "A data processing job needs to query the LLM to process gigabytes of text files on weekends.",
            },
            {
              id: "scenario_3",
              text: "The company's engineering team needs to create an API that can process small pieces of text content and provide low-latency predictions.",
            },
          ],
          correct_answer: {
            scenario_1: "Real-time inference",
            scenario_2: "Batch transform",
            scenario_3: "Real-time inference",
          },
          explanation:
            "The company's chatbot needs predictions from the LLM to understand users' intent with minimal latency - Real-time inference This scenario requires Real-time inference, as chatbots typically need immediate responses with low latency to provide an interactive user experience. A data processing job needs to query the LLM to process gigabytes of text files on weekends Batch transform This use case is best suited for Batch transform. Since the data processing job involves handling large amounts of data, and it is scheduled for weekends, batch processing can handle these large volumes efficiently. The company's engineering team needs to create an API that can process small pieces of text content and provide low-latency predictions - Real-time inference Real-time inference is the correct choice here, as the API needs to process text quickly and provide immediate responses, making real-time inference appropriate.",
          id: 126,
        },
        {
          number: "126",
          question:
            "A company needs to log all requests made to its Amazon Bedrock API. The company must retain the logs securely for 5 years at the lowest possible cost. Which combination of AWS service and storage class meets these requirements? (Choose two.)",
          options: {
            A: "AWS CloudTrail",
            B: "Amazon CloudWatch",
            C: "AWS Audit Manager",
            D: "Amazon S3 Intelligent-Tiering",
            E: "Amazon S3 Standard",
          },
          correct_answer: "AD",
          explanation:
            "AWS CloudTrail is the AWS service designed for logging and monitoring API calls made to AWS services, including Amazon Bedrock. CloudTrail records detailed information about the API requests, including the identity of the requester, the time of the request, and the source IP address. This service is ideal for logging all requests made to the Amazon Bedrock API and meets the logging requirement. Amazon S3 Intelligent-Tiering is a storage class designed for storing data that has unpredictable access patterns. It automatically moves data between two access tiers (frequent and infrequent) based on usage, which helps reduce costs while ensuring data is still available when needed. For retaining logs securely over 5 years at the lowest possible cost, this storage class provides an efficient way to handle long-term storage requirements without incurring unnecessary costs.",
          id: 127,
        },
        {
          number: "127",
          question:
            "An ecommerce company wants to improve search engine recommendations by customizing the results for each user of the company’s ecommerce platform. Which AWS service meets these requirements?",
          options: {
            A: "Amazon Personalize",
            B: "Amazon Kendra",
            C: "Amazon Rekognition",
            D: "Amazon Transcribe",
          },
          correct_answer: "A",
          explanation:
            "Amazon Personalize is an AWS service specifically designed to build and deploy personalized recommendations for users. It allows you to create custom machine learning models for personalized user experiences, such as product recommendations, search results, or content suggestions. For an ecommerce company aiming to improve search engine recommendations tailored to each user, Amazon Personalize is the ideal choice as it leverages user behavior data and machine learning to deliver highly relevant recommendations.",
          id: 128,
        },
        {
          number: "128",
          question:
            "A hospital is developing an AI system to assist doctors in diagnosing diseases based on patient records and medical images. To comply with regulations, the sensitive patient data must not leave the country the data is located in. Which data governance strategy will ensure compliance and protect patient privacy?",
          options: {
            A: "Data residency",
            B: "Data quality",
            C: "Data discoverability",
            D: "Data enrichment",
          },
          correct_answer: "A",
          explanation:
            "Data residency refers to the practice of ensuring that data is stored and processed within a specific geographical location to comply with regulations or policies. In this scenario, where sensitive patient data must not leave the country it originates from, implementing a data residency strategy ensures compliance with legal and regulatory requirements while protecting patient privacy. This approach is crucial for organizations like hospitals operating under strict data governance frameworks such as HIPAA or GDPR.",
          id: 129,
        },
        {
          number: "129",
          question:
            "A company needs to monitor the performance of its ML systems by using a highly scalable AWS service. Which AWS service meets these requirements?",
          options: {
            A: "Amazon CloudWatch",
            B: "AWS CloudTrail",
            C: "AWS Trusted Advisor",
            D: "AWS Config",
          },
          correct_answer: "A",
          explanation:
            "Amazon CloudWatch is a highly scalable AWS service designed for monitoring and observability. It provides real-time monitoring of system metrics, including performance data for ML systems, such as resource utilization (CPU, memory, etc.), model inference latency, request counts, and errors. CloudWatch enables users to set up alarms, visualize metrics, and automate actions based on performance thresholds, making it ideal for monitoring the performance of ML systems.",
          id: 130,
        },
        {
          number: "130",
          question:
            "An AI practitioner is developing a prompt for an Amazon Titan model. The model is hosted on Amazon Bedrock. The AI practitioner is using the model to solve numerical reasoning challenges. The AI practitioner adds the following phrase to the end of the prompt: “Ask the model to show its work by explaining its reasoning step by step.” Which prompt engineering technique is the AI practitioner using?",
          options: {
            A: "Chain-of-thought prompting",
            B: "Prompt injection",
            C: "Few-shot prompting",
            D: "Prompt templating",
          },
          correct_answer: "A",
          explanation:
            'Chain-of-thought prompting is a technique in prompt engineering where the AI model is encouraged to break down its reasoning process step by step to solve a problem, such as numerical reasoning challenges. By explicitly instructing the model to "show its work by explaining its reasoning step by step," the practitioner ensures the model provides a logical sequence of intermediate steps leading to the solution. This improves the accuracy and transparency of the model\'s outputs, particularly for complex reasoning tasks.',
          id: 131,
        },
        {
          number: "131",
          question:
            "Which AWS service makes foundation models (FMs) available to help users build and scale generative AI applications?",
          options: {
            A: "Amazon Q Developer",
            B: "Amazon Bedrock",
            C: "Amazon Kendra",
            D: "Amazon Comprehend",
          },
          correct_answer: "B",
          explanation:
            "Amazon Bedrock is an AWS service that allows users to build and scale generative AI applications using foundation models (FMs) without the need to manage infrastructure. It provides access to pre-trained models from AWS and third-party providers, enabling developers to integrate generative AI capabilities like text generation, summarization, and question-answering into their applications. This makes Amazon Bedrock the ideal choice for building and scaling generative AI solutions.",
          id: 132,
        },
        {
          number: "132",
          question:
            "A company is building a mobile app for users who have a visual impairment. The app must be able to hear what users say and provide voice responses. Which solution will meet these requirements?",
          options: {
            A: "Use a deep learning neural network to perform speech recognition.",
            B: "Build ML models to search for patterns in numeric data.",
            C: "Use generative AI summarization to generate human-like text.",
            D: "Build custom models for image classification and recognition.",
          },
          correct_answer: "A",
          explanation:
            'To meet the requirements of enabling the app to "hear what users say and provide voice responses," the solution must include speech recognition and text-to-speech capabilities. Using a deep learning neural network for speech recognition allows the app to convert spoken words into text. Once the input is understood, text-to-speech systems can provide voice responses back to users. This approach is fundamental in applications that assist users with visual impairments by enabling interaction through spoken language.',
          id: 133,
        },
        {
          number: "133",
          question:
            "A company wants to enhance response quality for a large language model (LLM) for complex problemsolving tasks. The tasks require detailed reasoning and a step-by-step explanation process. Which prompt engineering technique meets these requirements?",
          options: {
            A: "Few-shot prompting",
            B: "Zero-shot prompting",
            C: "Directional stimulus prompting",
            D: "Chain-of-thought prompting",
          },
          correct_answer: "D",
          explanation:
            "Chain-of-thought prompting is specifically designed to enhance the reasoning capabilities of a large language model (LLM) by encouraging it to provide step-by-step explanations for complex problem-solving tasks. This approach helps the model break down a problem into smaller, logical steps, ensuring that the response is detailed, accurate, and easy to follow.",
          id: 134,
        },
        {
          number: "134",
          question:
            "A company wants to keep its foundation model (FM) relevant by using the most recent data. The company wants to implement a model training strategy that includes regular updates to the FM. Which solution meets these requirements?",
          options: {
            A: "Batch learning",
            B: "Continuous pre-training",
            C: "Static training",
            D: "Latent training",
          },
          correct_answer: "B",
          explanation:
            "Continuous pre-training keeps a foundation model “always learning” by periodically (or even continuously) ingesting the latest data and updating its weights. This approach ensures the model stays current without having to start training from scratch every time new data arrives, unlike static or strictly batch-oriented methods.",
          id: 135,
        },
        {
          number: "135",
          type: "dropdown_multiple",
          question:
            "A company wants to develop ML applications to improve business operations and efficiency. Select the correct ML paradigm from the following list for each use case. Each ML paradigm should be selected one or more times.",
          options: ["Supervised learning", "Unsupervised learning"],
          scenarios: [
            {
              id: "scenario_1",
              text: "Binary classification",
            },
            {
              id: "scenario_2",
              text: "Multi-class classification",
            },
            {
              id: "scenario_3",
              text: "K-means clustering",
            },
            {
              id: "scenario_4",
              text: "Dimensionality reduction",
            },
          ],
          correct_answer: {
            scenario_1: "Supervised learning",
            scenario_2: "Supervised learning",
            scenario_3: "Unsupervised learning",
            scenario_4: "Unsupervised learning",
          },
          explanation:
            "Binary classification: Supervised learning Binary classification involves predicting one of two possible outcomes, requiring labeled training data. Multi-class classification: Supervised learning Multi-class classification extends binary classification to predict one of several classes, also requiring labeled data. K-means clustering: Unsupervised learning K-means clustering is used to group data points into clusters without requiring labeled data. Dimensionality reduction: Unsupervised learning Dimensionality reduction techniques, like PCA, reduce the number of features in the dataset without needing labeled data.",
          id: 136,
        },
        {
          number: "136",
          question:
            "Which option is a characteristic of AI governance frameworks for building trust and deploying humancentered AI technologies?",
          options: {
            A: "Expanding initiatives across business units to create long-term business value",
            B: "Ensuring alignment with business standards, revenue goals, and stakeholder expectations",
            C: "Overcoming challenges to drive business transformation and growth",
            D: "Developing policies and guidelines for data, transparency, responsible AI, and compliance",
          },
          correct_answer: "D",
          explanation:
            "AI governance frameworks focus on building trust and ensuring the responsible deployment of AI technologies. They establish clear policies and guidelines to address critical aspects such as data management, transparency, ethical considerations, responsible AI practices, and regulatory compliance. These frameworks help organizations mitigate risks, promote fairness, and foster public trust in AI systems, making them essential for creating human-centered AI technologies.",
          id: 137,
        },
        {
          number: "137",
          question:
            "An ecommerce company is using a generative AI chatbot to respond to customer inquiries. The company wants to measure the financial effect of the chatbot on the company’s operations. Which metric should the company use? ",
          options: {
            A: "Number of customer inquiries handled",
            B: "Cost of training AI models",
            C: "Cost for each customer conversation",
            D: "Average handled time (AHT)",
          },
          correct_answer: "C",
          explanation:
            "To measure the financial effect of a generative AI chatbot on the company's operations, the cost for each customer conversation is the most relevant metric. It directly quantifies the operational expense associated with using the chatbot to handle customer inquiries. By analyzing this metric, the company can evaluate how much it spends per conversation and compare it to the cost of alternative methods (e.g., human agents), providing insight into the chatbot's financial efficiency and ROI.",
          id: 138,
        },
        {
          number: "138",
          question:
            "A company wants to find groups for its customers based on the customers’ demographics and buying patterns. Which algorithm should the company use to meet this requirement?",
          options: {
            A: "K-nearest neighbors (k-NN)",
            B: "K-means",
            C: "Decision tree",
            D: "Support vector machine",
          },
          correct_answer: "B",
          explanation:
            "K-means is a clustering algorithm used in unsupervised learning to group data points into clusters based on their similarities. It is well-suited for finding patterns in customer demographics and buying behavior. The algorithm identifies groups (clusters) of customers with similar characteristics, which can then be used for targeted marketing, personalized recommendations, or segmentation.",
          id: 139,
        },
        {
          number: "139",
          question:
            "A company’s large language model (LLM) is experiencing hallucinations. How can the company decrease hallucinations?",
          options: {
            A: "Set up Agents for Amazon Bedrock to supervise the model training.",
            B: "Use data pre-processing and remove any data that causes hallucinations.",
            C: "Decrease the temperature inference parameter for the model.",
            D: "Use a foundation model (FM) that is trained to not hallucinate.",
          },
          correct_answer: "C",
          explanation:
            "In the context of large language models (LLMs), the temperature inference parameter controls the randomness of the model's output. Lowering the temperature reduces randomness and makes the model's responses more deterministic and focused on the most likely predictions. By decreasing the temperature, the likelihood of hallucinations (when the model generates incorrect or nonsensical information) is reduced, as the model relies more on high-probability outputs rather than exploring less likely possibilities.",
          id: 140,
        },
        {
          number: "140",
          question:
            "A company is using a large language model (LLM) on Amazon Bedrock to build a chatbot. The chatbot processes customer support requests. To resolve a request, the customer and the chatbot must interact a few times.     Which solution gives the LLM the ability to use content from previous customer messages?",
          options: {
            A: "Turn on model invocation logging to collect messages.",
            B: "Add messages to the model prompt.",
            C: "Use Amazon Personalize to save conversation history.",
            D: "Use Provisioned Throughput for the LLM.",
          },
          correct_answer: "B",
          explanation:
            "To give a large language model (LLM) the ability to use content from previous customer messages in a conversation, the previous messages must be included in the model prompt. This technique is known as prompt engineering and allows the LLM to retain context by incorporating a history of the interaction within the prompt. By appending prior exchanges to the prompt, the model can generate contextually relevant and coherent responses throughout the multi-turn conversation.",
          id: 141,
        },
        {
          number: "141",
          question:
            "A company’s employees provide product descriptions and recommendations to customers when customers call the customer service center. These recommendations are based on where the customers are located. The company wants to use foundation models (FMs) to automate this process. Which AWS service meets these requirements?",
          options: {
            A: "Amazon Macie",
            B: "Amazon Transcribe",
            C: "Amazon Bedrock",
            D: "Amazon Textract",
          },
          correct_answer: "C",
          explanation:
            "Amazon Bedrock enables companies to use foundation models (FMs) to build and automate tasks like generating product descriptions and recommendations. It allows the integration of pre-trained FMs into applications without managing infrastructure, making it an ideal choice for automating customer service tasks. With Amazon Bedrock, the company can leverage FMs to generate tailored recommendations based on customer locations, enabling dynamic and efficient customer interactions.",
          id: 142,
        },
        {
          number: "142",
          question:
            "A company wants to upload customer service email messages to Amazon S3 to develop a business analysis application. The messages sometimes contain sensitive data. The company wants to receive an alert every time sensitive information is found. Which solution fully automates the sensitive information detection process with the LEAST development effort?",
          options: {
            A: "Configure Amazon Macie to detect sensitive information in the documents that are uploaded to Amazon",
            B: "Use Amazon SageMaker endpoints to deploy a large language model (LLM) to redact sensitive data.",
            C: "Develop multiple regex patterns to detect sensitive data. Expose the regex patterns on an Amazon",
            D: "Ask the customers to avoid sharing sensitive information in their email messages.",
          },
          correct_answer: "A",
          explanation:
            "Amazon Macie is a fully managed data security and privacy service that uses machine learning to discover and protect sensitive data in Amazon S3. It can automatically detect sensitive information, such as personally identifiable information (PII) or financial data, and send alerts when such data is found. This approach minimizes development effort, as it does not require custom regex patterns or model development, and it is specifically designed to handle the scenario described.",
          id: 143,
        },
        {
          number: "143",
          type: "dropdown_multiple",
          question:
            "Select the correct prompting technique from the following list for each scenario. Each prompting technique should be selected one time.",
          options: [
            "Chain-of-thought reasoning",
            "Few-shot learning",
            "Zero-shot learning",
          ],
          scenarios: [
            {
              id: "scenario_1",
              text: '"Classify the following text as either sports, politics, or entertainment. [input text]."',
            },
            {
              id: "scenario_2",
              text: '"A [image 1], [image 2], and [image 3] are examples of [target class]. Classify the following image as [target class]."',
            },
            {
              id: "scenario_3",
              text: '"[Question] [Instructions to follow.] Think step by step and walk me through your thinking."',
            },
          ],
          correct_answer: {
            scenario_1: "Zero-shot learning",
            scenario_2: "Few-shot learning",
            scenario_3: "Chain-of-thought reasoning",
          },
          explanation:
            "Zero-shot learning is used when the model is asked to perform a task without any examples, such as directly classifying text into categories. Few-shot learning provides a few examples (like images 1, 2, and 3) to help the model understand the task before asking it to classify new input. Chain-of-thought reasoning is employed when the model is prompted to explain its reasoning step by step, encouraging a more transparent and logical thought process.",
          id: 144,
        },
        {
          number: "144",
          type: "dropdown_multiple",
          question:
            "A company is using a generative AI model to develop a digital assistant. The model's responses occasionally include undesirable and potentially harmful content. Select the correct Amazon Bedrock filter policy from the following list for each mitigation action. Each filter policy should be selected one time.",
          options: [
            "Content filters",
            "Contextual grounding check",
            "Denied topics",
            "Word filters",
          ],
          scenarios: [
            {
              id: "scenario_1",
              text: "Block input prompts or model responses that contain harmful content such as hate, insults, violence, or misconduct",
            },
            {
              id: "scenario_2",
              text: "Avoid subjects related to illegal investment advice or legal advice",
            },
            {
              id: "scenario_3",
              text: "Detect and block specific offensive terms",
            },
            {
              id: "scenario_4",
              text: "Detect and filter out information in the model's responses that is not grounded in the provided source information",
            },
          ],
          correct_answer: {
            scenario_1: "Content filters",
            scenario_2: "Denied topics",
            scenario_3: "Word filters",
            scenario_4: "Contextual grounding check",
          },
          explanation:
            "Block input prompts or model responses that contain harmful content such as hate, insults, violence, or misconduct - Content filters: This filter is designed to detect and block harmful content such as hate speech, insults, violence, or misconduct in both user inputs and model responses. Avoid subjects related to illegal investment advice or legal advice - Denied topics: This guardrail allows you to define specific topics that the model should avoid discussing, such as illegal investment advice or legal advice, preventing the model from engaging with restricted subject matter. Detect and block specific offensive terms - Word filters: Word filters enable you to create custom lists of specific words or phrases that should be blocked, allowing precise control over offensive or inappropriate language. Detect and filter out information in the model's responses that is not grounded in the provided source information - Contextual grounding check: This feature helps ensure that the model's responses are based on the provided source information, reducing hallucinations and improving accuracy by filtering out ungrounded information.",
          id: 145,
        },
        {
          number: "145",
          question:
            "Which option is a benefit of using Amazon SageMaker Model Cards to document AI models?",
          options: {
            A: "Providing a visually appealing summary of a model’s capabilities.",
            B: "Standardizing information about a model’s purpose, performance, and limitations.",
            C: "Reducing the overall computational requirements of a model.",
            D: "Physically storing models for archival purposes.",
          },
          correct_answer: "B",
          explanation:
            "Amazon SageMaker Model Cards provide a structured way to document key details about AI models, including their intended use, performance metrics, and limitations. This helps organizations maintain transparency, compliance, and governance in AI model development, making it easier to track and manage models over time.",
          id: 146,
        },
        {
          number: "146",
          question:
            "What does an F1 score measure in the context of foundation model (FM) performance?",
          options: {
            A: "Model precision and recall",
            B: "Model speed in generating responses",
            C: "Financial cost of operating the model",
            D: "Energy efficiency of the model’s computations",
          },
          correct_answer: "A",
          explanation:
            "The F1 score is a metric used to evaluate the performance of a classification model by considering both precision (the proportion of correctly predicted positive cases out of all predicted positives) and recall (the proportion of correctly predicted positive cases out of all actual positives). It is the harmonic mean of precision and recall, ensuring a balance between them, especially when dealing with imbalanced datasets.",
          id: 147,
        },
        {
          number: "147",
          question:
            "A company deployed an AI/ML solution to help customer service agents respond to frequently asked questions. The questions can change over time. The company wants to give customer service agents the ability to ask questions and receive automatically generated answers to common customer questions.  Which strategy will meet these requirements MOST cost-effectively? ",
          options: {
            A: "Fine-tune the model regularly.",
            B: "Train the model by using context data.",
            C: "Pre-train and benchmark the model by using context data.",
            D: "Use Retrieval Augmented Generation (RAG) with prompt engineering techniques.",
          },
          correct_answer: "D",
          explanation:
            "Retrieval Augmented Generation (RAG) is a cost-effective approach that enhances AI-generated responses by retrieving relevant information from external knowledge sources. Instead of fine-tuning or re-training a model, RAG dynamically pulls the most recent and relevant data at query time. This is particularly useful in scenarios where questions change over time, ensuring that the AI/ML solution provides accurate and up-todate responses without requiring expensive and time-consuming model retraining. Prompt engineering techniques further optimize how the model processes and generates responses, improving accuracy and relevance.",
          id: 148,
        },
        {
          number: "148",
          question:
            "A company built an AI-powered resume screening system. The company used a large dataset to train the model. The dataset contained resumes that were not representative of all demographics. Which core dimension of responsible AI does this scenario present?",
          options: {
            A: "Fairness",
            B: "Explainability",
            C: "Privacy and security",
            D: "Transparency",
          },
          correct_answer: "A",
          explanation:
            "The scenario describes a dataset that is not representative of all demographics, which can lead to biased model predictions. This directly relates to fairness, a core dimension of responsible AI that ensures AI systems make unbiased and equitable decisions across different demographic groups. Addressing fairness involves techniques such as balanced dataset curation, bias detection, and mitigation strategies to ensure that the AI system does not discriminate against any group.",
          id: 149,
        },
        {
          number: "149",
          question:
            "A global financial company has developed an ML application to analyze stock market data and provide stock market trends. The company wants to continuously monitor the application development phases and to ensure that company policies and industry regulations are followed. Which AWS services will help the company assess compliance requirements? (Choose two.)",
          options: {
            A: "AWS Audit Manager",
            B: "AWS Config",
            C: "Amazon Inspector",
            D: "Amazon CloudWatch",
            E: "AWS CloudTrail",
          },
          correct_answer: "AB",
          explanation:
            "AWS Audit Manager helps organizations continuously assess and audit compliance with industry regulations and internal policies by automating evidence collection and generating audit reports. This is essential for ensuring that the ML application meets regulatory requirements. AWS Config enables continuous monitoring and compliance checks by tracking configuration changes in AWS resources. It helps the company ensure that infrastructure settings align with security policies and industry standards.",
          id: 150,
        },
        {
          number: "150",
          question:
            "A company wants to improve the accuracy of the responses from a generative AI application. The application uses a foundation model (FM) on Amazon Bedrock. Which solution meets these requirements MOST cost-effectively?",
          options: {
            A: "Fine-tune the FM.",
            B: "Retrain the FM.",
            C: "Train a new FM.",
            D: "Use prompt engineering.",
          },
          correct_answer: "D",
          explanation:
            "Using prompt engineering is the most cost-effective way to improve the accuracy of responses from a generative AI application without retraining or fine-tuning the foundation model (FM). Prompt engineering involves carefully designing the input prompts to guide the model toward producing better responses, improving relevance and accuracy.",
          id: 151,
        },
        {
          number: "151",
          question:
            "A company wants to identify harmful language in the comments section of social media posts by using an ML model. The company will not use labeled data to train the model. Which strategy should the company use to identify harmful language?",
          options: {
            A: "Use Amazon Rekognition moderation.",
            B: "Use Amazon Comprehend toxicity detection.",
            C: "Use Amazon SageMaker built-in algorithms to train the model.",
            D: "Use Amazon Polly to monitor comments.",
          },
          correct_answer: "B",
          explanation:
            "Amazon Comprehend provides pre-trained NLP models, including toxicity detection, to analyze text for harmful language. Since the company does not plan to use labeled data for training, Amazon Comprehend is a suitable choice because it does not require custom training and can automatically detect toxic or harmful content in comments.",
          id: 152,
        },
        {
          number: "152",
          question:
            "A media company wants to analyze viewer behavior and demographics to recommend personalized content. The company wants to deploy a customized ML model in its production environment. The company also wants to observe if the model quality drifts over time. Which AWS service or feature meets these requirements? ",
          options: {
            A: "Amazon Rekognition",
            B: "Amazon SageMaker Clarify",
            C: "Amazon Comprehend",
            D: "Amazon SageMaker Model Monitor",
          },
          correct_answer: "D",
          explanation:
            "Amazon SageMaker Model Monitor continuously tracks deployed ML models in production to detect data drift, model drift, and quality degradation over time. This is essential for ensuring that the recommendation model remains accurate as viewer behavior and demographics change. Model Monitor helps detect anomalies and provides alerts when model performance deviates from expected trends, allowing the company to take corrective action.",
          id: 153,
        },
        {
          number: "153",
          question:
            "A company is deploying AI/ML models by using AWS services. The company wants to offer transparency into the models’ decision-making processes and provide explanations for the model outputs. Which AWS service or feature meets these requirements?",
          options: {
            A: "Amazon SageMaker Model Cards",
            B: "Amazon Rekognition",
            C: "Amazon Comprehend",
            D: "Amazon Lex",
          },
          correct_answer: "A",
          explanation:
            "Amazon SageMaker Model Cards help provide transparency into AI/ML models by documenting key details such as model purpose, training data, performance metrics, and limitations. This documentation enables organizations to explain model outputs and decision-making processes, ensuring accountability and compliance with responsible AI principles.",
          id: 154,
        },
        {
          number: "154",
          question:
            "A manufacturing company wants to create product descriptions in multiple languages. Which AWS service will automate this task?",
          options: {
            A: "Amazon Translate",
            B: "Amazon Transcribe",
            C: "Amazon Kendra",
            D: "Amazon Polly",
          },
          correct_answer: "A",
          explanation:
            "Amazon Translate is an AWS service that automates language translation using neural machine translation (NMT). It enables businesses to generate product descriptions in multiple languages quickly and accurately, making it the best choice for this task.",
          id: 155,
        },
        {
          number: "155",
          type: "dropdown_multiple",
          question:
            "A company wants more customized responses to its generative AI models' prompts. Select the correct customization methodology from the following list for each use case. Each use case should be selected one time.",
          options: [
            "Continued pre-training",
            "Data augmentation",
            "Model fine-tuning",
          ],
          scenarios: [
            {
              id: "scenario_1",
              text: "The models must be taught a new domain-specific task",
            },
            {
              id: "scenario_2",
              text: "A limited amount of labeled data is available and more data is needed",
            },
            {
              id: "scenario_3",
              text: "Only unlabeled data is available",
            },
          ],
          correct_answer: {
            scenario_1: "Model fine-tuning",
            scenario_2: "Data augmentation",
            scenario_3: "Continued pre-training",
          },
          explanation:
            "The models must be taught a new domain-specific task - Model fine-tuning: Fine-tuning is the best approach when a model needs to learn a specific domain-related task. It involves adjusting a pre-trained model with a smaller, task-specific dataset to improve performance in that domain. A limited amount of labeled data is available and more data is needed - Data augmentation: Data augmentation helps in scenarios where labeled data is scarce by artificially increasing the size and variability of the dataset, improving model generalization. Only unlabeled data is available - Continued pre-training: Continued pre-training allows an existing foundation model to be further trained on domain-specific, unlabeled data to adapt to new contexts without requiring labeled data.",
          id: 156,
        },
        {
          number: "156",
          question:
            "Which AWS feature records details about ML instance data for governance and reporting?",
          options: {
            A: "Amazon SageMaker Model Cards",
            B: "Amazon SageMaker Debugger",
            C: "Amazon SageMaker Model Monitor",
            D: "Amazon SageMaker JumpStart",
          },
          correct_answer: "A",
          explanation:
            "Amazon SageMaker Model Cards record key details about machine learning models, including intended use, training data, evaluation metrics, and compliance information. They support governance and reporting by providing a standardized way to document model information throughout its lifecycle.",
          id: 157,
        },
        {
          number: "157",
          question:
            "A financial company is using ML to help with some of the company’s tasks. Which option is a use of generative AI models?",
          options: {
            A: "Summarizing customer complaints",
            B: "Classifying customers based on product usage",
            C: "Segmenting customers based on type of investments",
            D: "Forecasting revenue for certain products",
          },
          correct_answer: "A",
          explanation:
            "Generative AI models are designed to generate new content such as text, images, or audio. Summarizing customer complaints involves generating concise versions of longer texts, which is a task well-suited for generative AI models like large language models.",
          id: 158,
        },
        {
          number: "158",
          question:
            "A medical company wants to develop an AI application that can access structured patient records, extract relevant information, and generate concise summaries. Which solution will meet these requirements?",
          options: {
            A: "Use Amazon Comprehend Medical to extract relevant medical entities and relationships. Apply rulebased logic to structure and format summaries.",
            B: "Use Amazon Personalize to analyze patient engagement patterns. Integrate the output with a general purpose text summarization tool.",
            C: "Use Amazon Textract to convert scanned documents into digital text. Design a keyword extraction system to generate summaries.",
            D: "Implement Amazon Kendra to provide a searchable index for medical records. Use a template-based system to format summaries.",
          },
          correct_answer: "A",
          explanation:
            "Amazon Comprehend Medical is specifically designed to extract structured medical information (such as medication, condition, test results) from unstructured text. By applying rule-based logic afterward, relevant data can be formatted into concise summaries, meeting both the extraction and summarization needs.",
          id: 159,
        },
        {
          number: "159",
          question: "Which option describes embeddings in the context of AI?  ",
          options: {
            A: "A method for compressing large datasets",
            B: "An encryption method for securing sensitive data",
            C: "A method for visualizing high-dimensional data",
            D: "A numerical method for data representation in a reduced dimensionality space",
          },
          correct_answer: "D",
          explanation:
            "Embeddings are numerical representations of data, such as words, images, or items, in a lowerdimensional vector space. They capture semantic relationships and patterns in the data, enabling models to process and compare inputs efficiently.",
          id: 160,
        },
        {
          number: "160",
          question:
            "A company is building an AI application to summarize books of varying lengths. During testing, the application fails to summarize some books. Why does the application fail to summarize some books?",
          options: {
            A: "The temperature is set too high.",
            B: "The selected model does not support fine-tuning.",
            C: "The Top P value is too high.",
            D: "The input tokens exceed the model’s context size.",
          },
          correct_answer: "D",
          explanation:
            "Language models have a maximum context size, which limits the number of input tokens they can process at once. If a book's length exceeds this limit, the model cannot handle the full input, leading to failure in summarization.",
          id: 161,
        },
        {
          number: "161",
          question:
            "An airline company wants to build a conversational AI assistant to answer customer questions about flight schedules, booking, and payments. The company wants to use large language models (LLMs) and a knowledge base to create a text-based chatbot interface. Which solution will meet these requirements with the LEAST development effort?",
          options: {
            A: "Train models on Amazon SageMaker Autopilot.",
            B: "Develop a Retrieval Augmented Generation (RAG) agent by using Amazon Bedrock.",
            C: "Create a Python application by using Amazon Q Developer.",
            D: "Fine-tune models on Amazon SageMaker Jumpstart.",
          },
          correct_answer: "B",
          explanation:
            "Amazon Bedrock allows quick integration of large language models with a knowledge base using RAG architecture, enabling accurate and dynamic responses based on up-to-date information. This approach requires minimal development effort while leveraging powerful generative capabilities.",
          id: 162,
        },
        {
          number: "162",
          question:
            "What is tokenization used for in natural language processing (NLP)?  ",
          options: {
            A: "To encrypt text data",
            B: "To compress text files",
            C: "To break text into smaller units for processing",
            D: "To translate text between languages",
          },
          correct_answer: "C",
          explanation:
            "Tokenization is the process of dividing text into smaller units, such as words, subwords, or characters, which can then be analyzed or processed by NLP models. It is a foundational step in preparing text data for machine learning.",
          id: 163,
        },
        {
          number: "163",
          question:
            "Which option is a characteristic of transformer-based language models?",
          options: {
            A: "Transformer-based language models use convolutional layers to apply filters across an input to capture local patterns through filtered views.",
            B: "Transformer-based language models can process only text data.",
            C: "Transformer-based language models use self-attention mechanisms to capture contextual relationships.",
            D: "Transformer-based language models process data sequences one element at a time in cyclic iterations.",
          },
          correct_answer: "C",
          explanation:
            "A key characteristic of transformer models is their use of self-attention mechanisms, which allow the model to weigh the importance of different words in a sequence relative to each other, enabling a deep understanding of context and meaning across the entire input.",
          id: 164,
        },
        {
          number: "164",
          question:
            "A financial company is using AI systems to obtain customer credit scores as part of the loan application process. The company wants to expand to a new market in a different geographic area. The company must ensure that it can operate in that geographic area. Which compliance laws should the company review?",
          options: {
            A: "Local health data protection laws",
            B: "Local payment card data protection laws",
            C: "Local education privacy laws",
            D: "Local algorithm accountability laws",
          },
          correct_answer: "D",
          explanation:
            "When deploying AI systems that affect individuals, such as credit scoring, companies must comply with local algorithm accountability laws. These laws regulate the fairness, transparency, and impact of automated decision-making, ensuring ethical use of AI in new geographic regions.",
          id: 165,
        },
        {
          number: "165",
          question:
            "A company uses Amazon Bedrock for its generative AI application. The company wants to use Amazon Bedrock Guardrails to detect and filter harmful user inputs and model-generated outputs. Which content categories can the guardrails filter? (Choose two.)",
          options: {
            A: "Hate",
            B: "Politics",
            C: "Violence",
            D: "Gambling",
            E: "Religion",
          },
          correct_answer: "AC",
          explanation:
            "Amazon Bedrock Guardrails provide configurable content filters to detect and block harmful content in generative AI applications. Specifically, they include filters for categories such as Hate and Violence, among others. These filters can be applied to both user inputs and model-generated outputs to ensure that the AI application adheres to responsible AI practices and organizational policies.",
          id: 166,
        },
        {
          number: "166",
          question:
            "Which scenario describes a potential risk and limitation of prompt engineering in the context of a generative AI model?",
          options: {
            A: "Prompt engineering does not ensure that the model always produces consistent and deterministic outputs, eliminating the need for validation.",
            B: "Prompt engineering could expose the model to vulnerabilities such as prompt injection attacks.",
            C: "Properly designed prompts reduce but do not eliminate the risk of data poisoning or model hijacking.",
            D: "Prompt engineering does not ensure that the model will consistently generate highly reliable outputs when working with real-world data.",
          },
          correct_answer: "B",
          explanation:
            "A key risk in prompt engineering is prompt injection, where attackers manipulate input prompts to alter a model's behavior or produce unintended outputs. This vulnerability arises from the model's sensitivity to input structure and content, making it a critical limitation in secure prompt design.",
          id: 167,
        },
        {
          number: "167",
          question:
            "A publishing company built a Retrieval Augmented Generation (RAG) based solution to give its users the ability to interact with published content. New content is published daily. The company wants to provide a near real-time experience to users. Which steps in the RAG pipeline should the company implement by using offline batch processing to meet these requirements? (Choose two.)",
          options: {
            A: "Generation of content embeddings",
            B: "Generation of embeddings for user queries",
            C: "Creation of the search index",
            D: "Retrieval of relevant content",
            E: "Response generation for the user",
          },
          correct_answer: "AC",
          explanation:
            "Generation of content embeddings New or updated documents can be converted to vector representations in periodic batches (e.g., hourly, nightly). Because a document’s embedding remains fixed until the document itself changes, this step does not have to run for every user query, making it ideal for offline processing. Creation of the search index After new embeddings are produced, the vector (or hybrid) index can be rebuilt or incrementally updated in batch mode. Index construction is computationally intensive but happens infrequently relative to user requests, so running it offline avoids adding latency to real-time interactions.",
          id: 168,
        },
        {
          number: "168",
          question:
            "Which technique breaks a complex task into smaller subtasks that are sent sequentially to a large language model (LLM)?",
          options: {
            A: "One-shot prompting",
            B: "Prompt chaining",
            C: "Tree of thoughts",
            D: "Retrieval Augmented Generation (RAG)",
          },
          correct_answer: "B",
          explanation:
            "Prompt chaining involves breaking a complex task into smaller, manageable subtasks and passing them sequentially to a large language model. Each step builds upon the previous one, enabling more structured reasoning and improved task execution.",
          id: 169,
        },
        {
          number: "169",
          question:
            "An AI practitioner needs to improve the accuracy of a natural language generation model. The model uses rapidly changing inventory data. Which technique will improve the model's accuracy?",
          options: {
            A: "Transfer learning",
            B: "Federated learning",
            C: "Retrieval Augmented Generation (RAG)",
            D: "One-shot prompting",
          },
          correct_answer: "C",
          explanation:
            "RAG enhances a language model by fetching up-to-date, domain-specific data (e.g., current inventory) at inference time and conditioning the generation on those facts, ensuring the output reflects the latest information and improving accuracy without retraining the core model.",
          id: 170,
        },
        {
          number: "170",
          question:
            "A company wants to collaborate with several research institutes to develop an AI model. The company needs standardized documentation of model version tracking and a record of model development. Which solution meets these requirements?",
          options: {
            A: "Track the model changes by using Git.",
            B: "Track the model changes by using Amazon Fraud Detector.",
            C: "Track the model changes by using Amazon SageMaker Model Cards.",
            D: "Track the model changes by using Amazon Comprehend.",
          },
          correct_answer: "C",
          explanation:
            "SageMaker Model Cards provide a built-in framework for capturing and versioning key metadata, such as training data details, performance metrics, lineage, and intended use, for each model iteration. This standardized documentation and history of development fulfills requirements for model version tracking and auditable records.",
          id: 171,
        },
        {
          number: "171",
          question:
            "A company that uses multiple ML models wants to identify changes in original model quality so that the company can resolve any issues. Which AWS service or feature meets these requirements?",
          options: {
            A: "Amazon SageMaker JumpStart",
            B: "Amazon SageMaker HyperPod",
            C: "Amazon SageMaker Data Wrangler",
            D: "Amazon SageMaker Model Monitor",
          },
          correct_answer: "D",
          explanation:
            "SageMaker Model Monitor continuously measures data and prediction quality in production, detects deviations from your model’s baseline (such as data drift or accuracy degradation), and alerts you so you can investigate and resolve issues promptly.",
          id: 172,
        },
        {
          number: "172",
          question:
            "What is the purpose of chunking in Retrieval Augmented Generation (RAG)?",
          options: {
            A: "To avoid database storage limitations for large text documents by storing parts or chunks of the text",
            B: "To improve efficiency by avoiding the need to convert large text into vector embeddings",
            C: "To improve the contextual relevancy of results retrieved from the vector index",
            D: "To decrease the cost of storage by storing parts or chunks of the text",
          },
          correct_answer: "C",
          explanation:
            "By splitting large documents into smaller, semantically coherent chunks, the retrieval system can match and return only the most context-relevant segments for a given query, enhancing the precision and accuracy of the generated responses.",
          id: 173,
        },
        {
          number: "173",
          question:
            "A company is developing an editorial assistant application that uses generative AI. During the pilot phase, usage is low and application performance is not a concern. The company cannot predict application usage after the application is fully deployed and wants to minimize application costs. Which solution will meet these requirements?",
          options: {
            A: "Use GPU-powered Amazon EC2 instances.",
            B: "Use Amazon Bedrock with Provisioned Throughput.",
            C: "Use Amazon Bedrock with On-Demand Throughput.",
            D: "Use Amazon SageMaker JumpStart.",
          },
          correct_answer: "C",
          explanation:
            "On-Demand Throughput in Bedrock charges only for the inference calls you actually make, with no upfront capacity commitment. This lets you start with minimal pilot usage and elastically handle unpredictable future load while keeping costs as low as possible.",
          id: 174,
        },
        {
          number: "174",
          question:
            "A company deployed a Retrieval Augmented Generation (RAG) application on Amazon Bedrock that gathers financial news to distribute in daily newsletters. Users have recently reported politically influenced ideas in the newsletters.   Which Amazon Bedrock guardrail can identify and filter this content?",
          options: {
            A: "Word filters",
            B: "Denied topics",
            C: "Sensitive information filters",
            D: "Content filters",
          },
          correct_answer: "B",
          explanation:
            "The Denied topics guardrail lets you define high-level categories (such as politics) that the model must avoid. Bedrock enforces these rules during generation, filtering out any content related to politically influenced ideas.",
          id: 175,
        },
        {
          number: "175",
          question:
            "A financial company is developing a fraud detection system that flags potential fraud cases in credit card transactions. Employees will evaluate the flagged fraud cases. The company wants to minimize the amount of time the employees spend reviewing flagged fraud cases that are not actually fraudulent. Which evaluation metric meets these requirements?",
          options: {
            A: "Recall",
            B: "Accuracy",
            C: "Precision",
            D: "Lift chart",
          },
          correct_answer: "C",
          explanation:
            "Precision measures the proportion of flagged cases that are truly fraudulent (TP / [TP + FP]). Maximizing precision reduces the number of false positives employees must review, cutting down wasted effort on nonfraudulent cases.",
          id: 176,
        },
        {
          number: "176",
          question:
            "A company designed an AI-powered agent to answer customer inquiries based on product manuals. Which strategy can improve customer confidence levels in the AI-powered agent's responses?",
          options: {
            A: "Writing the confidence level in the response",
            B: "Including referenced product manual links in the response",
            C: "Designing an agent avatar that looks like a computer",
            D: "Training the agent to respond in the company's language style",
          },
          correct_answer: "B",
          explanation:
            "Providing direct links to the exact sections of the product manual that support each answer lets customers verify and trust the information, boosting confidence in the AI agent’s responses.",
          id: 177,
        },
        {
          number: "177",
          question:
            "A hospital developed an AI system to provide personalized treatment recommendations for patients. The AI system must provide the rationale behind the recommendations and make the insights accessible to doctors and patients.     Which human-centered design principle does this scenario present?",
          options: {
            A: "Explainability",
            B: "Privacy and security",
            C: "Fairness",
            D: "Data governance",
          },
          correct_answer: "A",
          explanation:
            "Explainability ensures that the AI system reveals the reasoning behind its recommendations in an understandable way, making insights transparent and accessible to both doctors and patients.",
          id: 178,
        },
        {
          number: "178",
          question:
            "Which statement presents an advantage of using Retrieval Augmented Generation (RAG) for natural language processing (NLP) tasks?",
          options: {
            A: "RAG can use external knowledge sources to generate more accurate and informative responses.",
            B: "RAG is designed to improve the speed of language model training.",
            C: "RAG is primarily used for speech recognition tasks.",
            D: "RAG is a technique for data augmentation in computer vision tasks.",
          },
          correct_answer: "A",
          explanation:
            "By retrieving relevant documents or data at inference time and conditioning the generation on that external knowledge, RAG enriches model outputs with up-to-date, domain-specific information, boosting accuracy and informativeness without retraining the core model.",
          id: 179,
        },
        {
          number: "179",
          question:
            "A company has created a custom model by fine-tuning an existing large language model (LLM) from Amazon Bedrock. The company wants to deploy the model to production and use the model to handle a steady rate of requests each minute. Which solution meets these requirements MOST cost-effectively?",
          options: {
            A: "Deploy the model by using an Amazon EC2 compute optimized instance.",
            B: "Use the model with on-demand throughput on Amazon Bedrock.",
            C: "Store the model in Amazon S3 and host the model by using AWS Lambda.",
            D: "Purchase Provisioned Throughput for the model on Amazon Bedrock.",
          },
          correct_answer: "D",
          explanation:
            "Provisioned Throughput is priced lower per request when you have a predictable, steady volume of calls. By committing to a fixed throughput level, you secure the necessary capacity at a reduced unit cost compared to on-demand, making it the most cost-effective choice for steady-minute usage.",
          id: 180,
        },
        {
          number: "180",
          question:
            "Which technique involves training AI models on labeled datasets to adapt the models to specific industry terminology and requirements? ",
          options: {
            A: "Data augmentation",
            B: "Fine-tuning",
            C: "Model quantization",
            D: "Continuous pre-training",
          },
          correct_answer: "B",
          explanation:
            "Fine-tuning takes a pre-trained model and continues training it on a labeled, domain-specific dataset so the model learns industry terminology and task nuances directly from that specialized data.",
          id: 181,
        },
        {
          number: "181",
          question:
            "A company is creating an agent for its application by using Amazon Bedrock Agents. The agent is performing well, but the company wants to improve the agent’s accuracy by providing some specific examples. Which solution meets these requirements?",
          options: {
            A: "Modify the advanced prompts for the agent to include the examples.",
            B: "Create a guardrail for the agent that includes the examples.",
            C: "Use Amazon SageMaker Ground Truth to label the examples.",
            D: "Run a script in AWS Lambda that adds the examples to the training dataset.",
          },
          correct_answer: "A",
          explanation:
            "Embedding specific input–output examples directly into the agent’s advanced prompt (few-shot prompting) guides the model toward more accurate behavior without retraining or additional tooling.",
          id: 182,
        },
        {
          number: "182",
          question:
            "Which option is a benefit of using infrastructure as code (IaC) in machine learning operations (MLOps)?",
          options: {
            A: "IaC eliminates the need for hyperparameter tuning.",
            B: "IaC always provisions powerful compute instances, contributing to the training of more accurate models.",
            C: "IaC streamlines the deployment of scalable and consistent ML workloads in cloud environments.",
            D: "IaC minimizes overall expenses by deploying only low-cost instances.",
          },
          correct_answer: "C",
          explanation:
            "By describing infrastructure in code, teams can version, automate, and repeatably provision resources, ensuring that ML training and inference environments are consistent, scalable, and easy to reproduce across development, testing, and production.",
          id: 183,
        },
        {
          number: "183",
          question:
            "A company wants to fine-tune a foundation model (FM) to answer questions for a specific domain. The company wants to use instruction-based fine-tuning. How should the company prepare the training data?",
          options: {
            A: "Gather company internal documents and industry-specific materials. Merge the documents and materials into a single file.",
            B: "Collect external company reviews from various online sources. Manually label each review as either positive or negative.",
            C: "Create pairs of questions and answers that specifically address topics related to the company's industry domain.",
            D: "Create few-shot prompts to instruct the model to answer only domain knowledge.",
          },
          correct_answer: "C",
          explanation:
            "Instruction-based fine-tuning requires a dataset of instruction–response examples. By curating question– answer pairs focused on the company’s domain, you teach the model exactly how to interpret domainspecific queries and generate the correct responses during inference.",
          id: 184,
        },
        {
          number: "184",
          question:
            "Which ML technique ensures data compliance and privacy when training AI models on AWS?",
          options: {
            A: "Reinforcement learning",
            B: "Transfer learning",
            C: "Federated learning",
            D: "Unsupervised learning",
          },
          correct_answer: "C",
          explanation:
            "Federated learning lets you train a global model across multiple data holders (for example, different AWS accounts or edge devices) without moving raw data to a central location. Each participant trains locally on its own private dataset and only shares model updates, preserving data privacy and ensuring compliance.",
          id: 185,
        },
        {
          number: "185",
          type: "dropdown_multiple",
          question:
            "A company needs to customize a base model that is hosted on Amazon Bedrock. Select the correct model customization method from the following list of company requirements. Each model customization method should be selected one or more times. ",
          options: ["Continued pre-training", "Fine-tuning"],
          scenarios: [
            {
              id: "scenario_1",
              text: "The company wants to improve the model's performance on specific tasks and examples.",
            },
            {
              id: "scenario_2",
              text: "The company wants to improve the model's domain knowledge by providing specific documents.",
            },
            {
              id: "scenario_3",
              text: "The company wants to retrain the model by using more unlabeled data over time.",
            },
          ],
          correct_answer: {
            scenario_1: "Fine-tuning",
            scenario_2: "Continued pre-training",
            scenario_3: "Continued pre-training",
          },
          explanation:
            "Fine-tuning is the process of taking a pre-trained model and further training it on a smaller, task-specific dataset. This approach is ideal when you want to improve the model's performance on specific tasks and examples, as it allows the model to adapt to particular use cases while retaining the general knowledge from pre-training. Continued pre-training involves further training a pre-trained model on additional unlabeled data, typically from a specific domain or with updated information. This method is suitable for improving the model's domain knowledge by providing specific documents or retraining with more unlabeled data over time, as it helps the model better understand domain-specific language and concepts without losing its general capabilities.",
          id: 186,
        },
        {
          number: "186",
          question:
            "A manufacturing company has an application that ingests consumer complaints from publicly available sources. The application uses complex hard-coded logic to process the complaints. The company wants to scale this logic across markets and product lines. Which advantage do generative AI models offer for this scenario?",
          options: {
            A: "Predictability of outputs",
            B: "Adaptability",
            C: "Less sensitivity to changes in inputs",
            D: "Explainability",
          },
          correct_answer: "B",
          explanation:
            "Generative AI models can generalize learned patterns to new domains and formats with minimal manual reconfiguration. This adaptability lets you extend complaint-processing logic across different markets and product lines without rewriting complex hard-coded rules.",
          id: 187,
        },
        {
          number: "187",
          question:
            "A financial company wants to flag all credit card activity as possibly fraudulent or non-fraudulent based on transaction data. Which type of ML model meets these requirements?",
          options: {
            A: "Regression",
            B: "Diffusion",
            C: "Binary classification",
            D: "Multi-class classification",
          },
          correct_answer: "C",
          explanation:
            "Binary classification models are designed to distinguish between two classes - fraudulent versus nonfraudulent transactions - making them the appropriate choice for this use case.",
          id: 188,
        },
        {
          number: "188",
          type: "dropdown_multiple",
          question:
            "A company is designing a customer service chatbot by using a fine-tuned large language model (LLM). The company wants to ensure that the chatbot uses responsible AI characteristics.   Select the correct responsible AI characteristic from the following list for each application design action. Each responsible AI characteristic should be selected one time or not at all. ",
          options: [
            "Governance",
            "Privacy and security",
            "Safety",
            "Transparency",
          ],
          scenarios: [
            {
              id: "scenario_1",
              text: "Anonymize personal information during training data preparation",
            },
            {
              id: "scenario_2",
              text: "Design the customer service chatbot to provide explainable decisions",
            },
            {
              id: "scenario_3",
              text: "Use Amazon Bedrock Guardrails to prevent harmful output and misuse of the chatbot",
            },
          ],
          correct_answer: {
            scenario_1: "Privacy and security",
            scenario_2: "Transparency",
            scenario_3: "Safety",
          },
          explanation:
            "Anonymize personal information during training data preparation - Privacy and security: This action directly addresses protecting user privacy by ensuring personally identifiable information (PII) is removed or anonymized from training data. Design the customer service chatbot to provide explainable decisions - Transparency: Providing explainable decisions ensures users understand how and why the chatbot arrives at specific responses, which is a core principle of transparency in responsible AI. Use Amazon Bedrock Guardrails to prevent harmful output and misuse of the chatbot - Safety: Implementing guardrails helps prevent the generation of harmful, biased, or inappropriate content, ensuring the chatbot operates safely for all users.",
          id: 189,
        },
        {
          number: "189",
          question:
            "A hospital wants to use a generative AI solution with speech-to-text functionality to help improve employee skills in dictating clinical notes. Which AWS service meets these requirements? ",
          options: {
            A: "Amazon Q Developer",
            B: "Amazon Polly",
            C: "Amazon Rekognition",
            D: "AWS HealthScribe",
          },
          correct_answer: "D",
          explanation:
            "AWS HealthScribe is specifically designed for healthcare workflows, providing accurate speech-to-text transcription of clinical conversations and generating structured clinical notes, meeting the hospital’s need to improve employee dictation of clinical documentation.",
          id: 190,
        },
        {
          number: "190",
          question: "Which type of AI model makes numeric predictions?",
          options: {
            A: "Diffusion",
            B: "Regression",
            C: "Transformer",
            D: "Multi-modal",
          },
          correct_answer: "B",
          explanation:
            "Regression models are designed to predict continuous numerical values (for example, forecasting sales figures or estimating house prices), making them the appropriate choice for numeric predictions.",
          id: 191,
        },
        {
          number: "191",
          type: "dropdown_multiple",
          question:
            "A company wants to use Amazon SageMaker features for various use cases. Select the correct SageMaker feature from the following list for each use case. Each SageMaker feature should be selected one time or not at all. ",
          options: [
            "SageMaker Canvas",
            "SageMaker Feature Store",
            "SageMaker Ground Truth",
            "SageMaker JumpStart",
            "SageMaker Model Monitor",
          ],
          scenarios: [
            {
              id: "scenario_1",
              text: "Preparing data through a visual interface without using code",
            },
            {
              id: "scenario_2",
              text: "Finding and using a prebuilt solution for fraud detection",
            },
            {
              id: "scenario_3",
              text: "Create labeled datasets with human intervention",
            },
          ],
          correct_answer: {
            scenario_1: "SageMaker Canvas",
            scenario_2: "SageMaker JumpStart",
            scenario_3: "SageMaker Ground Truth",
          },
          explanation:
            "SageMaker Canvas provides a visual interface for data preparation and model building without writing code. SageMaker JumpStart offers prebuilt solutions and models for common use cases like fraud detection. SageMaker Ground Truth enables creation of labeled datasets with human annotators for training ML models.",
          id: 192,
        },
        {
          number: "192",
          question:
            "What is the purpose of vector embeddings in a large language model (LLM)?   ",
          options: {
            A: "Splitting text into manageable pieces of data",
            B: "Grouping a set of characters to be treated as a single unit",
            C: "Providing the ability to mathematically compare texts",
            D: "Providing the count of every word in the input",
          },
          correct_answer: "C",
          explanation:
            "Embeddings convert text into high-dimensional vectors that capture semantic relationships, allowing you to compute distances or similarities between pieces of text for tasks like retrieval or clustering.",
          id: 193,
        },
        {
          number: "193",
          question:
            "A company wants to fine-tune a foundation model (FM) by using AWS services. The company needs to ensure that its data stays private, safe, and secure in the source AWS Region where the data is stored. Which combination of steps will meet these requirements MOST cost-effectively? (Choose two.)",
          options: {
            A: "Host the model on premises by using AWS Outposts.",
            B: "Use the Amazon Bedrock API.",
            C: "Use AWS PrivateLink and a VPC.",
            D: "Host the Amazon Bedrock API on premises.",
            E: "Use Amazon CloudWatch logs and metrics.",
          },
          correct_answer: "BC",
          explanation:
            "Use the Amazon Bedrock API: You’ll call Bedrock’s managed fine-tuning endpoints directly in your AWS Region, so your data never leaves that region. Use AWS PrivateLink and a VPC: Front Bedrock API traffic through a VPC endpoint via PrivateLink to keep all network traffic on the AWS backbone and within your account’s private network.",
          id: 194,
        },
        {
          number: "194",
          question:
            "A financial company uses AWS to host its generative AI models. The company must generate reports to show adherence to international regulations for handling sensitive customer data. Which AWS service meets these requirements?",
          options: {
            A: "Amazon Macie",
            B: "AWS Artifact",
            C: "AWS Secrets Manager",
            D: "AWS Config",
          },
          correct_answer: "B",
          explanation:
            "AWS Artifact provides on-demand access to AWS’s compliance reports and certifications (for example, ISO, SOC, GDPR), enabling the company to demonstrate its generative AI workloads and data handling practices adhere to international regulatory requirements.",
          id: 195,
        },
        {
          number: "195",
          question:
            "A medical company wants to modernize its onsite information processing application. The company wants to use generative AI to respond to medical questions from patients.     Which AWS service should the company use to ensure responsible AI for the application?",
          options: {
            A: "Guardrails for Amazon Bedrock",
            B: "Amazon Inspector",
            C: "Amazon Rekognition",
            D: "AWS Trusted Advisor",
          },
          correct_answer: "A",
          explanation:
            "Guardrails for Amazon Bedrock let you define and enforce safety, compliance, and bias controls on generative AI workloads. By embedding these guardrails into your application, you can ensure that patientfacing responses meet medical accuracy, privacy, and regulatory requirements without having to build custom monitoring or filtering pipelines yourself.",
          id: 196,
        },
        {
          number: "196",
          question:
            "Which metric is used to evaluate the performance of foundation models (FMs) for text summarization tasks?",
          options: {
            A: "F1 score",
            B: "Bilingual Evaluation Understudy (BLEU) score",
            C: "Accuracy",
            D: "Mean squared error (MSE)",
          },
          correct_answer: "A",
          explanation:
            "Text summarization quality is most commonly assessed using ROUGE metrics, which report recall, precision, and an F1-score that balances the two. The ROUGE-F1 (often simply called the F1 score in this context) measures how well the model’s summary overlaps with reference summaries, making it the standard choice for evaluating foundation models on summarization tasks.",
          id: 197,
        },
        {
          number: "197",
          question:
            "What is the benefit of fine-tuning a foundation model (FM)?",
          options: {
            A: "Fine-tuning reduces the FM's size and complexity and enables slower inference.",
            B: "Fine-tuning uses specific training data to retrain the FM from scratch to adapt to a specific use case.",
            C: "Fine-tuning keeps the FM's knowledge up to date by pre-training the FM on more recent data.",
            D: "Fine-tuning improves the performance of the FM on a specific task by further training the FM on new",
          },
          correct_answer: "D",
          explanation:
            "By updating the model’s weights with task-relevant examples, fine-tuning sharpens its understanding of domain-specific language and objectives, yielding higher accuracy and relevance on that particular task.",
          id: 198,
        },
        {
          number: "198",
          question:
            "A company wants to improve its chatbot's responses to match the company's desired tone. The company has 100 examples of high-quality conversations between customer service agents and customers. The company wants to use this data to incorporate company tone into the chatbot's responses.  Which solution meets these requirements?  ",
          options: {
            A: "Use Amazon Personalize to generate responses.",
            B: "Create an Amazon SageMaker HyperPod pre-training job.",
            C: "Host the model by using Amazon SageMaker. Use TensorRT for large language model (LLM)",
            D: "Create an Amazon Bedrock fine-tuning job.",
          },
          correct_answer: "D",
          explanation:
            "Bedrock’s fine-tuning lets you adapt a foundation model on your own conversation examples—in this case, the 100 high-quality transcripts to instill your company’s tone directly into the model’s response generation.",
          id: 199,
        },
        {
          number: "199",
          question:
            "An ecommerce company is using a chatbot to automate the customer order submission process. The chatbot is powered by AI and is available to customers directly from the company's website 24 hours a day, 7 days a week. Which option is an AI system input vulnerability that the company needs to resolve before the chatbot is made available?",
          options: {
            A: "Data leakage",
            B: "Prompt injection",
            C: "Large language model (LLM) hallucinations",
            D: "Concept drift",
          },
          correct_answer: "B",
          explanation:
            "Prompt injection is an input-level attack where a user crafts malicious input to override or manipulate the chatbot’s instructions or behavior. Addressing this vulnerability by sanitizing and constraining user prompts ensures the chatbot won’t execute unintended or harmful instructions when it goes live.",
          id: 200,
        },
        {
          number: "200",
          question:
            "A social media company wants to prevent users from posting discriminatory content on the company's application. The company wants to use Amazon Bedrock as part of the solution. How can the company use Amazon Bedrock to meet these requirements?",
          options: {
            A: "Give users the ability to interact based on user preferences.",
            B: "Block interactions related to predefined topics.",
            C: "Restrict user conversations to predefined topics.",
            D: "Provide a variety of responses to select from for user engagement.",
          },
          correct_answer: "B",
          explanation:
            "Amazon Bedrock’s built-in content moderation lets you define categories or keywords (e.g., hate speech, discriminatory language) and automatically block or filter any user inputs or outputs that fall under those topics. By configuring Bedrock’s safety filters with your predefined “disallowed” categories, the application will prevent discriminatory content from ever being posted.",
          id: 201,
        },
        {
          number: "201",
          question:
            "An education company waftion. The application will give users the ability to enter text or provide a picture of a question. The application will respond with a written answer and an explanation of the written answer.",
          options: {
            A: "Computer vision model",
            B: "Large multi-modal language model",
            C: "Diffusion model",
            D: "Text-to-speech model",
          },
          correct_answer: "B",
          explanation:
            "A large multi-modal language model can natively ingest both text and images as inputs and generate text outputs, making it ideal for a system that accepts typed questions or photos of questions and returns written answers with explanations.",
          id: 202,
        },
        {
          number: "202",
          question:
            "In which stage of the generative AI model lifecycle are tests performed to examine the model's accuracy?",
          options: {
            A: "Deployment",
            B: "Data selection",
            C: "Fine-tuning",
            D: "Evaluation",
          },
          correct_answer: "D",
          explanation:
            "The evaluation stage is when you run tests and benchmarks, such as accuracy, precision, and other performance metrics, to measure how well the generative AI model performs on hold-out or validation data before moving on to deployment.",
          id: 203,
        },
        {
          number: "203",
          question:
            "Which statement correctly describes embeddings in generative AI?",
          options: {
            A: "Embeddings represent data as high-dimensional vectors that capture semantic relationships.",
            B: "Embeddings is a technique that searches data to find the most helpful information to answer natural language questions.",
            C: "Embeddings reduce the hardware requirements of a model by using a less precise data type for the weights and activations.",
            D: "Embeddings provide the ability to store and retrieve data for generative AI applications.",
          },
          correct_answer: "A",
          explanation:
            "Embeddings map inputs like words, sentences, or documents into continuous vector spaces where semantic similarity corresponds to geometric proximity, enabling models to reason about meaning and relationships mathematically.",
          id: 204,
        },
        {
          number: "204",
          question:
            "A company wants to add generative AI functionality to its application by integrating a large language model (LLM). The responses from the LLM must be as deterministic and as stable as possible. Which solution meets these requirements?",
          options: {
            A: "Configure the application to automatically set the temperature parameter to 0 when submitting the prompt to the LLM.",
            B: 'Configure the application to automatically add "make your response deterministic" at the end of the prompt before submitting the prompt to the LLM.',
            C: 'Configure the application to automatically add "make your response deterministic" at the beginning of the prompt before submitting the prompt to the LLM.',
            D: "Configure the application to automatically set the temperature parameter to 1 when submitting the prompt to the LLM.",
          },
          correct_answer: "A",
          explanation:
            "The temperature hyperparameter directly controls the randomness of the LLM’s token sampling: setting it to 0 forces greedy decoding, yielding the most likely next token every time and thus fully deterministic, stable outputs.",
          id: 205,
        },
        {
          number: "205",
          question:
            "A company needs to select a generative AI model to build an application. The application must provide responses to users in real time. Which model characteristic should the company consider to meet these requirements?",
          options: {
            A: "Model complexity",
            B: "Innovation speed",
            C: "Inference speed",
            D: "Training time",
          },
          correct_answer: "C",
          explanation:
            "For real-time applications, the model’s inference speed - ability to generate responses with low latency - is the critical characteristic to ensure users receive answers promptly.",
          id: 206,
        },
        {
          number: "206",
          question:
            "Which term refers to the instructions given to foundation models (FMs) so that the FMs provide a more accurate response to a question?",
          options: {
            A: "Prompt",
            B: "Direction",
            C: "Dialog",
            D: "Translation",
          },
          correct_answer: "A",
          explanation:
            "A prompt is the input instruction or query you provide to a foundation model to guide its response, ensuring the model understands the task and delivers a more accurate answer.",
          id: 207,
        },
        {
          number: "207",
          question:
            "A retail company wants to build an ML model to recommend products to customers. The company wants to build the model based on responsible practices. Which practice should the company apply when collecting data to decrease model bias?",
          options: {
            A: "Use data from only customers who match the demographics of the company's overall customer base.",
            B: "Collect data from customers who have a past purchase history.",
            C: "Ensure that the data is balanced and collected from a diverse group.",
            D: "Ensure that the data is from a publicly available dataset.",
          },
          correct_answer: "C",
          explanation:
            "By gathering training data that reflects the full spectrum of your customer population - across demographics, behaviors, and preferences - you reduce skew toward any one subgroup and help the recommender treat all users equitably.",
          id: 208,
        },
        {
          number: "208",
          question:
            "A company is developing an ML model to predict customer churn. Which evaluation metric will assess the model's performance on a binary classification task such as predicting churn?",
          options: {
            A: "F1 score",
            B: "Mean squared error (MSE)",
            C: "R-squared",
            D: "Time used to train the model",
          },
          correct_answer: "A",
          explanation:
            "For binary classification tasks like churn prediction, the F1 score balances precision and recall into a single metric, making it ideal for evaluating how well the model identifies churners without over- or under-predicting.",
          id: 209,
        },
        {
          number: "209",
          question:
            "An AI practitioner is evaluating the performance of an Amazon SageMaker model. The AI practitioner must choose a performance metric. The metric must show the ratio of the number of correctly classified items to the total number of correctly and incorrectly classified items. Which metric meets these requirements?",
          options: {
            A: "Accuracy",
            B: "Precision",
            C: "F1 score",
            D: "Recall",
          },
          correct_answer: "A",
          explanation:
            "Accuracy is defined as the number of correctly classified items (both true positives and true negatives) divided by the total number of items evaluated (the sum of true positives, true negatives, false positives, and false negatives), matching exactly the ratio described.",
          id: 210,
        },
        {
          number: "210",
          question:
            "An ecommerce company receives multiple gigabytes of customer data daily. The company uses the data to train an ML model to forecast future product demand. The company needs a solution to perform inferences once each day. Which inference type meets these requirements?",
          options: {
            A: "Batch inference",
            B: "Asynchronous inference",
            C: "Real-time inference",
            D: "Serverless inference",
          },
          correct_answer: "A",
          explanation:
            "Batch inference is designed for high-volume, scheduled predictions: you can point it at the day’s gigabytes of data, run a job once daily, and generate all forecasts in one go without needing a persistent endpoint.",
          id: 211,
        },
        {
          number: "211",
          question:
            "A company has developed a generative AI model for customer segmentation. The model has been deployed in the company's production environment for a long time. The company recently noticed some inconsistency in the model's responses. The company wants to evaluate model bias and drift. Which AWS service or feature meets these requirements?",
          options: {
            A: "Amazon SageMaker Model Monitor",
            B: "Amazon SageMaker Clarify",
            C: "Amazon SageMaker Model Cards",
            D: "Amazon SageMaker Feature Store",
          },
          correct_answer: "A",
          explanation:
            "SageMaker Model Monitor continuously collects inference and ground-truth data to automatically detect data drift, concept drift, and bias shifts in production models. By configuring baseline metrics and thresholds, the company can track inconsistencies in responses over time and receive alerts whenever bias or drift exceed acceptable limits.",
          id: 212,
        },
        {
          number: "212",
          question:
            "A company has signed up for Amazon Bedrock access to build applications. The company wants to restrict employee access to specific models available on Amazon Bedrock. Which solution meets these requirements?",
          options: {
            A: "Use AWS Identity and Access Management (IAM) policies to restrict model access.",
            B: "Use AWS Security Token Service (AWS STS) to generate temporary credentials for model use.",
            C: "Use AWS Identity and Access Management (IAM) service roles to restrict model subscription.",
            D: "Use Amazon Inspector to monitor model access.",
          },
          correct_answer: "A",
          explanation:
            "With IAM policies you can grant or deny Bedrock API actions scoped to specific model ARNs, ensuring employees can only invoke the exact models you authorize.",
          id: 213,
        },
        {
          number: "213",
          question:
            "Which ML technique uses training data that is labeled with the correct output values?  ",
          options: {
            A: "Supervised learning",
            B: "Unsupervised learning",
            C: "Reinforcement learning",
            D: "Transfer learning",
          },
          correct_answer: "A",
          explanation:
            "Supervised learning trains models on datasets where each example includes both input features and the correct output label, allowing the algorithm to learn the mapping from inputs to known targets.",
          id: 214,
        },
        {
          number: "214",
          question:
            "Which large language model (LLM) parameter controls the number of possible next words or tokens considered at each step of the text generation process?",
          options: {
            A: "Maximum tokens",
            B: "Top K",
            C: "Temperature",
            D: "Batch size",
          },
          correct_answer: "B",
          explanation:
            "The Top K parameter limits token sampling at each generation step to the K most probable next tokens. By choosing how many of the highest-probability candidates to consider, Top K directly controls the breadth of options the model evaluates when generating each token.",
          id: 215,
        },
        {
          number: "215",
          question:
            "A company is making a chatbot. The chatbot uses Amazon Lex and Amazon OpenSearch Service. The chatbot uses the company's private data to answer questions. The company needs to convert the data into a vector representation before storing the data in a database. Which type of foundation model (FM) meets these requirements?",
          options: {
            A: "Text completion model",
            B: "Instruction following model",
            C: "Text embeddings model",
            D: "Image generation model",
          },
          correct_answer: "C",
          explanation:
            "Text embeddings models transform input text into high-dimensional vector representations, making them ideal for storing and retrieving your private data in a vector database for the chatbot.",
          id: 216,
        },
        {
          number: "216",
          question:
            "A company wants to use a large language model (LLM) to generate product descriptions. The company wants to give the model example descriptions that follow a format. Which prompt engineering technique will generate descriptions that match the format?",
          options: {
            A: "Zero-shot prompting",
            B: "Chain-of-thought prompting",
            C: "One-shot prompting",
            D: "Few-shot prompting",
          },
          correct_answer: "D",
          explanation:
            "Few-shot prompting provides the model with several example product descriptions that follow your desired format, enabling it to learn the pattern and generate new descriptions in the same style without further training.",
          id: 217,
        },
        {
          number: "217",
          question:
            "A bank is fine-tuning a large language model (LLM) on Amazon Bedrock to assist customers with questions about their loans. The bank wants to ensure that the model does not reveal any private customer data. Which solution meets these requirements?",
          options: {
            A: "Use Amazon Bedrock Guardrails.",
            B: "Remove personally identifiable information (PII) from the customer data before fine-tuning the LLM.",
            C: "Increase the Top-K parameter of the LLM.",
            D: "Store customer data in Amazon S3. Encrypt the data before fine-tuning the LLM.",
          },
          correct_answer: "B",
          explanation:
            "Preprocessing your fine-tuning dataset to strip out names, account numbers, and other PII prevents the model from ever ingesting - or later regurgitating - private customer information. This data sanitization step is the most reliable way to guarantee that sensitive details aren’t embedded into the model.",
          id: 218,
        },
        {
          number: "218",
          question:
            "A grocery store wants to create a chatbot to help customers find products in the store. The chatbot must check the inventory in real time and provide the product location in the store. Which prompt engineering technique should the store use to build the chatbot?",
          options: {
            A: "Zero-shot prompting",
            B: "Few-shot prompting",
            C: "Least-to-most prompting",
            D: "Reasoning and acting (ReAct) prompting",
          },
          correct_answer: "D",
          explanation:
            "ReAct prompting interleaves the model’s chain-of-thought reasoning with explicit “actions” (such as calling your real-time inventory API), then uses the API response to inform its final answer. By structuring your prompts to have the LLM think, choose the “CheckInventory” action with the product name, receive the live location data, and then respond to the user, you seamlessly integrate real-time lookups and precise product locations into the chatbot’s replies.",
          id: 219,
        },
        {
          number: "219",
          question:
            "A company uses a third-party model on Amazon Bedrock to analyze confidential documents. The company is concerned about data privacy. Which statement describes how Amazon Bedrock protects data privacy?",
          options: {
            A: "User inputs and model outputs are anonymized and shared with third-party model providers.",
            B: "User inputs and model outputs are not shared with any third-party model providers.",
            C: "User inputs are kept confidential, but model outputs are shared with third-party model providers.",
            D: "User inputs and model outputs are redacted before the inputs and outputs are shared with third-party",
          },
          correct_answer: "B",
          explanation:
            "Amazon Bedrock processes your data entirely within AWS’s secure environment and does not forward your inputs or the generated outputs back to the model vendor, ensuring that your confidential documents remain private.",
          id: 220,
        },
        {
          number: "220",
          question:
            "An animation company wants to provide subtitles for its content. Which AWS service meets this requirement?",
          options: {
            A: "Amazon Comprehend",
            B: "Amazon Polly",
            C: "Amazon Transcribe",
            D: "Amazon Translate",
          },
          correct_answer: "C",
          explanation:
            "Amazon Transcribe converts spoken audio into accurate, time-stamped text transcripts, making it the ideal service for generating subtitles from your animation audio tracks.",
          id: 221,
        },
        {
          number: "221",
          question:
            "An ecommerce company wants to group customers based on their purchase history and preferences to personalize the user experience of the company's application. Which ML technique should the company use?",
          options: {
            A: "Classification",
            B: "Clustering",
            C: "Regression",
            D: "Content generation",
          },
          correct_answer: "B",
          explanation:
            "Clustering is an unsupervised learning technique that automatically groups data points - in this case, customers with similar purchase histories and preferences - into segments without needing predefined labels, enabling personalized experiences.",
          id: 222,
        },
        {
          number: "222",
          question:
            "A company wants to control employee access to publicly available foundation models (FMs). Which solution meets these requirements?",
          options: {
            A: "Analyze cost and usage reports in AWS Cost Explorer.",
            B: "Download AWS security and compliance documents from AWS Artifact.",
            C: "Configure Amazon SageMaker JumpStart to restrict discoverable FMs.",
            D: "Build a hybrid search solution by using Amazon OpenSearch Service.",
          },
          correct_answer: "C",
          explanation:
            "  SageMaker JumpStartlets you curate which publicly available foundation models appear inyour account’s model catalog. By using JumpStart’s employees from discovering or deploying any FMs you haven’t explicitly allowed.",
          id: 223,
        },
        {
          number: "223",
          question:
            "A company has set up a translation tool to help its customer service team handle issues from customers around the world. The company wants to evaluate the performance of the translation tool. The company sets up a parallel data process that compares the responses from the tool to responses from actual humans. Both sets of responses are generated on the same set of documents. Which strategy should the company use to evaluate the translation tool?",
          options: {
            A: "Use the Bilingual Evaluation Understudy (BLEU) score to estimate the absolute translation quality of the two methods.",
            B: "Use the Bilingual Evaluation Understudy (BLEU) score to estimate the relative translation quality of the two methods.",
            C: "Use the BERTScore to estimate the absolute translation quality of the two methods.",
            D: "Use the BERTScore to estimate the relative translation quality of the two methods.",
          },
          correct_answer: "B",
          explanation:
            "BLEU measures n-gram overlap between a candidate translation and one or more reference translations. When you apply it to both the tool’s outputs and the human outputs on the same source documents, you get comparable scores that let you directly assess which approach yields better translations.",
          id: 224,
        },
        {
          number: "224",
          question:
            "An AI practitioner wants to generate more diverse and more creative outputs from a large language model (LLM). How should the AI practitioner adjust the inference parameter?",
          options: {
            A: "Increase the temperature value.",
            B: "Decrease the Top K value.",
            C: "Increase the response length.",
            D: "Decrease the prompt length.",
          },
          correct_answer: "A",
          explanation:
            "Raising the temperature softens the model’s probability distribution, increasing the chance of sampling lesslikely tokens and thus producing more varied and creative responses.",
          id: 225,
        },
        {
          number: "225",
          question:
            "A company has developed custom computer vision models. The company needs a user-friendly interface for data labeling to minimize model mistakes on new real-world data. Which AWS service, feature, or tool meets these requirements?",
          options: {
            A: "Amazon SageMaker Ground Truth",
            B: "Amazon SageMaker Canvas",
            C: "Amazon Bedrock playground",
            D: "Amazon Bedrock Agents",
          },
          correct_answer: "A",
          explanation:
            "SageMaker Ground Truth provides a built-in, easy-to-use labeling console with workflows and quality controls specifically for computer vision tasks, enabling you to efficiently generate high-quality labeled data and reduce model errors on real-world images.",
          id: 226,
        },
        {
          number: "226",
          question:
            "A company is integrating AI into its employee recruitment and hiring solution. The company wants to mitigate bias risks and ensure responsible AI practices while prioritizing equitable hiring decisions. Which core dimensions of responsible AI should the company consider? (Choose two.)",
          options: {
            A: "Fairness",
            B: "Tolerance",
            C: "Flexibility",
            D: "Open source",
            E: "Transparency",
          },
          correct_answer: "AE",
          explanation:
            "Fairness - Ensuring equitable outcomes across all demographic groups helps mitigate bias risks in hiring decisions. Transparency - Providing clear explanations of how the AI makes decisions builds trust and allows auditing for bias.",
          id: 227,
        },
        {
          number: "227",
          question:
            "A financial company has deployed an ML model to predict customer churn. The model has been running in production for 1 week. The company wants to evaluate how accurately the model predicts churn compared to actual customer behavior. Which metric meets these requirements?",
          options: {
            A: "Root mean squared error (RMSE)",
            B: "Return on investment (ROI)",
            C: "F1 score",
            D: "Bilingual Evaluation Understudy (BLEU) score",
          },
          correct_answer: "C",
          explanation:
            "The F1 score combines precision (how many predicted churners actually churned) and recall (how many actual churners were correctly identified) into a single metric, making it ideal for measuring your model’s accuracy on the binary churn‐ prediction task.",
          id: 228,
        },
        {
          number: "228",
          question:
            "A company has a generative AI application that uses a pre-trained foundation model (FM) on Amazon Bedrock. The company wants the FM to include more context by using company information. Which solution meets these requirements MOST cost-effectively?",
          options: {
            A: "Use Amazon Bedrock Knowledge Bases.",
            B: "Choose a different FM on Amazon Bedrock.",
            C: "Use Amazon Bedrock Agents.",
            D: "Deploy a custom model on Amazon Bedrock.",
          },
          correct_answer: "A",
          explanation:
            "Amazon Bedrock Knowledge Bases let you index and retrieve your company’s documents at inference time, injecting relevant context into the pre-trained FM’s prompts without the cost or complexity of re-training or fine-tuning a custom model. This retrieval-augmented approach delivers up-to-date, domain-specific information in responses while keeping costs low.",
          id: 229,
        },
        {
          number: "229",
          type: "dropdown_multiple",
          question:
            "A company is using Amazon SageMaker to develop AI models. Select the correct SageMaker feature or resource from the following list for each step in the AI model lifecycle workflow. Each SageMaker feature or resource should be selected one time or not at all. ",
          options: [
            "SageMaker Clarify",
            "SageMaker Model Registry",
            "SageMaker Serverless Inference",
          ],
          scenarios: [
            {
              id: "scenario_1",
              text: "Managing different versions of the model",
            },
            {
              id: "scenario_2",
              text: "Using the current model to make predictions",
            },
          ],
          correct_answer: {
            scenario_1: "SageMaker Model Registry",
            scenario_2: "SageMaker Serverless Inference",
          },
          explanation:
            "Managing different versions of the model - SageMaker Model Registry: Amazon SageMaker Model Registry is a centralized repository to manage model versions. It helps you catalog models, track their lineage, and maintain versioning throughout the ML lifecycle. Using the current model to make predictions - SageMaker Serverless Inference: SageMaker Serverless Inference is an inference option that automatically provisions, scales, and turns off compute capacity based on the volume of inference requests, making it ideal for making predictions with the current model without managing infrastructure.",
          id: 230,
        },
        {
          number: "230",
          question:
            "A food service company wants to collect a dataset to predict customer food preferences. The company wants to ensure that the food preferences of all demographics are included in the data. Which dataset characteristic does this scenario present?",
          options: {
            A: "Accuracy",
            B: "Diversity",
            C: "Recency bias",
            D: "Reliability",
          },
          correct_answer: "B",
          explanation:
            "Ensuring representation of all demographic groups addresses the dataset’s diversity, so the model learns from a wide range of customer preferences.",
          id: 231,
        },
        {
          number: "231",
          question:
            "A company wants to create a chatbot that answers questions about human resources policies. The company is using a large language model (LLM) and has a large digital documentation base. Which technique should the company use to optimize the generated responses?",
          options: {
            A: "Use Retrieval Augmented Generation (RAG).",
            B: "Use few-shot prompting.",
            C: "Set the temperature to 1.",
            D: "Decrease the token size.",
          },
          correct_answer: "A",
          explanation:
            "RAG lets the chatbot pull in precise, up-to-date passages from your HR documentation at inference time, grounding its answers in the actual policy text and ensuring accuracy without overloading the LLM’s context window.",
          id: 232,
        },
        {
          number: "232",
          question:
            "An education company is building a chatbot whose target audience is teenagers. The company is training a custom large language model (LLM). The company wants the chatbot to speak in the target audience's language style by using creative spelling and shortened words. Which metric will assess the LLM's performance?",
          options: {
            A: "F1 score",
            B: "BERTScore",
            C: "Recall-Oriented Understudy for Gisting Evaluation (ROUGE)",
            D: "Bilingual Evaluation Understudy (BLEU) score",
          },
          correct_answer: "D",
          explanation:
            "BLEU evaluates surface-level n-gram overlap between the LLM’s outputs and reference examples—in this case, targets written in creative spelling and shorthand, making it well suited to measure how closely the model’s style matches the teenager-oriented language.",
          id: 233,
        },
        {
          number: "233",
          question:
            "A customer service team is developing an application to analyze customer feedback and automatically classify the feedback into different categories. The categories include product quality, customer service, and delivery experience. Which AI concept does this scenario present?",
          options: {
            A: "Computer vision",
            B: "Natural language processing (NLP)",
            C: "Recommendation systems",
            D: "Fraud detection",
          },
          correct_answer: "B",
          explanation:
            "Automatically analyzing and classifying free-text feedback into thematic categories is a core NLP task (text classification).",
          id: 234,
        },
        {
          number: "234",
          question:
            "A financial services company must ensure that its generative AI-powered chatbot provides factual responses for regulatory compliance. Which solution prevents the underlying foundation model (FM) from hallucinating?",
          options: {
            A: "Use AWS Config to query compliance metadata by using natural language.",
            B: "Configure Amazon Bedrock Guardrails to evaluate user inputs and model responses.",
            C: "Use Amazon Fraud Detector to detect potentially fraudulent online activities.",
            D: "Use AWS Audit Manager to prepare IT audit and compliance reports.",
          },
          correct_answer: "B",
          explanation:
            "Amazon Bedrock Guardrails allows you to define rules and policies that evaluate and constrain both user inputs and model responses. By configuring guardrails, you can prevent the generative AI model from producing hallucinated or non-factual outputs, ensuring the chatbot adheres to compliance requirements and provides factual responses. This directly addresses the need to control hallucinations in generative AI systems for regulatory compliance.",
          id: 235,
        },
        {
          number: "235",
          type: "dropdown_multiple",
          question:
            "A company wants to develop a solution that uses generative AI to create content for product advertisements, including sample images and slogans. Select the correct model type from the following list for each action. Each model type should be selected one time.",
          options: [
            "Diffusion model",
            "Object detection model",
            "Transformer-based model",
          ],
          scenarios: [
            {
              id: "scenario_1",
              text: "Create high-quality images that are influenced by the generated slogans and product",
            },
            {
              id: "scenario_2",
              text: "Create contextually relevant slogans based on the advertisement product",
            },
            {
              id: "scenario_3",
              text: "Ensure that company brand elements are properly placed in the images",
            },
          ],
          correct_answer: {
            scenario_1: "Diffusion model",
            scenario_2: "Transformer-based model",
            scenario_3: "Object detection model",
          },
          explanation:
            "Create high-quality images that are influenced by the generated slogans and product -> Diffusion model. Diffusion models are state-of-the-art for high-quality image generation. Create contextually relevant slogans based on the advertisement product -> Transformer-based model. Transformer-based models excel at natural language processing, making them suitable for generating contextually relevant slogans. Ensure that company brand elements are properly placed in the images -> Object detection model. Object detection models are designed to identify and ensure correct placement of specific objects (e.g., brand elements) within images.",
          id: 236,
        },
        {
          number: "236",
          question:
            "A company has created multiple ML models. The company needs a solution for storing, managing, and versioning the models. Which AWS service or feature meets these requirements?",
          options: {
            A: "AWS Audit Manager",
            B: "Amazon SageMaker Model Monitor",
            C: "Amazon SageMaker Model Registry",
            D: "Amazon SageMaker Canvas",
          },
          correct_answer: "C",
          explanation:
            "Amazon SageMaker Model Registry is the AWS service designed to store, manage, and version machine learning models. It provides a central repository for tracking model versions and managing their deployment status throughout the ML lifecycle.",
          id: 237,
        },
        {
          number: "237",
          question:
            "An AI practitioner is building an ML model. The AI practitioner wants to provide model transparency and explainability to stakeholders. Which solution will meet these requirements?",
          options: {
            A: "Present the model Shapley values.",
            B: "Provide the model accuracy measure.",
            C: "Provide the model confusion matrix.",
            D: "Provide a secure model inference endpoint.",
          },
          correct_answer: "A",
          explanation:
            "Presenting the model Shapley values provides transparency and explainability by showing how each feature contributes to individual predictions, helping stakeholders understand the reasoning behind the model’s outputs.",
          id: 238,
        },
        {
          number: "238",
          question:
            "A company is developing an ML application. The application must automatically group similar customers and products based on their characteristics. Which ML strategy should the company use to meet these requirements?",
          options: {
            A: "Unsupervised learning",
            B: "Supervised learning",
            C: "Reinforcement learning",
            D: "Semi-supervised learning",
          },
          correct_answer: "A",
          explanation:
            "Unsupervised learning is used to automatically group or cluster similar customers and products based on their characteristics without the need for labeled data. This strategy fits scenarios where the goal is to discover patterns or groupings in the data.",
          id: 239,
        },
        {
          number: "239",
          question:
            "A news agency publishes articles in English. The agency wants to make articles available in other languages.   Which solution meets these requirements?",
          options: {
            A: "Add Amazon Transcribe to the company’s website.",
            B: "Use the Amazon Translate real-time translation feature.",
            C: "Add Amazon Personalize to the company’s website.",
            D: "Use the Amazon Textract real-time document processing feature.",
          },
          correct_answer: "B",
          explanation:
            "Amazon Translate provides real-time translation of text, making it suitable for automatically translating articles from English into other languages for publication.",
          id: 240,
        },
        {
          number: "240",
          question:
            "A bank is building a chatbot to answer customer questions about opening a bank account. The chatbot will use public bank documents to generate responses. The company will use Amazon Bedrock and prompt engineering to improve the chatbot’s responses. Which prompt engineering technique meets these requirements?",
          options: {
            A: "Complexity-based prompting",
            B: "Zero-shot prompting",
            C: "Few-shot prompting",
            D: "Directional stimulus prompting",
          },
          correct_answer: "C",
          explanation:
            "Few-shot prompting involves providing the generative AI model with several examples (drawn from public bank documents) to guide it in generating more accurate and relevant responses. This technique helps the chatbot better align its answers with the desired content and format.",
          id: 241,
        },
        {
          number: "241",
          question:
            "A company wants to fine-tune an ML model that is hosted on Amazon Bedrock. The company wants to use its own sensitive data that is stored in private databases in a VPC. The data needs to stay within the company’s private network. Which solution will meet these requirements?",
          options: {
            A: "Restrict access to Amazon Bedrock by using an AWS Identity and Access Management (IAM) service",
            B: "Restrict access to Amazon Bedrock by using an AWS Identity and Access Management (IAM) resource",
            C: "Use AWS PrivateLink to connect the VPC and Amazon Bedrock.",
            D: "Use AWS Key Management Service (AWS KMS) keys to encrypt the data.",
          },
          correct_answer: "C",
          explanation:
            "AWS PrivateLink enables secure, private connectivity between your VPC and Amazon Bedrock without exposing data to the public internet, ensuring that sensitive data stays within your private network during fine-tuning.",
          id: 242,
        },
        {
          number: "242",
          question:
            "A documentary filmmaker wants to reach more viewers. The filmmaker wants to automatically add subtitles and voice-overs in multiple languages to their films. Which combination of steps will meet these requirements? (Choose two.)",
          options: {
            A: "Use Amazon Transcribe and Amazon Translate to generate subtitles in other languages.",
            B: "Use Amazon Textract and Amazon Translate to generate subtitles in other languages.",
            C: "Use Amazon Polly to generate voice-overs in other languages.",
            D: "Use Amazon Translate to generate voice-overs in other languages.",
          },
          correct_answer: "AC",
          explanation:
            "Use Amazon Transcribe and Amazon Translate to generate subtitles in other languages: Amazon Transcribe converts spoken dialogue to text (subtitles), and Amazon Translate can then translate these subtitles into multiple languages. Use Amazon Polly to generate voice-overs in other languages: Amazon Polly converts translated text into lifelike speech, enabling the creation of multilingual voice-overs.",
          id: 243,
        },
        {
          number: "243",
          question:
            "A company wants to create a chatbot to answer employee questions about company policies. Company policies are updated frequently. The chatbot must reflect the changes in near real time. The company wants to choose a large language model (LLM). Which solution meets these requirements?",
          options: {
            A: "Fine-tune an LLM on the company policy text by using Amazon SageMaker.",
            B: "Select a foundation model (FM) from Amazon Bedrock to build an application.",
            C: "Create a Retrieval Augmented Generation (RAG) workflow by using Amazon Bedrock Knowledge",
            D: "Use Amazon Q Business to build a custom Q App.",
          },
          correct_answer: "C",
          explanation:
            "A RAG workflow with Amazon Bedrock Knowledge Bases allows the chatbot to access the most recent company policy documents dynamically, ensuring responses reflect policy updates in near real time without the need to retrain or fine-tune the LLM each time content changes.",
          id: 244,
        },
        {
          number: "244",
          question:
            "A company is using supervised learning to train an AI model on a small labeled dataset that is specific to a target task. Which step of the foundation model (FM) lifecycle does this describe?",
          options: {
            A: "Fine-tuning",
            B: "Data selection",
            C: "Pre-training",
            D: "Evaluation",
          },
          correct_answer: "A",
          explanation:
            "Fine-tuning is the process of training a pre-existing foundation model on a smaller, labeled dataset that is specific to a target task, allowing the model to adapt to the company's requirements.",
          id: 245,
        },
        {
          number: "245",
          type: "dropdown_multiple",
          question:
            "A company is developing an AI application to help the company approve or deny personal loans. The application must follow the principles of responsible AI. Select the correct responsible AI principle from the following list for each action. Select each responsible AI principle one time or not at all. ",
          options: [
            "Explainability",
            "Fairness",
            "Privacy and security",
            "Robustness",
            "Safety",
          ],
          scenarios: [
            {
              id: "scenario_1",
              text: "Encrypt the application data, and isolate the application on a private network.",
            },
            {
              id: "scenario_2",
              text: "Evaluate how different population groups will be impacted.",
            },
            {
              id: "scenario_3",
              text: "Test the application with unexpected data to ensure the application will work in unique situations.",
            },
          ],
          correct_answer: {
            scenario_1: "Privacy and security",
            scenario_2: "Fairness",
            scenario_3: "Robustness",
          },
          explanation:
            "Encrypt the application data, and isolate the application on a private network -> Privacy and security: Protecting data and networks is part of privacy and security. Evaluate how different population groups will be impacted -> Fairness: Assessing impacts on different groups ensures fairness. Test the application with unexpected data to ensure the application will work in unique situations -> Robustness: Testing with unexpected data checks the application's robustness.",
          id: 246,
        },
        {
          number: "246",
          question:
            "A company is introducing a new feature for its application. The feature will refine the style of output messages. The company will fine-tune a large language model (LLM) on Amazon Bedrock to implement the feature. Which type of data does the company need to meet these requirements?",
          options: {
            A: "Samples of only input messages",
            B: "Samples of only output messages",
            C: "Samples of pairs of input and output messages",
            D: "Separate samples of input and output messages",
          },
          correct_answer: "C",
          explanation:
            "Fine-tuning a large language model to refine the style of output messages requires training data consisting of paired input and output messages. These pairs allow the model to learn the relationship between the input provided and the desired styled output, ensuring the fine-tuned model produces responses with the intended style.",
          id: 247,
        },
        {
          number: "247",
          question:
            "A healthcare company is building an AI solution to predict patient readmission within 30 days of patient discharge. The company has trained a model on historical patient data including medical history, demographics, and treatment specifications, to provide readmission predictions in real time. Which task describes AI model inference in this scenario?",
          options: {
            A: "Gather historical patient readmission data.",
            B: "Use appropriate metrics and assess model performance.",
            C: "Use data to identify patient patterns and correlations.",
            D: "Use a trained model to predict patient readmission.",
          },
          correct_answer: "D",
          explanation:
            "Model inference refers to the process of applying a trained model to new data—in this case, using the trained model to make real-time predictions about whether a patient will be readmitted within 30 days of discharge.",
          id: 248,
        },
        {
          number: "248",
          question:
            "A financial company wants to build workflows for human review of ML predictions. The company wants to define confidence thresholds for its use case and adjust the thresholds over time. Which AWS service meets these requirements?",
          options: {
            A: "Amazon Personalize",
            B: "Amazon Augmented AI (Amazon A2I)",
            C: "Amazon Inspector",
            D: "AWS Audit Manager",
          },
          correct_answer: "B",
          explanation:
            "Amazon Augmented AI (Amazon A2I) enables you to build workflows for human review of machine learning predictions, allowing you to define and adjust confidence thresholds for when human intervention is required.",
          id: 249,
        },
        {
          number: "249",
          question:
            "A company wants to develop an AI assistant for employees to query internal data. Which AWS service will meet this requirement?",
          options: {
            A: "Amazon Rekognition",
            B: "Amazon Textract",
            C: "Amazon Lex",
            D: "Amazon Q Business",
          },
          correct_answer: "D",
          explanation:
            "Amazon Q Business is designed to build generative AI assistants for querying and interacting with internal organizational data, making it the ideal service for creating an AI assistant for employees to access company information.",
          id: 250,
        },
        {
          number: "250",
          question:
            "A company wants to build and deploy ML models on AWS without writing any code. Which AWS service or feature meets these requirements?",
          options: {
            A: "Amazon SageMaker Canvas",
            B: "Amazon Rekognition",
            C: "AWS DeepRacer",
            D: "Amazon Comprehend",
          },
          correct_answer: "A",
          explanation:
            "Amazon SageMaker Canvas allows users to build, train, and deploy machine learning models using a nocode visual interface, meeting the requirement to create ML solutions without writing any code.",
          id: 251,
        },
        {
          number: "251",
          question:
            "A design company is using a foundation model (FM) on Amazon Bedrock to generate images for various projects. The company wants to have control over how detailed or abstract each generated image appears Which model parameter should the company modify?",
          options: {
            A: "Model checkpoint",
            B: "Batch size",
            C: "Generation step",
            D: "Token length",
          },
          correct_answer: "C",
          explanation:
            "The “generation step” parameter controls how detailed or abstract an image appears during generation. By adjusting the number of generation steps, you can influence the level of refinement and detail in the output more steps yield more detailed images, while fewer steps result in more abstract images.",
          id: 252,
        },
        {
          number: "252",
          question:
            "A financial company has offices in different countries worldwide. The company requires that all API calls between generative AI applications and foundation models (FM) must not travel across the public internet. Which AWS service should the company use?",
          options: {
            A: "AWS PrivateLink",
            B: "Amazon Q",
            C: "Amazon CloudFront",
            D: "AWS CloudTrail",
          },
          correct_answer: "A",
          explanation:
            "AWS PrivateLink provides secure, private connectivity between VPCs and AWS services, ensuring that API calls do not traverse the public internet. This meets the requirement for private communication between generative AI applications and foundation models across global offices.",
          id: 253,
        },
        {
          number: "253",
          question:
            "An ecommerce company is deploying a chatbot. The chatbot will give users the ability to ask questions about the company’s products and receive details on users’ orders. The company must implement safeguards for the chatbot to filter harmful content from the input prompts and chatbot responses. Which AWS feature or resource meets these requirements?",
          options: {
            A: "Amazon Bedrock Guardrails",
            B: "Amazon Bedrock Agents",
            C: "Amazon Bedrock inference APIs",
            D: "Amazon Bedrock custom models",
          },
          correct_answer: "A",
          explanation:
            "Amazon Bedrock Guardrails enable you to implement safeguards that filter out harmful or inappropriate content in both user input prompts and chatbot responses, ensuring safe and compliant chatbot interactions.",
          id: 254,
        },
        {
          number: "254",
          question:
            "A company wants to learn about generative AI applications in an experimental environment.   Which solution will meet this requirement MOST cost-effectively?",
          options: {
            A: "Amazon Q Developer",
            B: "Amazon SageMaker JumpStart",
            C: "Amazon Bedrock PartyRock",
            D: "Amazon Q Business",
          },
          correct_answer: "C",
          explanation:
            "Amazon Bedrock PartyRock provides a highly cost-effective and experimental environment for learning about generative AI applications, allowing users to quickly build, test, and iterate on AI apps without incurring significant costs.",
          id: 255,
        },
        {
          number: "255",
          question:
            "A company needs to collect a large dataset to train an AI assistant in a specific content area. Which dataset will meet this requirement?",
          options: {
            A: "Diverse conversations that use relevant terminology",
            B: "Time series data of general purpose historical sales",
            C: "Sentiment analysis of news articles",
            D: "Unique product IDs and corresponding user IDs",
          },
          correct_answer: "A",
          explanation:
            "To train an AI assistant for a specific content area, you need a large dataset of diverse conversations that incorporate the relevant terminology and context for that area. This enables the assistant to learn how to respond accurately and appropriately within the targeted subject.",
          id: 256,
        },
        {
          number: "256",
          question:
            "A financial company is developing a generative AI application for loan approval decisions. The company needs the application output to be responsible and fair. Which solution meets these requirements?",
          options: {
            A: "Review the training data to check for biases. Include data from all demographics in the training data.",
            B: "Use a deep learning model with many hidden layers.",
            C: "Keep the model’s decision-making process a secret to protect proprietary algorithms.",
            D: "Continuously monitor the model’s performance on a static test dataset",
          },
          correct_answer: "A",
          explanation:
            "Reviewing the training data for biases and ensuring representation from all demographics is essential for developing a responsible and fair generative AI application, especially in sensitive domains like loan approval decisions. This approach helps reduce bias and supports ethical AI practices.",
          id: 257,
        },
        {
          number: "257",
          type: "dropdown_multiple",
          question:
            "Select the correct AWS service or tool from the following list for each use case. Select each AWS service or tool one time or not at all.",
          options: [
            "Amazon SageMaker Clarify",
            "Amazon SageMaker Ground Truth",
            "Amazon Bedrock Guardrails",
            "AWS CloudTrail",
            "AWS Trusted Advisor",
          ],
          scenarios: [
            {
              id: "scenario_1",
              text: "Apply human feedback across the ML lifecycle to improve the accuracy and relevancy of models.",
            },
            {
              id: "scenario_2",
              text: "Implement safeguards that align with responsible AI policies.",
            },
            {
              id: "scenario_3",
              text: "Detect potential bias during data preparation and model training.",
            },
          ],
          correct_answer: {
            scenario_1: "Amazon SageMaker Ground Truth",
            scenario_2: "Amazon Bedrock Guardrails",
            scenario_3: "Amazon SageMaker Clarify",
          },
          explanation:
            "Apply human feedback across the ML lifecycle to improve the accuracy and relevancy of models. -> Amazon SageMaker Ground Truth Implement safeguards that align with responsible AI policies. -> Amazon Bedrock Guardrails Detect potential bias during data preparation and model training. -> Amazon SageMaker Clarify Amazon SageMaker Ground Truth is used for human-in-the-loop data labeling and model improvement. Amazon Bedrock Guardrails are designed to enforce responsible AI safeguards. Amazon SageMaker Clarify detects and helps mitigate bias during data preparation and model training.",
          id: 258,
        },
        {
          number: "258",
          question:
            "An AI practitioner who has minimal ML knowledge wants to predict employee attrition without writing code. Which Amazon SageMaker feature meets this requirement?",
          options: {
            A: "SageMaker Canvas",
            B: "SageMaker Clarify",
            C: "SageMaker Model Monitor",
            D: "SageMaker Data Wrangler",
          },
          correct_answer: "A",
          explanation:
            "SageMaker Canvas provides a no-code visual interface that allows users with minimal ML knowledge to build, train, and deploy models for predictions, such as employee attrition, without writing any code.",
          id: 259,
        },
        {
          number: "259",
          question:
            "A company is using AI to improve its services. The company needs to ensure that the AI system is fair and explainable. The company wants to require training for members of the AI system development team. Which training will meet these requirements? ",
          options: {
            A: "Training on advanced coding skills",
            B: "Training on data privacy and encryption protocols",
            C: "Training on bias awareness and responsible AI",
            D: "Training on advanced ML algorithms",
          },
          correct_answer: "C",
          explanation:
            "Training on bias awareness and responsible AI equips the development team with the knowledge and practices necessary to ensure that AI systems are fair, unbiased, and explainable, which aligns with the company’s requirements.",
          id: 260,
        },
        {
          number: "260",
          question:
            "A company has an ML model. The company wants to know how the model makes predictions. Which term refers to understanding model predictions?",
          options: {
            A: "Model interpretability",
            B: "Model training",
            C: "Model interoperability",
            D: "Model performance",
          },
          correct_answer: "A",
          explanation:
            "Model interpretability refers to understanding how a model makes its predictions, providing insights into the reasoning behind the model’s outputs.",
          id: 261,
        },
        {
          number: "261",
          question:
            "A company wants to identify groups for its customers based on the customers’ demographics and buying patterns. Which algorithm should the company use to meet this requirement?",
          options: {
            A: "K-nearest neighbors (k-NN)",
            B: "K-means",
            C: "Decision tree",
            D: "Support vector machine",
          },
          correct_answer: "B",
          explanation:
            "K-means is a clustering algorithm commonly used to identify and group customers based on shared characteristics such as demographics and buying patterns, making it suitable for customer segmentation tasks.",
          id: 262,
        },
        {
          number: "262",
          question:
            "A company is working on a large language model (LLM) and noticed that the LLM’s outputs are not as diverse as expected. Which parameter should the company adjust?",
          options: {
            A: "Temperature",
            B: "Batch size",
            C: "Learning rate",
            D: "Optimizer type",
          },
          correct_answer: "A",
          explanation:
            "The “temperature” parameter controls the randomness and diversity of outputs generated by a large language model. Increasing the temperature produces more diverse and creative responses, while lowering it makes the outputs more focused and deterministic.",
          id: 263,
        },
        {
          number: "263",
          question:
            "A company is using an Amazon Nova Canvas model to generate images. The model generates images successfully. The company needs to prevent the model from including specific items in the generated images. Which solution will meet this requirement?",
          options: {
            A: "Use a higher temperature value.",
            B: "Use a more detailed prompt.",
            C: "Use a negative prompt.",
            D: "Use another foundation model (FM).",
          },
          correct_answer: "C",
          explanation:
            "A negative prompt is used to specify items or characteristics that you want the image generation model to avoid including in the output, ensuring that specific items are excluded from generated images.",
          id: 264,
        },
        {
          number: "264",
          type: "dropdown_multiple",
          question:
            "A company uses ML techniques to build applications. Select the correct ML technique from the following list for each task. Select each ML technique one time.",
          options: [
            "Binary classification",
            "Multiclass classification",
            "Regression",
          ],
          scenarios: [
            {
              id: "scenario_1",
              text: "Analyze a text question to determine if the answer is correct.",
            },
            {
              id: "scenario_2",
              text: "Analyze ecological factors to determine the number of species in a certain area.",
            },
            {
              id: "scenario_3",
              text: "Analyze car attributes to determine the car model.",
            },
          ],
          correct_answer: {
            scenario_1: "Binary classification",
            scenario_2: "Regression",
            scenario_3: "Multiclass classification",
          },
          explanation:
            "Analyze a text question to determine if the answer is correct -> Binary classification Analyze ecological factors to determine the number of species in a certain area -> Regression Analyze car attributes to determine the car model -> Multiclass classification Determining if an answer is correct is a yes/no (binary) decision. Predicting the number of species (a numeric value) is a regression task. Identifying a car model from attributes involves choosing from multiple possible classes, which is multiclass classification.",
          id: 265,
        },
        {
          number: "265",
          question:
            "A company wants to label training datasets by using human feedback to fine-tune a foundation model (FM). The company does not want to develop labeling applications or manage a labeling workforce. Which AWS service or feature meets these requirements?",
          options: {
            A: "Amazon SageMaker Data Wrangler",
            B: "Amazon SageMaker Ground Truth Plus",
            C: "Amazon Transcribe",
            D: "Amazon Macie",
          },
          correct_answer: "B",
          explanation:
            "Amazon SageMaker Ground Truth Plus provides managed data labeling services, allowing you to label datasets using human feedback without developing labeling applications or managing a labeling workforce.",
          id: 266,
        },
        {
          number: "266",
          question:
            "An online media streaming company wants to give its customers the ability to perform natural languagebased image search and filtering. The company needs a vector database that can help with similarity searches and nearest neighbor queries. Which AWS service meets these requirements? ",
          options: {
            A: "Amazon Comprehend",
            B: "Amazon Personalize",
            C: "Amazon Polly",
            D: "Amazon OpenSearch Service",
          },
          correct_answer: "D",
          explanation:
            "Amazon OpenSearch Service supports vector search capabilities, allowing you to perform similarity searches and nearest neighbor queries - key requirements for natural language-based image search and filtering.",
          id: 267,
        },
        {
          number: "267",
          type: "dropdown_multiple",
          question:
            "A company is building an AI solution by using Amazon SageMaker AI. The company wants to use SageMaker AI features to facilitate application development. Select the correct SageMaker AI feature from the following list for each use case. Select each feature one time.",
          options: ["Clarify", "Data Wrangler", "Model Cards"],
          scenarios: [
            {
              id: "scenario_1",
              text: "Determine the most suitable model to use for a business case.",
            },
            {
              id: "scenario_2",
              text: "Prepare data through a low-code or no-code interface.",
            },
            {
              id: "scenario_3",
              text: "Identify biases or imbalances in the data.",
            },
          ],
          correct_answer: {
            scenario_1: "Model Cards",
            scenario_2: "Data Wrangler",
            scenario_3: "Clarify",
          },
          explanation:
            "Determine the most suitable model to use for a business case -> Model Cards Prepare data through a low-code or no-code interface -> Data Wrangler Identify biases or imbalances in the data -> Clarify Model Cards help summarize and communicate model characteristics for selection. Data Wrangler provides a no-code/low-code interface for data preparation. Clarify is used to detect bias and imbalances in datasets and models.",
          id: 268,
        },
        {
          number: "268",
          question:
            "A company is building a generative AI tool. The company will use internal documents to customize a foundation model (FM). Which approach will meet this requirement?",
          options: {
            A: "Classification",
            B: "Continued pre-training",
            C: "Distillation",
            D: "Regression",
          },
          correct_answer: "B",
          explanation:
            "Continued pre-training involves further training a foundation model (FM) on internal documents, allowing the model to better understand and generate content specific to the company's context and requirements. This approach customizes the FM using the company’s own data.",
          id: 269,
        },
        {
          number: "269",
          question:
            "A company is monitoring a predictive model by using Amazon SageMaker Model Monitor. The company notices data drift beyond a defined threshold. The company wants to mitigate a potentially adverse impact on the predictive model. Which solution will meet these requirements?",
          options: {
            A: "Restart the SageMaker AI endpoint.",
            B: "Adjust the monitoring sensitivity.",
            C: "Re-train the model with fresh data.",
            D: "Set up experiments tracking.",
          },
          correct_answer: "C",
          explanation:
            "When Amazon SageMaker Model Monitor detects data drift beyond a defined threshold, it indicates that the current data differs from what the model was trained on. Retraining the model with recent data ensures it adapts to new patterns and maintains predictive accuracy.",
          id: 270,
        },
        {
          number: "270",
          question:
            "A financial company uses a generative AI model to assign credit limits to new customers. The company wants to make the decision-making process of the model more transparent to its customers. Which solution meets these requirements?",
          options: {
            A: "Use a rule-based system instead of an ML model.",
            B: "Apply explainable AI techniques to show customers which factors influenced the model’s decision.",
            C: "Develop an interactive UI for customers and provide clear technical explanations about the system.",
            D: "Increase the accuracy of the model to reduce the need for transparency.",
          },
          correct_answer: "B",
          explanation:
            "Explainable AI techniques provide transparency by identifying and displaying the specific factors that influenced the generative AI model’s credit limit decisions, making the decision-making process understandable to customers.",
          id: 271,
        },
        {
          number: "271",
          question:
            "A company deployed a model to production. After 4 months, the model inference quality degraded. The company wants to receive a notification if the model inference quality degrades. The company also wants to ensure that the problem does not happen again. Which solution will meet these requirements?",
          options: {
            A: "Retrain the model. Monitor model drift by using Amazon SageMaker Clarify.",
            B: "Retrain the model. Monitor model drift by using Amazon SageMaker Model Monitor.",
            C: "Build a new model. Monitor model drift by using Amazon SageMaker Feature Store.",
            D: "Build a new model. Monitor model drift by using Amazon SageMaker JumpStart.",
          },
          correct_answer: "B",
          explanation:
            "Amazon SageMaker Model Monitor enables continuous monitoring of model inference quality and detects data/model drift in production. Setting up notifications ensures the company is alerted if quality degrades again, allowing timely intervention and retraining as needed.",
          id: 272,
        },
        {
          number: "272",
          question: "Which option is an example of unsupervised learning?",
          options: {
            A: "A model that groups customers based on their purchase history",
            B: "A model that classifies images as dogs or cats",
            C: "A model that predicts a house’s price based on various features",
            D: "A model that learns to play chess by using trial and error",
          },
          correct_answer: "A",
          explanation:
            "Grouping customers based on purchase history is an example of unsupervised learning, where the model discovers patterns or clusters in the data without using labeled outputs.",
          id: 273,
        },
        {
          number: "273",
          question:
            "A company is evaluating several large language models (LLMs) for a text summarization task. The company needs to select a metric to evaluate the quality of the summaries that the LLMs generate. Which metric will meet this requirement?",
          options: {
            A: "Recall",
            B: "Area under the ROC curve (AUC)",
            C: "Recall-Oriented Understudy for Gisting Evaluation (ROUGE)",
            D: "Mean squared error (MSE)",
          },
          correct_answer: "C",
          explanation:
            "The ROUGE metric is widely used to evaluate the quality of summaries generated by language models, as it measures the overlap between the generated summary and reference summaries in terms of recall, precision, and F1-score for n-grams and sequences.",
          id: 274,
        },
        {
          number: "274",
          question:
            "A research group wants to test different generative AI models to create research papers. The research group has defined a prompt and needs a method to assess the models’ output. The research group wants to use a team of scientists to perform the output assessments. Which solution will meet these requirements?",
          options: {
            A: "Use automatic evaluation on Amazon Personalize.",
            B: "Use content moderation on Amazon Rekognition.",
            C: "Use model evaluation on Amazon Bedrock.",
            D: "Use sentiment analysis on Amazon Comprehend.",
          },
          correct_answer: "C",
          explanation:
            "Amazon Bedrock provides model evaluation capabilities, allowing a team of scientists to manually assess and compare the outputs of different generative AI models based on defined prompts and criteria. This meets the requirement for human-in-the-loop model output evaluation.",
          id: 275,
        },
        {
          number: "275",
          type: "dropdown_multiple",
          question:
            "An ecommerce company is developing a generative AI solution to create personalized product recommendations for its application users. The company wants to track how effectively the AI solution increases product sales and user engagement in the application. Select the correct business metric from the following list for each business goal. Each business metric should be selected one time.",
          options: [
            "Average order value (AOV)",
            "Click-through rate (CTR)",
            "Retention rate",
          ],
          scenarios: [
            {
              id: "scenario_1",
              text: "Measure how engaging the product recommendations are to users",
            },
            {
              id: "scenario_2",
              text: "Determine the effect of the AI solution on the total value of user purchases",
            },
            {
              id: "scenario_3",
              text: "Assess the AI solution's ability to encourage users to return to the platform",
            },
          ],
          correct_answer: {
            scenario_1: "Click-through rate (CTR)",
            scenario_2: "Average order value (AOV)",
            scenario_3: "Retention rate",
          },
          explanation:
            "Measure how engaging the product recommendations are to users -> Click-through rate (CTR) Determine the effect of the AI solution on the total value of user purchases -> Average order value (AOV) Assess the AI solution's ability to encourage users to return to the platform -> Retention rate CTR reflects engagement with recommendations. AOV measures the impact on purchase value. Retention rate evaluates if users are returning due to the AI solution.",
          id: 276,
        },
        {
          number: "276",
          question:
            "An AI practitioner wants to evaluate ML models. The AI practitioner wants to provide explanations of model predictions to customers and stakeholders. Which AWS service or feature will meet these requirements?",
          options: {
            A: "Amazon QuickSight",
            B: "Amazon Comprehend",
            C: "AWS Trusted Advisor",
            D: "Amazon SageMaker Clarify",
          },
          correct_answer: "D",
          explanation:
            "Amazon SageMaker Clarify provides tools to explain model predictions, allowing practitioners to generate and share interpretability reports with customers and stakeholders, improving transparency and trust in ML models.",
          id: 277,
        },
        {
          number: "277",
          question:
            "Sentiment analysis is a subset of which broader field of AI?",
          options: {
            A: "Computer vision",
            B: "Robotics",
            C: "Natural language processing (NLP)",
            D: "Time series forecasting",
          },
          correct_answer: "C",
          explanation:
            "Sentiment analysis is a subset of natural language processing (NLP), which focuses on analyzing and interpreting human language, including the detection of sentiment within text.",
          id: 278,
        },
        {
          number: "278",
          question:
            "A company wants to set up private access to Amazon Bedrock APIs from the company’s AWS account. The company also wants to protect its data from internet exposure. Which solution meets these requirements?",
          options: {
            A: "Use Amazon CloudFront to restrict access to the company’s private content.",
            B: "Use AWS Glue to set up data encryption across the company’s data catalog.",
            C: "Use AWS Lake Formation to manage centralized data governance and cross-account data sharing.",
            D: "Use AWS PrivateLink to configure a private connection between the company’s VPC and Amazon",
          },
          correct_answer: "D",
          explanation:
            "AWS PrivateLink enables private connectivity between your VPC and Amazon Bedrock APIs without exposing data to the public internet, ensuring that all API calls and data transfers remain secure and private within your AWS account.",
          id: 279,
        },
        {
          number: "279",
          question:
            "A company receives a large amount of unstructured user feedback in text format. The company wants to analyze the sentiment of the user feedback. Which solution will meet these requirements?",
          options: {
            A: "Use a large language model (LLM) to perform natural language processing (NLP) for sentiment analysis",
            B: "Use a regression algorithm to classify the feedback based on predefined categories. Then, analyze user sentiment",
            C: "Use a recommendation engine algorithm to detect user sentiment.",
            D: "Use a time series algorithm to predict user sentiment based on past feedback.",
          },
          correct_answer: "A",
          explanation:
            "A large language model (LLM) can analyze unstructured text using NLP techniques to accurately determine the sentiment (positive, negative, or neutral) of user feedback. This approach is well-suited for handling large volumes of text data.",
          id: 280,
        },
        {
          number: "280",
          type: "dropdown_multiple",
          question:
            "A company wants to improve multiple ML models. Select the correct technique from the following list of use cases. Each technique should be selected one time or not at all.",
          options: [
            "Few-shot learning",
            "Fine-tuning",
            "Retrieval Augmented Generation (RAG)",
            "Zero-shot learning",
          ],
          scenarios: [
            {
              id: "scenario_1",
              text: "Enhancing the capabilities of a large language model (LLM) by using external sources",
            },
            {
              id: "scenario_2",
              text: "Querying a model to generalize and make predictions on unseen tasks",
            },
            {
              id: "scenario_3",
              text: "Querying a model with a limited amount of data for new tasks",
            },
          ],
          correct_answer: {
            scenario_1: "Retrieval Augmented Generation (RAG)",
            scenario_2: "Zero-shot learning",
            scenario_3: "Few-shot learning",
          },
          explanation:
            "RAG (Retrieval Augmented Generation) enhances LLMs by incorporating information from external data sources at query time. Zero-shot learning allows a model to perform tasks it wasn't explicitly trained on, using its general knowledge to make predictions. Few-shot learning enables a model to handle new tasks with only a small number of examples, adapting quickly to new scenarios.",
          id: 281,
        },
        {
          number: "281",
          question:
            "RAG improves LLMs by incorporating information from external data sources at query time. A company wants to create an AI solution to generate images and descriptions for a product catalog. The company needs to select a foundation model (FM) for this solution. The company must consider the output types of each FM. Which FM characteristic is the company evaluating?",
          options: {
            A: "Latency",
            B: "Model size",
            C: "Model customization",
            D: "Modality",
          },
          correct_answer: "D",
          explanation:
            "Modality refers to the types of input and output data a foundation model can handle, such as text, images, or both. Evaluating modality ensures the selected FM can generate both images and text descriptions for the product catalog.",
          id: 282,
        },
        {
          number: "282",
          question:
            "A company wants to use an ML model to analyze customer reviews on social media. The model must determine if each review has a neutral, positive, or negative sentiment. Which model evaluation strategy will meet these requirements?",
          options: {
            A: "Open-ended generation",
            B: "Text summarization",
            C: "Machine translation",
            D: "Classification",
          },
          correct_answer: "D",
          explanation:
            "Classification is the evaluation strategy that assigns input data (such as customer reviews) to predefined categories - in this case, neutral, positive, or negative sentiment. This approach is standard for sentiment analysis tasks.",
          id: 283,
        },
        {
          number: "283",
          type: "dropdown_multiple",
          question:
            "Select the correct AI term from the following list for each statement. Each AI term should be selected one time.",
          options: ["AI", "Deep learning", "ML"],
          scenarios: [
            {
              id: "scenario_1",
              text: "Simulates human problem-solving capabilities",
            },
            {
              id: "scenario_2",
              text: "Applies data-driven learning techniques to make predictions",
            },
            {
              id: "scenario_3",
              text: "Focuses on processing data through intricate neural networks",
            },
          ],
          correct_answer: {
            scenario_1: "AI",
            scenario_2: "ML",
            scenario_3: "Deep learning",
          },
          explanation:
            "Simulates human problem-solving capabilities -> AI Applies data-driven learning techniques to make predictions -> ML Focuses on processing data through intricate neural networks -> Deep learning AI (Artificial Intelligence) is the broadest concept and includes simulating human-like problem-solving. ML (Machine Learning) is a subset of AI that applies data-driven learning techniques to make predictions. Deep learning is a subset of ML that uses neural networks with many layers to process complex data.",
          id: 284,
        },
        {
          number: "284",
          question: "Which option is an example of unsupervised learning?",
          options: {
            A: "Clustering data points into groups based on their similarity",
            B: "Training a model to recognize images of animals",
            C: "Predicting the price of a house based on the house’s features",
            D: "Generating human-like text based on a given prompt",
          },
          correct_answer: "A",
          explanation:
            "Clustering data points based on similarity is an example of unsupervised learning, where the algorithm groups data without using labeled examples.",
          id: 285,
        },
        {
          number: "285",
          question:
            "An online learning company with large volumes of education materials wants to use enterprise search. Which AWS service meets these requirements?",
          options: {
            A: "Amazon Comprehend",
            B: "Amazon Textract",
            C: "Amazon Kendra",
            D: "Amazon Personalize",
          },
          correct_answer: "C",
          explanation:
            "Amazon Kendra is an intelligent enterprise search service that enables organizations to search large volumes of unstructured data, such as educational materials, making it ideal for enterprise search needs.",
          id: 286,
        },
        {
          number: "286",
          question:
            "A company creates video content. The company wants to use generative AI to generate new creative content and to reduce video creation time. Which solution will meet these requirements in the MOST operationally efficient way?",
          options: {
            A: "Use the Amazon Titan Image Generator model on Amazon Bedrock to generate intermediate images. Use video editing software to create videos.",
            B: "Use the Amazon Nova Canvas model on Amazon Bedrock to generate intermediate images. Use video editing software to create videos.",
            C: "Use the Amazon Nova Reel model on Amazon Bedrock to generate videos.",
            D: "Use the Amazon Nova Pro model on Amazon Bedrock to generate videos.",
          },
          correct_answer: "C",
          explanation:
            "The Amazon Nova Reel model on Amazon Bedrock is specifically designed for generative video creation, enabling the company to efficiently generate new creative content and significantly reduce video creation time in an operationally efficient manner.",
          id: 287,
        },
        {
          number: "287",
          question:
            "A company is training ML models on datasets. The datasets contain some classes that have more examples than other classes. The company wants to measure how well the model balances detecting and labeling the classes. Which metric should the company use?",
          options: {
            A: "Accuracy",
            B: "Recall",
            C: "Precision",
            D: "F1 score",
          },
          correct_answer: "D",
          explanation:
            "The F1 score is the harmonic mean of precision and recall, making it especially useful for evaluating model performance on datasets with class imbalance. It measures how well the model balances detecting (recall) and correctly labeling (precision) all classes.",
          id: 288,
        },
        {
          number: "288",
          question:
            "A company is analyzing financial transaction records. The company categorizes the records as either personal or business. The company inserts the categories into the transaction records. Which data preparation step does this describe?",
          options: {
            A: "Data encoding",
            B: "Data labeling",
            C: "Data normalization",
            D: "Data balancing",
          },
          correct_answer: "B",
          explanation:
            'Assigning categories such as "personal" or "business" to financial transaction records is an example of data labeling, where each data point is tagged with a specific label for use in machine learning or data analysis tasks.',
          id: 289,
        },
        {
          number: "289",
          question:
            "A company wants to extract key insights from large policy documents to increase employee efficiency. Which generative AI strategy meets this requirement?",
          options: {
            A: "Regression",
            B: "Clustering",
            C: "Summarization",
            D: "Classification",
          },
          correct_answer: "C",
          explanation:
            "Summarization is a generative AI strategy that extracts and condenses the key insights from large documents, such as policy documents, making it easier for employees to quickly understand and act on essential information.",
          id: 290,
        },
        {
          number: "290",
          question:
            "A company is using Amazon SageMaker to deploy a model that identifies if social media posts contain certain topics. The company needs to show how different input features influence model behavior.   Which SageMaker feature meets these requirements?",
          options: {
            A: "SageMaker Canvas",
            B: "SageMaker Clarify",
            C: "SageMaker Feature Store",
            D: "SageMaker Ground Truth",
          },
          correct_answer: "B",
          explanation:
            "SageMaker Clarify provides tools for model interpretability, allowing you to visualize and understand how different input features influence the behavior and predictions of your deployed model.",
          id: 291,
        },
        {
          number: "291",
          type: "dropdown_multiple",
          question:
            "An AI practitioner is determining the appropriate data type for various use cases. Select the correct data type from the following list for each use case.",
          options: [
            "Image data",
            "Tabular data",
            "Text data",
            "Time series data",
          ],
          scenarios: [
            {
              id: "scenario_1",
              text: "Build a sentiment analysis model for social media posts.",
            },
            {
              id: "scenario_2",
              text: "Train a self-driving car to recognize traffic signs.",
            },
            {
              id: "scenario_3",
              text: "Optimize ad campaigns by using customer demographic data and purchase history.",
            },
            {
              id: "scenario_4",
              text: "Forecast stock prices by using historical price data.",
            },
          ],
          correct_answer: {
            scenario_1: "Text data",
            scenario_2: "Image data",
            scenario_3: "Tabular data",
            scenario_4: "Time series data",
          },
          explanation:
            "Sentiment analysis deals with text data from social media posts. Recognizing traffic signs requires processing image data. Optimizing ad campaigns uses structured (tabular) customer and purchase data. Stock price forecasting uses time series data, which captures trends over time.",
          id: 292,
        },
        {
          number: "292",
          question:
            "A company wants to assess internet quality in remote areas of the world. The company needs to collect internet speed data and store the data in Amazon RDS. The company will analyze internet speed variation throughout each day. The company wants to create an AI model to predict potential internet disruptions. Which type of data should the company collect for this task?",
          options: {
            A: "Tabular data",
            B: "Text data",
            C: "Time series data",
            D: "Audio data",
          },
          correct_answer: "C",
          explanation:
            "Time series data consists of measurements (such as internet speed) collected over time, which is essential for analyzing variations throughout the day and for building predictive models of potential internet disruptions.",
          id: 293,
        },
        {
          number: "293",
          question:
            "A company wants to build an ML model to detect abnormal patterns in sensor data. The company does not have labeled data for training.     Which ML method will meet these requirements?",
          options: {
            A: "Linear regression",
            B: "Classification",
            C: "Decision tree",
            D: "Autoencoders",
          },
          correct_answer: "D",
          explanation:
            "Autoencoders are a type of unsupervised learning method used to detect abnormal patterns (anomalies) in sensor data when labeled data is not available. They learn to reconstruct normal patterns, making deviations (anomalies) easily detectable.",
          id: 294,
        },
        {
          number: "294",
          question:
            "A company uses Amazon Bedrock to implement a generative AI assistant on a website. The AI assistant helps customers with product recommendations and purchasing decisions. The company wants to measure the direct impact of the AI assistant on sales performance. Which metric will meet these requirements?",
          options: {
            A: "The conversion rate of customers who purchase products after AI assistant interactions.",
            B: "The number of customer interactions with the AI assistant",
            C: "Sentiment analysis scores from customer feedback after AI assistant interactions",
            D: "Natural language understanding accuracy rates",
          },
          correct_answer: "A",
          explanation:
            "Conversion rate directly measures the impact of the AI assistant on sales performance by quantifying the percentage of customers who make a purchase after interacting with the assistant. This metric links AI engagement to actual sales outcomes.",
          id: 295,
        },
        {
          number: "295",
          question:
            "Which AWS service or feature stores embeddings in a vector database for use with foundation models (FMs) and Retrieval Augmented Generation (RAG)?",
          options: {
            A: "Amazon SageMaker Ground Truth",
            B: "Amazon OpenSearch Service",
            C: "Amazon Transcribe",
            D: "Amazon Textract",
          },
          correct_answer: "B",
          explanation:
            "Amazon OpenSearch Service provides a vector database capability to store and search embeddings, which are essential for powering Retrieval Augmented Generation (RAG) workflows and integrating with foundation models (FMs) in generative AI solutions. T",
          id: 296,
        },
        {
          number: "296",
          question:
            "Which scenario represents a practical use case for generative AI?   ",
          options: {
            A: "Using an ML model to forecast product demand",
            B: "Employing a chatbot to provide human-like responses to customer queries in real time",
            C: "Using an analytics dashboard to track website traffic and user behavior",
            D: "Implementing a rule-based recommendation engine to suggest products to customers",
          },
          correct_answer: "B",
          explanation:
            "A generative AI use case involves generating new content, such as natural language responses. A chatbot that provides human-like responses to customer queries uses generative AI models (like large language models) to dynamically create relevant answers, making this a practical example.",
          id: 297,
        },
        {
          number: "297",
          question:
            "A company is using Amazon Bedrock for a generative AI solution. The solution must integrate a service with vector database storage and vector search capabilities. Which AWS service will meet these requirements?",
          options: {
            A: "Amazon DynamoDB",
            B: "Amazon OpenSearch Service",
            C: "Amazon ElastiCache",
            D: "Amazon Redshift",
          },
          correct_answer: "B",
          explanation:
            "Amazon OpenSearch Service supports vector database storage and vector search capabilities, which are essential for integrating with generative AI solutions like those on Amazon Bedrock that require efficient similarity search over embeddings.",
          id: 298,
        },
        {
          number: "298",
          question:
            "A media streaming platform wants to provide movie recommendations to users based on the users’ account history. Which AWS service meets these requirements?",
          options: {
            A: "Amazon Polly",
            B: "Amazon Comprehend",
            C: "Amazon Transcribe",
            D: "Amazon Personalize",
          },
          correct_answer: "D",
          explanation:
            "Amazon Personalize is an AWS service designed to deliver personalized recommendations, such as movie suggestions, to users based on their account history and preferences.",
          id: 299,
        },
        {
          number: "299",
          question:
            "A company has developed an ML model to approve or reject loan applications. The model's decision-making process must be transparent and explainable to comply with regulatory requirements. The company must document the decision-making process for audit purposes. Which solution will meet these requirements?",
          options: {
            A: "Amazon Textract",
            B: "Amazon SageMaker Model Card",
            C: "AWS Cloud Formation",
            D: "Amazon Comprehend",
          },
          correct_answer: "B",
          explanation:
            "Amazon SageMaker Model Card enables you to document a model’s decision-making process, including transparency and explainability details, which helps meet regulatory and audit requirements for AI-driven decisions such as loan approvals.",
          id: 300,
        },
        {
          number: "300",
          type: "dropdown_multiple",
          question:
            "A company is building a generative AI application and is reviewing foundation models (FMs). The company needs to consider multiple FM characteristics. Select the correct FM characteristic from the following list for each definition. Each FM characteristic should be selected one time. ",
          options: ["Concurrency", "Context windows", "Latency"],
          scenarios: [
            {
              id: "scenario_1",
              text: "Amount of information that can fit in a single prompt",
            },
            {
              id: "scenario_2",
              text: "Length of time it takes for a model to generate an output",
            },
            {
              id: "scenario_3",
              text: "Multiple users invoking an application endpoint simultaneously",
            },
          ],
          correct_answer: {
            scenario_1: "Context windows",
            scenario_2: "Latency",
            scenario_3: "Concurrency",
          },
          explanation:
            "Amount of information that can fit in a single prompt -> Context windows Length of time it takes for a model to generate an output -> Latency Multiple users invoking an application endpoint simultaneously -> Concurrency Context windows determine how much information the model can process at once. Latency is the response time for generating output. Concurrency measures how many users or requests the endpoint can handle at the same time.",
          id: 301,
        },
        {
          number: "301",
          question:
            "A company is using large language models (LLMs) to develop online tutoring applications. The company needs to apply configurable safeguards to the LLMs. These safeguards must ensure that the LLMs follow standard safety rules when creating applications. Which solution will meet these requirements with the LEAST effort?",
          options: {
            A: "Amazon Bedrock playgrounds",
            B: "Amazon SageMaker Clarify",
            C: "Amazon Bedrock Guardrails",
            D: "Amazon SageMaker Jumpstart",
          },
          correct_answer: "C",
          explanation:
            "Amazon Bedrock Guardrails provide configurable safeguards that enforce standard safety rules on large language models (LLMs) with minimal effort, making it easy to ensure safe and compliant AI application development.",
          id: 302,
        },
        {
          number: "302",
          question:
            "A company is exploring Amazon Nova models in Amazon Bedrock. The company needs a multimodal model that supports multiple languages. Which Nova model will meet these requirements MOST cost-effectively?",
          options: {
            A: "Nova Lite",
            B: "Nova Pro",
            C: "Nova Canvas",
            D: "Nova Reel",
          },
          correct_answer: "A",
          explanation:
            "  Nova Lite is a multimodal model in Amazon Bedrock that supports multiple languages andis designed the most cost-effective option among the Nova models, making it suitable for organizations seeking efficient, scalable, and economical",
          id: 303,
        },
        {
          number: "303",
          question:
            "A company is building a new generative AI chatbot. The chatbot uses an Amazon Bedrock foundation model (FM) to generate responses. During testing, the company notices that the chatbot is prone to prompt injection attacks. What can the company do to secure the chatbot with the LEAST implementation effort?",
          options: {
            A: "Fine-tune the FM to avoid harmful responses.",
            B: "Use Amazon Bedrock Guardrails content filters and denied topics.",
            C: "Change the FM to a more secure FM.",
            D: "Use chain-of-thought prompting to produce secure responses.",
          },
          correct_answer: "B",
          explanation:
            "Using Amazon Bedrock Guardrails' content filters and denied topics is the quickest and least effort solution to mitigate prompt injection attacks. These guardrails can be configured without model retraining, helping to automatically filter or block unsafe prompts and responses.",
          id: 304,
        },
        {
          number: "304",
          question: "What does inference refer to in the context of AI?",
          options: {
            A: "The process of creating new AI algorithms",
            B: "The use of a trained model to make predictions or decisions on unseen data",
            C: "The process of combining multiple AI models into one model",
            D: "The method of collecting training data for AI systems",
          },
          correct_answer: "B",
          explanation:
            "Inference in AI refers to applying a trained model to new, unseen data to generate predictions or decisions, leveraging the patterns learned during training.",
          id: 305,
        },
        {
          number: "305",
          question:
            "A company wants to build an AI assistant to provide responses to user queries. The AI assistant must evaluate specific data sources, query external APIs, generate response options, and compare and prioritize response options. Which Amazon Bedrock feature or resource will meet these requirements?",
          options: {
            A: "Prompt Management",
            B: "Response streaming",
            C: "Knowledge Bases",
            D: "Agents",
          },
          correct_answer: "D",
          explanation:
            "Amazon Bedrock Agents enable AI assistants to interact with specific data sources, query external APIs, generate and compare response options, and prioritize results, making them the ideal feature for building advanced, task-oriented chatbots with decision-making and orchestration capabilities.",
          id: 306,
        },
        {
          number: "306",
          question:
            "An AI practitioner notices a large language model (LLM) is generating different responses for the same input across multiple invocations. Which risk of AI does this describe?",
          options: {
            A: "Hallucinations",
            B: "Nondeterminism",
            C: "Accuracy",
            D: "Multimodality",
          },
          correct_answer: "B",
          explanation:
            "Nondeterminism refers to the property where a large language model generates different responses for the same input due to inherent randomness or stochasticity in the generation process. This can lead to varying outputs on multiple invocations with identical inputs.",
          id: 307,
        },
        {
          number: "307",
          question:
            "A company is building a generative AI application on AWS. The application will help improve reading comprehension for students. The application must give students the ability to add illustrations to stories. Which solution will meet this requirement?",
          options: {
            A: "Use Amazon Bedrock Stable Diffusion 3.5 Large to generate images based on text inputs.",
            B: "Use Amazon Polly to create an audiobook based on story texts.",
            C: "Use Amazon Rekognition to analyze image contents and detect text attributes.",
            D: "Create a standard prompt template. Use Amazon Q Business to illustrate stories.",
          },
          correct_answer: "A",
          explanation:
            "Amazon Bedrock Stable Diffusion 3.5 Large is a generative AI model designed to create high-quality images from text prompts, allowing students to add custom illustrations to their stories and enhancing reading comprehension through visual aids.",
          id: 308,
        },
        {
          number: "308",
          question:
            "A healthcare company wants to analyze patient data. The data was gathered over the previous year to detect patterns in disease outbreaks. The company needs to create a trend analysis report for each month to present to public health officials. The company must provide insights into patient data from the most recent month of the current year. Which inference method will meet these requirements MOST cost-effectively?",
          options: {
            A: "Real-time inference",
            B: "Batch transform",
            C: "Serverless inference",
            D: "Asynchronous inference",
          },
          correct_answer: "B",
          explanation:
            "Batch transform is the most cost-effective inference method for analyzing large amounts of data collected over a period (such as monthly patient data). It allows you to process and generate reports on all historical data in batches, rather than incurring the higher costs of real-time or serverless inference.",
          id: 309,
        },
        {
          number: "309",
          type: "dropdown_multiple",
          question:
            "Select and order the steps from the following list to correctly describe the ML lifecycle for a new custom model. Select each step one time.",
          options: [
            "Define the business objective.",
            "Deploy the model.",
            "Develop and train the model.",
            "Process the data.",
          ],
          scenarios: [
            {
              id: "scenario_1",
              text: "Step 1:",
            },
            {
              id: "scenario_2",
              text: "Step 2:",
            },
            {
              id: "scenario_3",
              text: "Step 3:",
            },
            {
              id: "scenario_4",
              text: "Step 4:",
            },
          ],
          correct_answer: {
            scenario_1: "Define the business objective.",
            scenario_2: "Process the data.",
            scenario_3: "Develop and train the model.",
            scenario_4: "Deploy the model.",
          },
          explanation:
            "The ML lifecycle for developing a new custom model follows a systematic process: 1. Define the business objective - Understanding the business problem and determining how ML can solve it is the critical first step. 2. Process the data - Data collection, cleaning, preprocessing, and feature engineering are essential before model development. 3. Develop and train the model - This involves selecting algorithms, training the model, and evaluating its performance. 4. Deploy the model - Once the model is trained and validated, it is deployed to production to generate predictions and deliver business value.",
          id: 310,
        },
        {
          number: "310",
          question:
            "A company acquires International Organization for Standardization (ISO) accreditation to manage AI risks and to use AI responsibly. What does this accreditation reflect about the company?",
          options: {
            A: "All members of the company are ISO certified.",
            B: "All AI systems that the company uses are ISO certified.",
            C: "All AI application team members are ISO certified.",
            D: "The company’s development framework is ISO certified.",
          },
          correct_answer: "D",
          explanation:
            "ISO accreditation for managing AI risks means that the company’s development processes, controls, and frameworks for AI are certified to meet ISO standards. It does not certify individual employees or AI systems, but rather the organizational framework and practices.",
          id: 311,
        },
        {
          number: "311",
          type: "dropdown_multiple",
          question:
            "Select the correct prompt engineering technique from the following list for each description. Select each prompt engineering technique one time or not at all.",
          options: [
            "Chain-of-thought prompting",
            "Few-shot prompting",
            "Role-based prompting",
            "Single-shot prompting",
            "Zero-shot prompting",
          ],
          scenarios: [
            {
              id: "scenario_1",
              text: "Provide a small number of examples to the model to understand the desired task before generating outputs.",
            },
            {
              id: "scenario_2",
              text: "Prompt a model to break down the step-by-step process that the model took to arrive at a final answer.",
            },
            {
              id: "scenario_3",
              text: "Prompt a model to perform a task without providing examples.",
            },
          ],
          correct_answer: {
            scenario_1: "Few-shot prompting",
            scenario_2: "Chain-of-thought prompting",
            scenario_3: "Zero-shot prompting",
          },
          explanation:
            "Few-shot prompting provides a small number of examples to guide the model in understanding the desired task before generating outputs. Chain-of-thought prompting asks the model to explain its step-by-step reasoning process to arrive at a final answer. Zero-shot prompting asks the model to perform a task without providing any examples, relying solely on its pre-trained knowledge.",
          id: 312,
        },
        {
          number: "312",
          question:
            "A company is developing an ML model to predict heart disease risk. The model uses patient data, such as age, cholesterol, blood pressure, smoking status, and exercise habits. The dataset includes a target value that indicates whether a patient has heart disease. Which ML technique will meet these requirements?",
          options: {
            A: "Unsupervised learning",
            B: "Supervised learning",
            C: "Reinforcement learning",
            D: "Semi-supervised learning",
          },
          correct_answer: "B",
          explanation:
            "Supervised learning is used when the dataset includes both input features (like age, cholesterol, blood pressure, etc.) and a target value indicating the presence of heart disease. The model learns to predict the target value from labeled examples.",
          id: 313,
        },
        {
          number: "313",
          type: "dropdown_multiple",
          question:
            "A company periodically updates its product database by manually uploading digital product guides. The product guides contain text and images. The company wants to automate this task by using generative AI. Select and order the steps from the following list to automate the database update task by using generative AI. Select each step one time.",
          options: [
            "Insert data into the product database.",
            "Upload the digital text and image files to an Amazon S3 bucket.",
            "Use Amazon Nova multimodal models to process the digital text and image files.",
          ],
          scenarios: [
            {
              id: "scenario_1",
              text: "Step 1:",
            },
            {
              id: "scenario_2",
              text: "Step 2:",
            },
            {
              id: "scenario_3",
              text: "Step 3:",
            },
          ],
          correct_answer: {
            scenario_1:
              "Upload the digital text and image files to an Amazon S3 bucket.",
            scenario_2:
              "Use Amazon Nova multimodal models to process the digital text and image files.",
            scenario_3: "Insert data into the product database.",
          },
          explanation:
            "To automate the product database update using generative AI, the correct workflow is: 1. Upload the digital text and image files to an Amazon S3 bucket - First, the raw data (product guides with text and images) must be stored in a centralized location like S3. 2. Use Amazon Nova multimodal models to process the digital text and image files - Amazon Nova multimodal models can analyze both text and images to extract relevant product information. 3. Insert data into the product database - Finally, the processed and structured data is inserted into the product database to complete the automation workflow.",
          id: 314,
        },
        {
          number: "314",
          question:
            "A company has guidelines for data storage and deletion.  Which data governance strategy does this describe? ",
          options: {
            A: "Data de-identification",
            B: "Data quality standards",
            C: "Data retention",
            D: "Log storage",
          },
          correct_answer: "C",
          explanation:
            "Data retention refers to the policies and practices that define how long data is stored and when it should be deleted, ensuring compliance with organizational guidelines and regulatory requirements.",
          id: 315,
        },
        {
          number: "315",
          question:
            "A company needs to apply numerical transformations to a set of images to transpose and rotate the images. Which solution will meet these requirements in the MOST operationally efficient way?",
          options: {
            A: "Create a deep neural network by using the images as input.",
            B: "Create an AWS Lambda function to perform the transformations.",
            C: "Use an Amazon Bedrock large language model (LLM) with a high temperature.",
            D: "Use AWS Glue Data Quality to make corrections to each image.",
          },
          correct_answer: "B",
          explanation:
            "An AWS Lambda function can efficiently perform numerical image transformations such as transposing and rotating images at scale without the need to build or train a neural network, making it the most operationally efficient solution.",
          id: 316,
        },
        {
          number: "316",
          question:
            "An AI practitioner is writing software code. The AI practitioner wants to quickly develop a test case and create documentation for the code. Which solution will meet these requirements with the LEAST effort?",
          options: {
            A: "Upload the code to an online coding assistant.",
            B: "Develop an application to use foundation models (FMs).",
            C: "Use Amazon Q Developer in an integrated development environment (IDE).",
            D: "Research and write test cases. Then, create test cases and add documentation.",
          },
          correct_answer: "C",
          explanation:
            "Amazon Q Developer integrated with an IDE can automatically generate test cases and create documentation for code, minimizing manual effort and accelerating development compared with building custom solutions or manual research.",
          id: 317,
        },
        {
          number: "317",
          question:
            "A company is developing a generative AI application to automatically generate product descriptions for an ecommerce website. The product descriptions must consist of paragraphs of text that are consistent in style and tone. The application must generate thousands of unique descriptions each day. Which type of generative model will meet these requirements?",
          options: {
            A: "A variational autoencoder (VAE) model",
            B: "A transformer-based model",
            C: "A diffusion model",
            D: "A generative adversarial network (GAN) model",
          },
          correct_answer: "B",
          explanation:
            "Transformer-based models, such as large language models, are designed to generate coherent and contextually consistent text at scale. They are well-suited for creating thousands of unique, styled product descriptions daily.",
          id: 318,
        },
        {
          number: "318",
          question:
            "An AI practitioner has trained a model on a training dataset. The model performs well on the training data. However, the model does not perform well on evaluation data. What is the MOST likely cause of this issue?",
          options: {
            A: "The model is underfit.",
            B: "The model requires prompt engineering.",
            C: "The model is biased.",
            D: "The model is overfit.",
          },
          correct_answer: "D",
          explanation:
            "Overfitting occurs when a model learns the training data too well, including noise and details that do not generalize. As a result, it performs well on training data but poorly on unseen evaluation data.",
          id: 319,
        },
        {
          number: "319",
          question:
            "A music company wants to build a system to search for songs by the meaning of the lyrics. Which solution meets this requirement?",
          options: {
            A: "Create prompts from the lyrics. Use Amazon QuickSight for searching.",
            B: "Create vector embeddings from the lyrics. Use Amazon OpenSearch Service for searching.",
            C: "Create tokens. Use Amazon Lex for searching.",
            D: "Create stems. Use Amazon Personalize for searching.",
          },
          correct_answer: "B",
          explanation:
            "Creating vector embeddings from the lyrics and using Amazon OpenSearch Service for searching allows for semantic search based on the meaning of the lyrics rather than just keyword matching. Vector embeddings capture the semantic meaning of text, enabling similarity search for songs with similar lyrical content.",
          id: 320,
        },
        {
          number: "320",
          question:
            "An ecommerce company is developing a new mobile app. The app will give users the ability to search the company's product catalog by entering keyword search terms or by uploading a photo of a similar item to find visually similar products. The company wants to use Amazon Titan for both search capabilities. Which Amazon Titan model meets these requirements?",
          options: {
            A: "Amazon Titan Text Express",
            B: "Amazon Titan Text Lite",
            C: "Amazon Titan Embeddings",
            D: "Amazon Titan Multimodal Embeddings",
          },
          correct_answer: "D",
          explanation:
            "Amazon Titan Multimodal Embeddings can handle both text and image inputs, making it suitable for both keyword search (text) and visual similarity search (image) capabilities in the mobile app.",
          id: 321,
        },
        {
          number: "321",
          question:
            "A company wants to use Amazon Bedrock to summarize the customer documents that are in the company's database. Which metric will evaluate the accuracy of the summaries?",
          options: {
            A: "BERTScore",
            B: "Root mean squared error (RSE)",
            C: "Detoxify algorithm",
            D: "Mean absolute percentage error (MAPE)",
          },
          correct_answer: "A",
          explanation:
            "BERTScore is a metric used to evaluate the quality of generated text by comparing it to reference text using contextual embeddings from BERT. It measures the semantic similarity between the generated summaries and the original documents, making it suitable for evaluating summary accuracy.",
          id: 322,
        },
        {
          number: "322",
          question:
            "A company wants to use a database to support a generative AI model. The model will create intelligent agents that provide conversational search experiences. Which AWS services meet these requirements?",
          options: {
            A: "Amazon Neptune",
            B: "Amazon Elastic Kubernetes Service (Amazon EKS)",
            C: "Amazon S3",
            D: "Amazon OpenSearch Service",
            E: "Amazon EMR",
          },
          correct_answer: "AD",
          explanation:
            "Amazon Neptune is a graph database that can store and query complex relationships between entities, making it suitable for conversational search experiences. Amazon OpenSearch Service provides vector search capabilities and can be used to build intelligent search applications with generative AI models.",
          id: 323,
        },
        {
          number: "323",
          question:
            "A food company wants to use generative AI to automatically generate product descriptions based on customer profiles and the product catalog. Which solution meets these requirements?",
          options: {
            A: "Use an Amazon Bedrock standalone third-party foundation model (FM).",
            B: "Implement a data lake on Aws that is customized to the product catalog.",
            C: "Implement human-in-the-loop monitoring to annotate the product catalog.",
            D: "Use an Amazon Bedrock knowledge base that uses company data sources.",
          },
          correct_answer: "D",
          explanation:
            "Using an Amazon Bedrock knowledge base that incorporates company data sources allows the generative AI model to access customer profiles and product catalog information to generate personalized product descriptions automatically.",
          id: 324,
        },
        {
          number: "324",
          question:
            "A company uses a foundation model (FM) on Amazon Bedrock for text summarization. The company wants to check the accuracy, robustness, and non- toxicity of the FM. Which Amazon Bedrock feature or resource provides the ability to assess the FM?",
          options: {
            A: "Agents for Amazon Bedrock",
            B: "Amazon Bedrock model evaluation job report cards.",
            C: "Amazon Bedrock custom models.",
            D: "Amazon Bedrock inference parameters",
          },
          correct_answer: "B",
          explanation:
            "Amazon Bedrock model evaluation job report cards provide comprehensive assessment capabilities for foundation models. This feature allows you to evaluate FM performance across multiple dimensions including accuracy (how well the model performs on specific tasks), robustness (how consistently the model performs across different inputs), and toxicity (ensuring the model doesn't generate harmful or inappropriate content). The evaluation generates detailed report cards that quantify these metrics, helping organizations ensure their chosen FM meets quality and safety standards before deployment.",
          id: 325,
        },
        {
          number: "325",
          question:
            "A company wants to build its own generative AI application by using an existing third-party foundation model (FM). The company wants to directly integrate the third-party FM with the company's workload by using an API. Which scope in the Generative AI Security Scoping Matrix does this use case apply to?",
          options: {
            A: "Enterprise App",
            B: "Fine-tuned Models",
            C: "Pre-trained Models",
            D: "Self-trained Models",
          },
          correct_answer: "C",
          explanation:
            "This use case involves using a pre-trained foundation model directly via API without modification, which falls under the 'Pre-trained Models' scope in the Generative AI Security Scoping Matrix. This scope covers scenarios where organizations use existing models as-is rather than fine-tuning or training new ones.",
          id: 326,
        },
        {
          number: "326",
          question:
            "A company is building a chatbot to answer customer questions by using a foundation model (FM). The company wants to confirm that the model is unbiased and fair. Which solution will assess the fairness of the model?",
          options: {
            A: "Fine-tuning a custom model on Amazon Bedrock",
            B: "User surveys about chatbot satisfaction",
            C: "Automatic model evaluation tasks",
            D: "Regular accuracy testing of chatbot responses",
          },
          correct_answer: "C",
          explanation:
            "Automatic model evaluation tasks are designed to systematically assess model fairness by measuring bias across different demographic groups and ensuring equitable performance. This provides objective, automated metrics rather than subjective user feedback or basic accuracy tests.",
          id: 327,
        },
        {
          number: "327",
          question:
            "A company is building a language model application. The company wants to protect against prompt injection attacks. Which option is an example of a prompt injection attack?",
          options: {
            A: "Cryptographic failures",
            B: "Extracting conversation history",
            C: "Broken access control",
            D: "Server-side request forgery",
          },
          correct_answer: "B",
          explanation:
            "Prompt injection attacks involve maliciously crafting inputs to override or extract information from a model's system prompt or conversation history. Extracting conversation history is a classic example where attackers try to reveal previous interactions or sensitive data stored in the model's context.",
          id: 328,
        },
        {
          number: "328",
          question:
            "An AI practitioner is generating a prompt for a large language model (LLM). The problem requires complex, multi-step reasoning. Which prompt engineering technique should the AI practitioner use to meet these requirements?",
          options: {
            A: "Chain-of-thought prompting",
            B: "Zero-shot prompting",
            C: "Prompt templating",
            D: "Retrieval Augmented Generation (RAG)",
          },
          correct_answer: "A",
          explanation:
            "Chain-of-thought prompting encourages the model to break down complex problems into step-by-step reasoning, making it ideal for multi-step logical tasks. This technique helps the model arrive at more accurate conclusions by explicitly showing its thought process.",
          id: 329,
        },
        {
          number: "329",
          question:
            "An ecommerce company wants to build a forecasting model to predict demand for products. Which data type meets these requirements?",
          options: {
            A: "Unstructured data",
            B: "Time-series data",
            C: "Labeled data",
            D: "Unlabeled data",
          },
          correct_answer: "B",
          explanation:
            "Time-series data consists of observations collected over time intervals, making it essential for forecasting models that need to identify patterns, trends, and seasonality in product demand over time.",
          id: 330,
        },
        {
          number: "330",
          question:
            "Which characteristic of generative AI models is an advantage in natural language processing (NLP) tasks?",
          options: {
            A: "Inherent transparency",
            B: "Deterministic outputs",
            C: "Novel and diverse outputs",
            D: "Computational efficiency",
          },
          correct_answer: "C",
          explanation:
            "Generative AI models excel in NLP by producing novel and diverse outputs, allowing them to create varied, contextually appropriate responses rather than being limited to predefined templates or deterministic results.",
          id: 331,
        },
        {
          number: "331",
          question:
            "A company is using prompt engineering to provide customized outputs for text generation. The company needs the model to generate responses that use industry-specific jargon and technical terms. The company has a small amount of unlabeled data and must use this data to customize the model. The solution must use minimal computational resources. Which solution will meet these requirements?",
          options: {
            A: "Pre-train a new LLM based on the unlabeled data",
            B: "Perform domain adaptation fine-tuning on the existing model",
            C: "Perform instruction-based fine-tuning based on the unlabeled data",
            D: "Fine-tune prompt templates",
          },
          correct_answer: "D",
          explanation:
            "Fine-tuning prompt templates with a small amount of unlabeled data requires minimal computational resources compared to model training or fine-tuning. This approach customizes outputs by providing context-specific examples in the prompt itself.",
          id: 332,
        },
        {
          number: "332",
          question:
            "A company is building a chatbot by using generative AI. The company wants to ensure that the chatbot is inclusive. Which solution will meet these requirements?",
          options: {
            A: "Use unbiased training data for fine-tuning",
            B: "Use Amazon Fraud Detector",
            C: "Use prompt templates to set up the prompt guidelines",
            D: "Set up agents and use Retrieval Augmented Generation (RAG)",
          },
          correct_answer: "A",
          explanation:
            "Inclusive chatbots require training on diverse, representative datasets that avoid bias. Using unbiased training data during fine-tuning ensures the model learns from equitable examples across different demographics and perspectives.",
          id: 333,
        },
        {
          number: "333",
          question:
            "A company wants to deploy a generative AI application to help employees create images from text inputs. Which type of generative AI model meets this requirement?",
          options: {
            A: "Transformer-based model",
            B: "Multi-class classification model",
            C: "Regression model",
            D: "Diffusion model",
          },
          correct_answer: "D",
          explanation:
            "Diffusion models are specifically designed for generating high-quality images from textual descriptions through iterative noise reduction processes, making them ideal for text-to-image applications.",
          id: 334,
        },
        {
          number: "334",
          question:
            "A research company wants to develop a text summarization application that can ingest multiple research papers and summarize the research papers. The company wants to use an Amazon Bedrock foundation model (FM). The company wants to summarize the research papers in a single model call. The FM must be able to accept large inputs in a single prompt. Which model factor should the company consider before selecting an FM for this task?",
          options: {
            A: "Vocabulary size",
            B: "Model complexity",
            C: "Model latency",
            D: "Context window",
          },
          correct_answer: "D",
          explanation:
            "The context window determines the maximum input length a model can process in a single call. For summarizing multiple research papers, a large context window is essential to include all documents in one prompt.",
          id: 335,
        },
        {
          number: "335",
          question:
            "A company wants to use Amazon Bedrock to build a generative AI application for product advertising. The company wants to customize a foundation model (FM) by using unlabeled data to make the model more specialized. Which model customization technique meets these requirements?",
          options: {
            A: "Fine-tuning",
            B: "Retrieval Augmented Generation (RAG)",
            C: "Continued pre-training",
            D: "Prompt engineering",
          },
          correct_answer: "C",
          explanation:
            "Continued pre-training is the technique used to further train a foundation model on unlabeled data to make it more specialized for a specific domain or task, which matches the company's requirements for customizing the FM using unlabeled data.",
          id: 336,
        },
        {
          number: "336",
          question:
            "A startup company is using AWS services for its software development lifecycle. The company wants to use generative AI services to optimize programming productivity and to decrease time to market for the company's applications. Which combination of steps meets these requirements? (Select TWO).",
          options: {
            A: "Use Amazon Comprehend notebooks to process data.",
            B: "Use Amazon Q Developer for real-time code suggestions.",
            C: "Use Amazon Fraud Detector for built-in security scans.",
            D: "Use Amazon Q Developer for built-in security scans.",
            E: "Use Amazon Rekognition for real-time code suggestions.",
          },
          correct_answer: "BD",
          explanation:
            "Amazon Q Developer provides both real-time code suggestions to improve programming productivity and built-in security scans to help decrease time to market by catching issues early in development.",
          id: 337,
        },
        {
          number: "337",
          question:
            "A company is building a solution to predict risk by using generative AI. The company needs to understand and trust the automated solution before replacing previous manual processes. Which solution meets these requirements?",
          options: {
            A: "Model monitoring",
            B: "Model training",
            C: "Model explainability",
            D: "Model inference evaluation",
          },
          correct_answer: "C",
          explanation:
            "Model explainability helps understand how the AI model makes predictions, which is essential for building trust in the automated solution before replacing manual processes.",
          id: 338,
        },
        {
          number: "338",
          question:
            "Which type of neural network offers the ability to process text sequences with parallel computation?",
          options: {
            A: "Transformer neural network",
            B: "Recurrent neural network (RNN)",
            C: "Convolutional neural network (CNN)",
            D: "Long short-term memory (LSTM) network",
          },
          correct_answer: "A",
          explanation:
            "Transformer neural networks use self-attention mechanisms that allow for parallel processing of text sequences, unlike RNNs and LSTMs which process sequences sequentially.",
          id: 339,
        },
        {
          number: "339",
          question:
            "A global company has built a generative AI application by using Amazon Bedrock foundation models (FMs). The company's highest priority is cloud security. According to the AWS shared responsibility model, which option is the customer's responsibility?",
          options: {
            A: "Access permissions and data encryption",
            B: "Physical networking setup and configuration",
            C: "Host operating system and virtualization",
            D: "Security and compliance certifications",
          },
          correct_answer: "A",
          explanation:
            "According to the AWS shared responsibility model, customers are responsible for access permissions and data encryption in the cloud, while AWS handles the security of the cloud infrastructure itself.",
          id: 340,
        },
        {
          number: "340",
          question:
            "A company is using a generative AI model that creates images based on multiple text prompts and image examples that users provide. The company wants the AI-generated images to closely match the image examples that users provide. Which prompt engineering technique meets these requirements?",
          options: {
            A: "Zero-shot prompting",
            B: "Few-shot prompting",
            C: "One-shot prompting",
            D: "Tree-of-thought prompting",
          },
          correct_answer: "B",
          explanation:
            "Few-shot prompting involves providing multiple examples (few-shot) in the prompt to help the model understand the desired output better, which is ideal for making AI-generated images closely match the provided image examples.",
          id: 341,
        },
        {
          number: "341",
          question:
            "A retail company is tagging its product inventory. A tag is automatically assigned to each product based on the product desoription. The company created one product category by using a large language model （LLM） on Amazon Bedrock in few-shot learning mod The company collected a labeled dataset and wants to scale the solution to all product categories. Which solution meets these requirements?",
          options: {
            A: "Use prompt engineering with zero-shot learning.",
            B: "Use prompt engineering with prompt templates.",
            C: "Customize the model with continued pre-training.",
            D: "Customize the model with fine-tuning.",
          },
          correct_answer: "D",
          explanation:
            "Fine-tuning is the best approach when you have a labeled dataset and want to adapt a pre-trained model to specific tasks. It allows the model to learn from the collected data to improve performance across all product categories, rather than just prompt engineering or continued pre-training which wouldn't leverage the labeled data as effectively.",
          id: 342,
        },
        {
          number: "342",
          question:
            "A trading company wants to use a large language model （LLM） for financial market forecasts based on daily market data, news, and Comment company reports. Which solution meets these requirements?",
          options: {
            A: "Fine-tune the LLM on a domain-specific dataset until the LLM achieves 100% accuracy.",
            B: "Pick a general purpose LLM without modifications.",
            C: "Pick a general purpose LLM after evaluating the LLM on a test dataset.",
            D: "Continually pre-train an LLM on domain-specific knowledge.",
          },
          correct_answer: "D",
          explanation:
            "Continual pre-training on domain-specific knowledge is ideal for financial forecasting as it allows the model to continuously learn from evolving market data, news, and reports. This approach builds long-term knowledge in the financial domain rather than one-time fine-tuning or using unmodified general-purpose models.",
          id: 343,
        },
        {
          number: "343",
          question:
            "Which component of Amazon Bedrock Studio can help secure the content that AI systems generate?",
          options: {
            A: "Access controls",
            B: "Function calling",
            C: "Guardrails",
            D: "Knowledge bases",
          },
          correct_answer: "C",
          explanation:
            "Guardrails in Amazon Bedrock Studio are specifically designed to implement safeguards for generative AI applications. They help prevent the generation of harmful, inappropriate, or unsafe content by defining policies and filters that the model must follow when generating responses.",
          id: 344,
        },
        {
          number: "344",
          question:
            "An AI practitioner wants to deploy a chatbot that can assist customer service representatives by answering questions. The chatbot uses a large language model （LLM）。 The chatbot needs to produce fine-grained responses based on evolving product documentation. Which solution meets these requirements?",
          options: {
            A: "Apply continuous pre-training with product documentation to build a custom model.",
            B: "Use product documentation with Retrieval Augmented Generation （RAG） to update the knowledge base.",
            C: "Use transfer learning to train a generalized odel on various product domains.",
            D: "Collaborate with research institutions to develop a new foundation model （FM）。",
          },
          correct_answer: "B",
          explanation:
            "RAG (Retrieval Augmented Generation) is the best solution for chatbots that need to provide accurate, up-to-date responses based on evolving documentation. It retrieves relevant information from the knowledge base in real-time and augments the LLM's response, ensuring fine-grained and current answers without retraining the model.",
          id: 345,
        },
        {
          number: "345",
          question:
            "What does an AI practitioner do during the deployment phase of a generative AI model lifecycle?",
          options: {
            A: "Chooses the most useful and varied data to train the model",
            B: "Sets up and optimizes AWS Infrastructure for the live production environment",
            C: "Makes the model available in a live environment for end users to access",
            D: "Continuously trains the model on new data to improve accuracy",
          },
          correct_answer: "C",
          explanation:
            "The deployment phase of the AI model lifecycle involves making the trained model available in a live production environment where end users can access and interact with it. This includes setting up the necessary infrastructure, monitoring, and ensuring the model performs as expected in real-world conditions.",
          id: 346,
        },
        {
          number: "346",
          question: "Which strategy will prevent model hallucinations?",
          options: {
            A: "Fact-check the output of the large language model （LLM）。",
            B: "Compare the output of the large language model （LLM） to the results of an internet search.",
            C: "Use contextual grounding.",
            D: "Use relevance grounding.",
          },
          correct_answer: "C",
          explanation:
            "Contextual grounding prevents model hallucinations by ensuring the model's responses are based on provided context and source material rather than generating plausible but incorrect information. This technique anchors the model's outputs to verified, relevant information to improve accuracy and reliability.",
          id: 347,
        },
        {
          number: "347",
          question:
            "A company stores millions of PDF documents in an Amazon S3 bucket. The company needs to extract the text from the PDFs, generate summaries of the text, and index the summaries for fast searching. Which combination of AWS services will meet these requirements? （Select TWO.）",
          options: {
            A: "Amazon Translate",
            B: "Amazon Bedrock",
            C: "Amazon Transcribe",
            D: "Amazon Polly",
            E: "Amazon Textract",
          },
          correct_answer: "BE",
          explanation:
            "Amazon Textract is used to extract text and data from PDF documents automatically using OCR and natural language processing. Amazon Bedrock provides access to foundation models that can be used to generate summaries of the extracted text. These two services together enable text extraction from PDFs and summary generation for indexing and fast searching.",
          id: 348,
        },
        {
          number: "348",
          question: "What is continued pre-training?",
          options: {
            A: "The process of fine-tuning a pre-trained language model on labeled data for a specific task",
            B: "The process of providing unlabeled data to a pre-trained language model to improve the model's domain knowledge",
            C: "The process of training a language model from the beginning on a specific dataset",
            D: "The process of evaluating the performance of a pre-trained language model on a test set",
          },
          correct_answer: "B",
          explanation:
            "Continued pre-training involves providing additional unlabeled data to a pre-trained language model to enhance its understanding of specific domains or contexts. This process helps the model adapt to new areas without requiring labeled data, making it more effective for domain-specific applications.",
          id: 349,
        },
        {
          number: "349",
          question:
            "An AI practitioner is training an ML model on a large dataset of financial market data to predict stock prices. The model is overfitting and generating incorrect predictions. Which parameter should the AI practitioner adjust to improve prediction accuracy?",
          options: {
            A: "Weights and biases",
            B: "Learning rate",
            C: "Activation function",
            D: "Pooling size",
          },
          correct_answer: "B",
          explanation:
            "Overfitting in machine learning models often occurs when the model memorizes training data too well, leading to poor generalization. Adjusting the learning rate can help control the model's convergence speed and prevent overfitting by finding a better balance between underfitting and overfitting during training.",
          id: 350,
        },
        {
          number: "350",
          question:
            "A company makes electronic record software. Users sometimes enter incomplete sentences in the records. The company wants to use ML to automatically complete the sentences. Which solution meets these requirements?",
          options: {
            A: "Use a large language model（LLM）trained on data from the records.",
            B: "Use a computer vision model trained on images and related metadata.",
            C: "Use a k-nearest neighbors （k-NN）algorithm to classify input terms.",
            D: "Use reinforcement learning with input data to produce full sentences.",
          },
          correct_answer: "A",
          explanation:
            "Large Language Models (LLMs) are specifically designed for natural language processing tasks, including text completion. Training or fine-tuning an LLM on the company's specific record data would enable it to understand the context and patterns in incomplete sentences, making it the most suitable solution for automatic sentence completion.",
          id: 351,
        },
        {
          number: "351",
          question:
            "A company needs to automate the discovery of sensitive data in an Amazon S3 bucket. Which solution will meet this requirement?",
          options: {
            A: "Configure S3 Object Lambda for the S3 bucket.",
            B: "Configure server-side encryption with AWS KMS keys（SSE-KMS）for the S3 bucket.",
            C: "Configure Amazon CloudWatch alarms for the S3 bucket.",
            D: "Configure Amazon Macie for the S3 bucket.",
          },
          correct_answer: "D",
          explanation:
            "Amazon Macie is a fully managed data security and data privacy service that uses machine learning and pattern matching to discover and protect sensitive data in AWS. It can automatically scan S3 buckets to identify sensitive information such as personally identifiable information (PII), financial data, and credentials.",
          id: 352,
        },
      ];
    </script>
  </head>
  <body>
    <div class="container">
      <header>
        <h1>AWS Certified AI Practitioner AIF-C01 答题系统</h1>
        <div class="progress-info">
          <div class="progress-bar">
            <div class="progress-fill"></div>
          </div>
          <div class="stats">
            <span class="current-question">题目 1 / 351</span>
            <span class="score">答对: 0 / 0</span>
            <span class="accuracy">正确率: 0%</span>
            <span class="avg-time">平均每题: 0秒</span>
            <span class="total-time">总用时: 00:00:00</span>
            <span class="estimated-time">预计剩余: --</span>
          </div>
        </div>
      </header>

      <main>
        <div class="question-container">
          <div class="question-header">
            <h2 class="question-number">
              Question <span id="q-number">1</span>
            </h2>
            <div class="navigation">
              <button id="prev-btn" onclick="previousQuestion()">上一题</button>
              <button id="random-btn" onclick="randomQuestion()">
                随机题目
              </button>
              <button id="next-btn" onclick="nextQuestion()">下一题</button>
              <button
                id="reset-btn"
                onclick="resetQuiz()"
                style="
                  background: linear-gradient(135deg, #ef4444, #dc2626);
                  margin-left: auto;
                "
              >
                重置进度
              </button>
            </div>
          </div>

          <div class="question-content">
            <p class="question-text" id="question-text"></p>

            <div class="options">
              <div class="option" data-option="A">
                <input type="radio" id="option-a" name="answer" value="A" />
                <label for="option-a">
                  <span class="option-letter">A</span>
                  <span class="option-text" id="option-a-text"></span>
                </label>
              </div>

              <div class="option" data-option="B">
                <input type="radio" id="option-b" name="answer" value="B" />
                <label for="option-b">
                  <span class="option-letter">B</span>
                  <span class="option-text" id="option-b-text"></span>
                </label>
              </div>

              <div class="option" data-option="C">
                <input type="radio" id="option-c" name="answer" value="C" />
                <label for="option-c">
                  <span class="option-letter">C</span>
                  <span class="option-text" id="option-c-text"></span>
                </label>
              </div>

              <div class="option" data-option="D">
                <input type="radio" id="option-d" name="answer" value="D" />
                <label for="option-d">
                  <span class="option-letter">D</span>
                  <span class="option-text" id="option-d-text"></span>
                </label>
              </div>

              <div class="option" data-option="E">
                <input type="radio" id="option-e" name="answer" value="E" />
                <label for="option-e">
                  <span class="option-letter">E</span>
                  <span class="option-text" id="option-e-text"></span>
                </label>
              </div>
            </div>
          </div>

          <div class="question-actions">
            <button id="submit-btn">提交</button>
            <!-- Toast 容器 -->
            <div id="toast" class="toast"></div>
          </div>

          <div class="explanation" id="explanation" style="display: none">
            <h3>解释</h3>
            <p id="explanation-text"></p>
          </div>
        </div>

        <div class="question-nav">
          <h3>题目导航</h3>
          <div class="nav-grid" id="nav-grid">
            <!-- 题目导航按钮将通过JavaScript动态生成 -->
          </div>
        </div>
      </main>

      <footer>
        <p>AWS Certified AI Practitioner AIF-C01 练习题库</p>
      </footer>
    </div>

    <script>
      // AWS Certified AI Practitioner AIF-C01 答题系统 JavaScript

      class QuizApp {
        constructor() {
          this.questions = [];
          this.currentQuestionIndex = 0;
          this.userAnswers = {};
          this.correctAnswers = 0;
          this.totalAnswered = 0;
          this.showingExplanation = false;

          // Timer related properties
          this.questionStartTime = null;
          this.currentQuestionElapsedTime = 0;
          this.timerInterval = null;
          this.isTimerPaused = false;
          this.questionTimings = {}; // Store time spent on each question

          // Total time tracker
          this.totalTimeElapsed = 0; // Total time in milliseconds
          this.totalTimeStartTime = null;
          this.totalTimeInterval = null;
          this.isTotalTimerPaused = false;

          // Auto-save interval
          this.autoSaveInterval = null;

          // Store shuffled option orders for each question
          this.questionOptionShuffles = {};

          // 从localStorage加载保存的进度
          this.loadProgress();

          this.init();
        }

        // 保存进度到localStorage
        saveProgress() {
          // Save current total time state
          if (this.totalTimeStartTime && !this.isTotalTimerPaused) {
            const elapsed = Date.now() - this.totalTimeStartTime;
            this.totalTimeElapsed += elapsed;
            this.totalTimeStartTime = Date.now(); // Reset start time
          }

          const progressData = {
            currentQuestionIndex: this.currentQuestionIndex,
            userAnswers: this.userAnswers,
            correctAnswers: this.correctAnswers,
            totalAnswered: this.totalAnswered,
            questionTimings: this.questionTimings,
            totalTimeElapsed: this.totalTimeElapsed,
            timestamp: new Date().toISOString(),
          };

          try {
            localStorage.setItem(
              "aws-ai-quiz-progress",
              JSON.stringify(progressData)
            );
          } catch (error) {
            console.warn("无法保存进度到localStorage:", error);
          }
        }

        // 从localStorage加载进度
        loadProgress() {
          try {
            const savedProgress = localStorage.getItem("aws-ai-quiz-progress");
            if (savedProgress) {
              const progressData = JSON.parse(savedProgress);

              // 检查数据是否有效且不是太旧（7天内）
              const saveTime = new Date(progressData.timestamp);
              const now = new Date();
              const daysDiff = (now - saveTime) / (1000 * 60 * 60 * 24);

              if (daysDiff < 7) {
                this.currentQuestionIndex =
                  progressData.currentQuestionIndex || 0;
                this.userAnswers = progressData.userAnswers || {};
                this.correctAnswers = progressData.correctAnswers || 0;
                this.totalAnswered = progressData.totalAnswered || 0;
                this.questionTimings = progressData.questionTimings || {};
                this.totalTimeElapsed = progressData.totalTimeElapsed || 0;
                console.log("已恢复之前的答题进度");
              } else {
                // 进度太旧，清除掉
                this.clearProgress();
                console.log("进度已过期，已清除");
              }
            }
          } catch (error) {
            console.warn("无法加载保存的进度:", error);
          }
        }

        // 清除保存的进度
        clearProgress() {
          try {
            localStorage.removeItem("aws-ai-quiz-progress");
            // 重置所有数据
            this.currentQuestionIndex = 0;
            this.userAnswers = {};
            this.correctAnswers = 0;
            this.totalAnswered = 0;
            this.showingExplanation = false;
            this.questionTimings = {};
            this.questionOptionShuffles = {}; // Clear shuffled option orders
            this.stopTimer();

            // Reset total timer
            this.stopTotalTimer();
            this.totalTimeElapsed = 0;
            this.updateTotalTimeDisplay(0);
            this.startTotalTimer();

            console.log("答题进度已重置");
          } catch (error) {
            console.warn("无法清除保存的进度:", error);
          }
        }

        // 添加重置按钮功能
        resetQuiz() {
          if (confirm("确定要重置所有答题进度吗？此操作不可撤销。")) {
            this.clearProgress();

            // Jump to a random question after reset
            const randomIndex = Math.floor(
              Math.random() * this.questions.length
            );
            this.loadQuestion(randomIndex);

            this.generateNavigation();
            this.updateProgress();

            // Clear progress again after loadQuestion (which auto-saves)
            localStorage.removeItem("aws-ai-quiz-progress");

            alert("答题进度已重置！");
          }
        }

        async init() {
          try {
            // 使用内嵌的题目数据
            this.questions = window.questionsData;

            if (this.questions.length === 0) {
              throw new Error("没有找到题目数据");
            }

            this.setupEventListeners();
            this.setupVisibilityListener();

            // Update total time display from saved progress before starting timer
            const initialTotalSeconds = Math.floor(
              this.totalTimeElapsed / 1000
            );
            this.updateTotalTimeDisplay(initialTotalSeconds);

            this.startTotalTimer();
            this.startAutoSave();
            this.loadQuestion(0);
            this.generateNavigation();
            this.updateProgress();
          } catch (error) {
            console.error("加载题目数据失败:", error);
            alert("加载题目数据失败，请检查 questions.json 文件是否存在");
          }
        }

        // Setup page visibility listener to pause/resume timer
        setupVisibilityListener() {
          document.addEventListener("visibilitychange", () => {
            if (document.hidden) {
              // Page is hidden, pause timer
              this.pauseTimer();
              this.pauseTotalTimer();
            } else {
              // Page is visible again, resume timer
              this.resumeTimer();
              this.resumeTotalTimer();
            }
          });

          // Save progress before page unload
          window.addEventListener("beforeunload", () => {
            this.saveProgress();
          });
        }

        // Start auto-save interval (save progress every 5 seconds)
        startAutoSave() {
          // Clear any existing auto-save interval
          if (this.autoSaveInterval) {
            clearInterval(this.autoSaveInterval);
          }

          // Save progress every 5 seconds
          this.autoSaveInterval = setInterval(() => {
            this.saveProgress();
            console.log("自动保存进度");
          }, 5000);
        }

        // Stop auto-save interval
        stopAutoSave() {
          if (this.autoSaveInterval) {
            clearInterval(this.autoSaveInterval);
            this.autoSaveInterval = null;
          }
        }

        // Start total time timer
        startTotalTimer() {
          this.totalTimeStartTime = Date.now();
          this.isTotalTimerPaused = false;

          // Update display every second
          this.totalTimeInterval = setInterval(() => {
            if (!this.isTotalTimerPaused && this.totalTimeStartTime) {
              const currentElapsed = Date.now() - this.totalTimeStartTime;
              const totalSeconds = Math.floor(
                (this.totalTimeElapsed + currentElapsed) / 1000
              );
              this.updateTotalTimeDisplay(totalSeconds);
            }
          }, 1000);
        }

        // Stop total timer
        stopTotalTimer() {
          if (this.totalTimeStartTime && !this.isTotalTimerPaused) {
            const elapsed = Date.now() - this.totalTimeStartTime;
            this.totalTimeElapsed += elapsed;
          }
          this.totalTimeStartTime = null;

          if (this.totalTimeInterval) {
            clearInterval(this.totalTimeInterval);
            this.totalTimeInterval = null;
          }
        }

        // Pause total timer (when page becomes hidden)
        pauseTotalTimer() {
          if (this.totalTimeStartTime && !this.isTotalTimerPaused) {
            const elapsed = Date.now() - this.totalTimeStartTime;
            this.totalTimeElapsed += elapsed;
            this.isTotalTimerPaused = true;
          }
        }

        // Resume total timer (when page becomes visible)
        resumeTotalTimer() {
          if (this.isTotalTimerPaused) {
            this.totalTimeStartTime = Date.now();
            this.isTotalTimerPaused = false;
          }
        }

        // Update total time display
        updateTotalTimeDisplay(totalSeconds) {
          const hours = Math.floor(totalSeconds / 3600);
          const minutes = Math.floor((totalSeconds % 3600) / 60);
          const seconds = totalSeconds % 60;

          const timeString = `${String(hours).padStart(2, "0")}:${String(
            minutes
          ).padStart(2, "0")}:${String(seconds).padStart(2, "0")}`;

          const totalTimeElement = document.querySelector(".total-time");
          if (totalTimeElement) {
            totalTimeElement.textContent = `总用时: ${timeString}`;
          }
        }

        // Start timer for current question
        startTimer() {
          // Clear any existing timer
          this.stopTimer();

          // Initialize timing for this question if not exists
          if (!this.questionTimings[this.currentQuestionIndex]) {
            this.questionTimings[this.currentQuestionIndex] = 0;
          }

          this.questionStartTime = Date.now();
          this.currentQuestionElapsedTime =
            this.questionTimings[this.currentQuestionIndex];
          this.isTimerPaused = false;
        }

        // Stop timer and save elapsed time
        stopTimer() {
          if (this.questionStartTime && !this.isTimerPaused) {
            const elapsed = Date.now() - this.questionStartTime;
            this.currentQuestionElapsedTime += elapsed;
            this.questionTimings[this.currentQuestionIndex] =
              this.currentQuestionElapsedTime;
          }
          this.questionStartTime = null;
        }

        // Pause timer (when page becomes hidden)
        pauseTimer() {
          if (this.questionStartTime && !this.isTimerPaused) {
            const elapsed = Date.now() - this.questionStartTime;
            this.currentQuestionElapsedTime += elapsed;
            this.questionTimings[this.currentQuestionIndex] =
              this.currentQuestionElapsedTime;
            this.isTimerPaused = true;
          }
        }

        // Resume timer (when page becomes visible)
        resumeTimer() {
          if (this.isTimerPaused) {
            this.questionStartTime = Date.now();
            this.isTimerPaused = false;
          }
        }

        // Calculate average answer time
        calculateAverageTime() {
          const answeredQuestions = Object.keys(this.userAnswers);
          if (answeredQuestions.length === 0) return 0;

          let totalTime = 0;
          let count = 0;

          answeredQuestions.forEach((index) => {
            if (this.questionTimings[index]) {
              totalTime += this.questionTimings[index];
              count++;
            }
          });

          return count > 0 ? Math.round(totalTime / count / 1000) : 0; // Convert to seconds
        }

        // Format time in seconds to readable format
        formatTime(seconds) {
          if (seconds < 60) {
            return `${seconds}秒`;
          } else {
            const mins = Math.floor(seconds / 60);
            const secs = seconds % 60;
            return `${mins}分${secs}秒`;
          }
        }

        setupEventListeners() {
          // 初始化时设置正确的input类型（因为HTML中默认是radio）
          this.initializeInputTypes();

          // 为选项添加点击事件
          document.querySelectorAll(".option").forEach((option) => {
            option.addEventListener("click", (e) => {
              if (!this.showingExplanation) {
                // 检查是否已经回答过这道题
                if (this.userAnswers[this.currentQuestionIndex] !== undefined) {
                  this.showToast(
                    "✗ 这道题你已经回答过了，无法修改答案",
                    "error",
                    3000
                  );
                  return;
                }

                const question = this.questions[this.currentQuestionIndex];
                const hasEmptyOptions =
                  !question.options ||
                  Object.keys(question.options).length === 0;
                const currentQuestionIsMultiChoice = this.isMultiChoiceQuestion(
                  question.correct_answer
                );
                const input = option.querySelector('input[name="answer"]');

                // 如果选项为空，不允许点击选择
                if (hasEmptyOptions) {
                  return;
                }

                if (currentQuestionIsMultiChoice) {
                  // 多选题：切换选项选中状态
                  input.checked = !input.checked;
                  if (input.checked) {
                    option.classList.add("selected");
                  } else {
                    option.classList.remove("selected");
                  }
                } else {
                  // 单选题：单选模式
                  document
                    .querySelectorAll('input[name="answer"]')
                    .forEach((radio) => {
                      radio.checked = false;
                    });
                  document.querySelectorAll(".option").forEach((opt) => {
                    opt.classList.remove("selected");
                  });
                  input.checked = true;
                  option.classList.add("selected");
                }
              }
            });
          });

          // 为提交按钮添加点击事件
          const submitBtn = document.getElementById("submit-btn");
          if (submitBtn) {
            submitBtn.addEventListener("click", () => {
              this.submitAnswer();
            });
          }

          // 添加键盘事件支持
          document.addEventListener("keydown", (e) => {
            const currentQuestion = this.questions[this.currentQuestionIndex];
            const isDropdownMultiple =
              currentQuestion.type === "dropdown_multiple";

            // 下拉选择题型不支持数字键选择，但支持Enter键
            if (isDropdownMultiple && e.key >= "1" && e.key <= "5") {
              return;
            }

            if (e.key >= "1" && e.key <= "5" && !this.showingExplanation) {
              // 检查是否已经回答过这道题
              if (this.userAnswers[this.currentQuestionIndex] !== undefined) {
                this.showToast(
                  "✗ 这道题你已经回答过了，无法修改答案",
                  "error",
                  3000
                );
                return;
              }

              const question = this.questions[this.currentQuestionIndex];
              const hasEmptyOptions =
                !question.options || Object.keys(question.options).length === 0;

              // 如果选项为空，不允许键盘选择
              if (hasEmptyOptions) {
                return;
              }

              // 检查对应的选项是否存在
              const optionLetter = String.fromCharCode(64 + parseInt(e.key));
              const optionText = question.options[optionLetter];
              if (!optionText || optionText.trim() === "") {
                return; // 选项不存在，不处理键盘输入
              }

              const currentQuestionIsMultiChoice = this.isMultiChoiceQuestion(
                question.correct_answer
              );

              const option = document.querySelector(
                `input[value="${optionLetter}"]`
              );
              if (
                option &&
                option.closest(".option").style.display !== "none"
              ) {
                if (currentQuestionIsMultiChoice) {
                  // 多选题：切换选项选中状态
                  option.checked = !option.checked;
                  if (option.checked) {
                    option.closest(".option").classList.add("selected");
                  } else {
                    option.closest(".option").classList.remove("selected");
                  }
                } else {
                  // 单选题：单选模式
                  document
                    .querySelectorAll('input[name="answer"]')
                    .forEach((input) => {
                      input.checked = false;
                    });
                  document.querySelectorAll(".option").forEach((opt) => {
                    opt.classList.remove("selected");
                  });
                  option.checked = true;
                  option.closest(".option").classList.add("selected");
                }
              }
            } else if (e.key === "Enter") {
              if (this.showingExplanation) {
                this.nextQuestion();
              } else {
                this.submitAnswer();
              }
            }
          });
        }

        // Get or generate shuffled option order for a question
        getShuffledOptions(questionIndex, availableOptions) {
          // If we already have a shuffle for this question, return it
          if (this.questionOptionShuffles[questionIndex]) {
            return this.questionOptionShuffles[questionIndex];
          }

          // Generate a new shuffle
          const shuffled = [...availableOptions];
          for (let i = shuffled.length - 1; i > 0; i--) {
            const j = Math.floor(Math.random() * (i + 1));
            [shuffled[i], shuffled[j]] = [shuffled[j], shuffled[i]];
          }

          // Store the shuffle for this question
          this.questionOptionShuffles[questionIndex] = shuffled;
          return shuffled;
        }

        loadQuestion(index) {
          if (index < 0 || index >= this.questions.length) return;

          this.currentQuestionIndex = index;

          // 保存进度
          this.saveProgress();
          const question = this.questions[index];

          // 更新题目编号和内容
          const qNumberElement = document.getElementById("q-number");
          qNumberElement.textContent = question.number;

          // 如果题目已经被回答过，添加视觉提示
          if (this.userAnswers[index] !== undefined) {
            qNumberElement.classList.add("answered");
            qNumberElement.title = "此题已回答";
          } else {
            qNumberElement.classList.remove("answered");
            qNumberElement.title = "";
          }

          // 检查题目选项是否为空
          const hasEmptyOptions =
            !question.options || Object.keys(question.options).length === 0;

          // 如果是多选题，在题目文本前添加提示
          const questionTextElement = document.getElementById("question-text");
          const isMultiChoice = this.isMultiChoiceQuestion(
            question.correct_answer
          );

          let questionText = question.question;
          if (isMultiChoice) {
            questionText = "【多选题】" + questionText;
          }
          if (hasEmptyOptions) {
            questionText = "【请查看原题】" + questionText;
          }

          questionTextElement.textContent = questionText;

          // 检查是否为下拉选择题型
          const isDropdownMultiple = question.type === "dropdown_multiple";

          // 更新选项
          if (hasEmptyOptions) {
            // 选项为空，显示提示信息
            document.getElementById("option-a-text").textContent =
              "请查看题目原文中的热点区域";
            document.getElementById("option-b-text").textContent = "";
            document.getElementById("option-c-text").textContent = "";
            document.getElementById("option-d-text").textContent = "";
            document.getElementById("option-e-text").textContent = "";
            // 隐藏选项容器或使其不可见
            document.querySelectorAll(".option").forEach((opt, index) => {
              if (index === 0) {
                opt.style.display = "block";
                opt.style.opacity = "0.7";
              } else {
                opt.style.display = "none";
              }
            });
          } else if (isDropdownMultiple) {
            // 下拉选择题型：显示场景和下拉选择框
            this.renderDropdownMultipleQuestion(question);
          } else {
            // 正常显示选项，但只显示存在的选项
            // First, ensure the standard options HTML structure exists
            this.ensureStandardOptionsStructure();

            const options = question.options || {};

            // Get available option keys that have content
            const availableOptionKeys = ["A", "B", "C", "D", "E"].filter(
              (key) => options[key] && options[key].trim() !== ""
            );

            // Get shuffled order for this question
            const shuffledKeys = this.getShuffledOptions(
              index,
              availableOptionKeys
            );

            // Map display positions to original option keys
            const displayLabels = ["A", "B", "C", "D", "E"];
            const optionElements = [
              { id: "option-a", textId: "option-a-text", value: "A" },
              { id: "option-b", textId: "option-b-text", value: "B" },
              { id: "option-c", textId: "option-c-text", value: "C" },
              { id: "option-d", textId: "option-d-text", value: "D" },
              { id: "option-e", textId: "option-e-text", value: "E" },
            ];

            optionElements.forEach((optElement, displayIndex) => {
              const optionDiv = document.querySelector(
                `[data-option="${optElement.value}"]`
              );
              const textElement = document.getElementById(optElement.textId);

              if (displayIndex < shuffledKeys.length) {
                // Get the original option key for this display position
                const originalKey = shuffledKeys[displayIndex];
                const optionText = options[originalKey];

                // Store the mapping from display position to original key
                optionDiv.setAttribute("data-original-key", originalKey);

                // Display the option
                textElement.textContent = optionText;
                optionDiv.style.display = "block";
                optionDiv.style.opacity = "1";
              } else {
                // Hide unused positions
                optionDiv.style.display = "none";
                optionDiv.removeAttribute("data-original-key");
              }
            });
          }

          // 根据题目类型设置input元素类型
          const currentQuestionIsMultiChoice = this.isMultiChoiceQuestion(
            question.correct_answer
          );

          if (isDropdownMultiple) {
            // 下拉选择题型：隐藏传统选项，不需要input元素
            document.querySelectorAll(".option").forEach((opt) => {
              opt.style.display = "none";
            });
          } else {
            const inputElements = document.querySelectorAll(
              'input[name="answer"]'
            );

            inputElements.forEach((input) => {
              if (currentQuestionIsMultiChoice) {
                // 多选题：使用checkbox
                input.type = "checkbox";
                input.name = "answer"; // 保持name相同以便同时选中多个
              } else {
                // 单选题：使用radio
                input.type = "radio";
                input.name = "answer"; // 保持name相同以便互斥选择
              }
            });
          }

          // 清空所有选择状态和高亮样式
          document.querySelectorAll('input[name="answer"]').forEach((input) => {
            input.checked = false;
          });
          document.querySelectorAll(".option").forEach((opt) => {
            opt.classList.remove("selected", "correct-answer", "wrong-answer");
          });

          // 检查是否已经回答过这道题
          const hasAnswered = this.userAnswers[index] !== undefined;

          // 清除下拉选择题型的答案对比显示（仅对未回答的题目）
          if (isDropdownMultiple && !hasAnswered) {
            document
              .querySelectorAll(".answer-comparison")
              .forEach((comparison) => {
                comparison.remove();
              });
            document.querySelectorAll(".scenario-item").forEach((item) => {
              item.classList.remove("scenario-correct", "scenario-wrong");
            });
            document
              .querySelectorAll(".dropdown-answer")
              .forEach((dropdown) => {
                dropdown.classList.remove("correct", "wrong");
              });
          }

          // 如果已经回答过，高亮显示用户的选择和正确答案；否则重置为答题模式
          if (hasAnswered) {
            const userAnswer = this.userAnswers[index];
            const currentQ = this.questions[index];
            // Get correct answer based on question type
            const correctAnswer =
              currentQ.type === "dropdown_multiple"
                ? currentQ.correct_answer
                : currentQ.correct_answer;
            console.log(
              "Loading answered question:",
              index,
              "User answer:",
              userAnswer,
              "Correct answer:",
              correctAnswer
            );

            // 使用统一的高亮方法
            this.highlightAnswerOptions(userAnswer, correctAnswer);

            // 显示答案结果
            this.showingExplanation = true;
            document.getElementById("explanation").style.display = "block";
            document.getElementById("explanation-text").textContent =
              this.questions[index].explanation;

            // 对于已答题目，隐藏提交按钮
            const submitBtn = document.getElementById("submit-btn");
            submitBtn.style.display = "none";
          } else {
            // 重置为答题模式
            this.showingExplanation = false;
            document.getElementById("explanation").style.display = "none";

            // 显示提交按钮
            const submitBtn = document.getElementById("submit-btn");
            submitBtn.style.display = "block";
            submitBtn.textContent = "提交";
          }

          // 更新导航按钮状态
          this.updateNavigationButtons();

          // 清除之前的高亮样式（但保留答题状态样式）
          document.querySelectorAll(".nav-btn").forEach((btn) => {
            btn.classList.remove("current");
          });

          // 高亮当前题目导航按钮
          const navBtn = document.querySelector(
            `.nav-btn[data-question="${index}"]`
          );
          if (navBtn) {
            navBtn.classList.add("current");
          }

          // Start timer for this question if not already answered
          if (this.userAnswers[index] === undefined) {
            this.startTimer();
          } else {
            // Stop timer if question is already answered
            this.stopTimer();
          }
        }

        updateNavigationButtons() {
          document.getElementById("prev-btn").disabled =
            this.currentQuestionIndex === 0;
          document.getElementById("next-btn").disabled =
            this.currentQuestionIndex === this.questions.length - 1;
        }

        submitAnswer() {
          // 检查是否已经回答过这道题
          if (this.userAnswers[this.currentQuestionIndex] !== undefined) {
            this.showToast(
              "✗ 这道题你已经回答过了，无法修改答案",
              "error",
              3000
            );
            return;
          }

          const question = this.questions[this.currentQuestionIndex];
          const isDropdownMultiple = question.type === "dropdown_multiple";
          const hasEmptyOptions =
            !question.options || Object.keys(question.options).length === 0;
          const currentQuestionIsMultiChoice = this.isMultiChoiceQuestion(
            question.correct_answer
          );

          if (hasEmptyOptions) {
            // 选项为空的题目，直接提交算作正确
            this.submitAnswerWithValidation("EMPTY", question.correct_answer);
          } else if (isDropdownMultiple) {
            // 下拉选择题型：收集所有下拉选择的值
            const dropdowns = document.querySelectorAll(".dropdown-answer");
            const answers = {};
            let allAnswered = true;

            dropdowns.forEach((dropdown) => {
              const scenarioId = dropdown.dataset.scenarioId;
              const value = dropdown.value;
              answers[scenarioId] = value;
              if (!value) {
                allAnswered = false;
              }
            });

            if (!allAnswered) {
              this.showToast("请为所有场景选择答案", "error", 2000);
              return;
            }

            this.submitAnswerWithValidation(
              JSON.stringify(answers),
              question.correct_answer
            );
          } else if (currentQuestionIsMultiChoice) {
            // 多选题：收集所有选中的选项
            const selectedOptions = document.querySelectorAll(
              'input[name="answer"]:checked'
            );
            if (selectedOptions.length === 0) {
              this.showToast("请至少选择一个答案", "error", 2000);
              return;
            }

            // Map display values to original keys
            const answers = Array.from(selectedOptions)
              .map((option) => {
                const optionDiv = option.closest(".option");
                return (
                  optionDiv.getAttribute("data-original-key") || option.value
                );
              })
              .sort()
              .join("");
            this.submitAnswerWithValidation(answers, question.correct_answer);
          } else {
            // 单选题：检查是否选中了一个选项
            const selectedOption = document.querySelector(
              'input[name="answer"]:checked'
            );
            if (!selectedOption) {
              this.showToast("请选择一个答案", "error", 2000);
              return;
            }

            // Map display value to original key
            const optionDiv = selectedOption.closest(".option");
            const originalKey =
              optionDiv.getAttribute("data-original-key") ||
              selectedOption.value;

            this.submitAnswerWithValidation(
              originalKey,
              question.correct_answer
            );
          }
        }

        submitAnswerWithValidation(userAnswer, correctAnswer) {
          // Stop timer when answer is submitted
          this.stopTimer();

          // 检查当前题目选项是否为空
          const currentQuestion = this.questions[this.currentQuestionIndex];
          const isDropdownMultiple =
            currentQuestion.type === "dropdown_multiple";
          const hasEmptyOptions =
            !currentQuestion.options ||
            Object.keys(currentQuestion.options).length === 0;

          if (hasEmptyOptions) {
            // 选项为空的题目，直接算作正确
            this.userAnswers[this.currentQuestionIndex] = userAnswer;
            this.saveProgress();
            this.showAnswerResult(userAnswer, correctAnswer);
            return;
          }

          // 记录用户答案
          this.userAnswers[this.currentQuestionIndex] = userAnswer;

          // 保存进度
          this.saveProgress();

          // 显示答案和解释
          this.showAnswerResult(userAnswer, correctAnswer);
        }

        // 获取题目类型图标
        getQuestionTypeIcon(question) {
          if (question.type === "dropdown_multiple") {
            return "▼"; // 下拉箭头图标
          } else if (this.isMultiChoiceQuestion(question.correct_answer)) {
            return "☐"; // 复选框图标（未选中状态，更清晰）
          } else {
            return "○"; // 单选框图标（空心圆，更清晰）
          }
        }

        // 判断题目是否为多选题（兼容下拉选择题型）
        isMultiChoiceQuestion(correctAnswer) {
          if (!correctAnswer) return false;
          if (typeof correctAnswer === "string") {
            return correctAnswer.length > 1;
          }
          return false;
        }

        // 判断答案是否正确（兼容不同题型）
        isAnswerCorrect(question, userAnswer) {
          if (!userAnswer) return false;

          if (question.type === "dropdown_multiple") {
            try {
              const userAnswers = JSON.parse(userAnswer);
              const correctAnswers = question.correct_answer;

              for (const scenarioId in correctAnswers) {
                if (userAnswers[scenarioId] !== correctAnswers[scenarioId]) {
                  return false;
                }
              }
              return true;
            } catch (e) {
              return false;
            }
          } else {
            return userAnswer === question.correct_answer;
          }
        }

        // 渲染下拉选择题型
        renderDropdownMultipleQuestion(question) {
          const optionsContainer = document.querySelector(".options");
          optionsContainer.innerHTML = "";

          // 添加题型标识
          const questionTextElement = document.getElementById("question-text");
          questionTextElement.textContent =
            "【下拉选择题】" + question.question;

          // 获取用户之前的答案（如果是已回答的题目）
          const currentIndex = this.currentQuestionIndex;
          const userAnswer = this.userAnswers[currentIndex];
          let userAnswersObj = {};
          if (userAnswer) {
            try {
              userAnswersObj = JSON.parse(userAnswer);
            } catch (e) {
              console.error("解析用户答案失败:", e);
            }
          }

          question.scenarios.forEach((scenario, index) => {
            const scenarioDiv = document.createElement("div");
            scenarioDiv.className = "scenario-item";

            // 获取用户对这个情景的选择
            const userChoice = userAnswersObj[scenario.id] || "";

            scenarioDiv.innerHTML = `
              <div class="scenario-text">${scenario.text}</div>
              <select class="dropdown-answer" data-scenario-id="${scenario.id}">
                <option value="">请选择...</option>
                ${question.options
                  .map(
                    (option) =>
                      `<option value="${option}" ${
                        option === userChoice ? "selected" : ""
                      }>${option}</option>`
                  )
                  .join("")}
              </select>
            `;
            optionsContainer.appendChild(scenarioDiv);

            // 设置下拉选择框的值
            if (userChoice) {
              const dropdown = scenarioDiv.querySelector(".dropdown-answer");
              dropdown.value = userChoice;
            }
          });

          // 添加样式
          this.addDropdownMultipleStyles();
        }

        // Ensure standard options HTML structure exists (for switching back from dropdown questions)
        ensureStandardOptionsStructure() {
          const optionsContainer = document.querySelector(".options");

          // Check if standard options already exist
          const optionA = document.querySelector('[data-option="A"]');
          if (optionA && optionA.querySelector('input[name="answer"]')) {
            // Standard structure already exists
            return;
          }

          // Recreate standard options structure
          const standardOptionsHTML = `
            <div class="option" data-option="A">
              <input type="radio" id="option-a" name="answer" value="A" />
              <label for="option-a">
                <span class="option-letter">A</span>
                <span class="option-text" id="option-a-text"></span>
              </label>
            </div>

            <div class="option" data-option="B">
              <input type="radio" id="option-b" name="answer" value="B" />
              <label for="option-b">
                <span class="option-letter">B</span>
                <span class="option-text" id="option-b-text"></span>
              </label>
            </div>

            <div class="option" data-option="C">
              <input type="radio" id="option-c" name="answer" value="C" />
              <label for="option-c">
                <span class="option-letter">C</span>
                <span class="option-text" id="option-c-text"></span>
              </label>
            </div>

            <div class="option" data-option="D">
              <input type="radio" id="option-d" name="answer" value="D" />
              <label for="option-d">
                <span class="option-letter">D</span>
                <span class="option-text" id="option-d-text"></span>
              </label>
            </div>

            <div class="option" data-option="E">
              <input type="radio" id="option-e" name="answer" value="E" />
              <label for="option-e">
                <span class="option-letter">E</span>
                <span class="option-text" id="option-e-text"></span>
              </label>
            </div>
          `;

          optionsContainer.innerHTML = standardOptionsHTML;

          // Re-attach click event listeners for options
          document.querySelectorAll(".option").forEach((opt) => {
            opt.addEventListener("click", (e) => {
              const input = opt.querySelector('input[name="answer"]');
              if (input && !this.userAnswers[this.currentQuestionIndex]) {
                const question = this.questions[this.currentQuestionIndex];
                const currentQuestionIsMultiChoice = this.isMultiChoiceQuestion(
                  question.correct_answer
                );

                if (currentQuestionIsMultiChoice) {
                  // Multi-choice: toggle checkbox
                  input.checked = !input.checked;
                  if (input.checked) {
                    opt.classList.add("selected");
                  } else {
                    opt.classList.remove("selected");
                  }
                } else {
                  // Single choice: select this option
                  document
                    .querySelectorAll('input[name="answer"]')
                    .forEach((inp) => {
                      inp.checked = false;
                    });
                  document.querySelectorAll(".option").forEach((o) => {
                    o.classList.remove("selected");
                  });
                  input.checked = true;
                  opt.classList.add("selected");
                }
              }
            });
          });
        }

        // 添加下拉选择题型的样式
        addDropdownMultipleStyles() {
          // 检查是否已经添加过样式
          if (document.getElementById("dropdown-multiple-styles")) {
            return;
          }

          const style = document.createElement("style");
          style.id = "dropdown-multiple-styles";
          style.textContent = `
            .scenario-item {
              display: flex;
              align-items: center;
              margin-bottom: 20px;
              padding: 15px;
              background: #f8f9fa;
              border-radius: 8px;
              border-left: 4px solid #667eea;
            }

            .scenario-text {
              flex: 1;
              margin-right: 20px;
              font-weight: 500;
              color: #333;
              line-height: 1.5;
            }

            .dropdown-answer {
              padding: 10px 15px;
              border: 2px solid #e5e7eb;
              border-radius: 6px;
              background: white;
              font-size: 14px;
              color: #333;
              cursor: pointer;
              min-width: 200px;
              transition: border-color 0.3s ease;
            }

            .dropdown-answer:focus {
              outline: none;
              border-color: #667eea;
              box-shadow: 0 0 0 3px rgba(102, 126, 234, 0.1);
            }

            .dropdown-answer:hover {
              border-color: #667eea;
            }

            .dropdown-answer.correct {
              border-color: #22c55e;
              background: linear-gradient(135deg, #f0fdf4, #dcfce7);
              color: #15803d;
              font-weight: 600;
              box-shadow: 0 0 0 2px rgba(34, 197, 94, 0.2);
            }

            .dropdown-answer.wrong {
              border-color: #ef4444;
              background: linear-gradient(135deg, #fef2f2, #fee2e2);
              color: #dc2626;
              font-weight: 600;
              box-shadow: 0 0 0 2px rgba(239, 68, 68, 0.2);
            }

            /* 为下拉选择框添加答案状态指示器 */
            .dropdown-answer.correct::after {
              content: " ✓";
              color: #22c55e;
              font-weight: bold;
            }

            .dropdown-answer.wrong::after {
              content: " ❌";
              color: #ef4444;
              font-weight: bold;
            }

            .scenario-correct {
              border-left-color: #22c55e;
              background: linear-gradient(135deg, #f0fdf4, #dcfce7);
              border-left-width: 5px;
            }

            .scenario-wrong {
              border-left-color: #ef4444;
              background: linear-gradient(135deg, #fef2f2, #fee2e2);
              border-left-width: 5px;
            }

            .answer-comparison {
              margin-top: 10px;
              padding: 10px 12px;
              background: linear-gradient(135deg, #f8fafc, #f1f5f9);
              border-radius: 6px;
              font-size: 13px;
              border: 1px solid #e2e8f0;
              display: flex;
              flex-direction: column;
              gap: 6px;
              box-shadow: 0 1px 3px rgba(0, 0, 0, 0.1);
            }

            .user-answer {
              color: #dc2626;
              font-weight: 600;
              display: flex;
              align-items: center;
              gap: 6px;
            }

            .user-answer::before {
              content: "❌";
              font-size: 14px;
            }

            .correct-answer {
              color: #059669;
              font-weight: 600;
              display: flex;
              align-items: center;
              gap: 6px;
            }

            .correct-answer::before {
              content: "✅";
              font-size: 14px;
            }
          `;
          document.head.appendChild(style);
        }

        // 高亮下拉选择题型的答案并显示答案对比
        highlightDropdownMultipleAnswers(userAnswer, correctAnswer) {
          try {
            const userAnswers = JSON.parse(userAnswer);
            const correctAnswers = correctAnswer;

            document
              .querySelectorAll(".scenario-item")
              .forEach((scenarioItem) => {
                const dropdown = scenarioItem.querySelector(".dropdown-answer");
                const scenarioText =
                  scenarioItem.querySelector(".scenario-text");

                if (dropdown && scenarioText) {
                  const scenarioId = dropdown.dataset.scenarioId;
                  const userValue = userAnswers[scenarioId];
                  const correctValue = correctAnswers[scenarioId];

                  if (userValue === correctValue) {
                    dropdown.classList.remove("wrong");
                    dropdown.classList.add("correct");
                    scenarioItem.classList.remove("scenario-wrong");
                    scenarioItem.classList.add("scenario-correct");
                  } else {
                    dropdown.classList.remove("correct");
                    dropdown.classList.add("wrong");
                    scenarioItem.classList.remove("scenario-correct");
                    scenarioItem.classList.add("scenario-wrong");

                    // 检查是否已经存在答案对比，如果存在则更新，否则创建
                    let answerComparison =
                      scenarioText.querySelector(".answer-comparison");
                    if (answerComparison) {
                      answerComparison.innerHTML = `
                        <div class="user-answer">${userValue || "未选择"}</div>
                        <div class="correct-answer">${correctValue}</div>
                      `;
                    } else {
                      answerComparison = document.createElement("div");
                      answerComparison.className = "answer-comparison";
                      answerComparison.innerHTML = `
                        <div class="user-answer">${userValue || "未选择"}</div>
                        <div class="correct-answer">${correctValue}</div>
                      `;
                      scenarioText.appendChild(answerComparison);
                    }
                  }
                }
              });
          } catch (e) {
            console.error("Error highlighting dropdown multiple answers:", e);
          }
        }

        // 初始化input元素类型
        initializeInputTypes() {
          // 初始时默认设置为单选题类型（radio）
          const inputElements = document.querySelectorAll(
            'input[name="answer"]'
          );
          inputElements.forEach((input) => {
            input.type = "radio";
          });
        }

        // 高亮显示答案选项
        highlightAnswerOptions(userAnswer, correctAnswer) {
          console.log(
            "highlightAnswerOptions called with:",
            userAnswer,
            correctAnswer
          );

          // 检查当前题目选项是否为空
          const currentQuestion = this.questions[this.currentQuestionIndex];
          const isDropdownMultiple =
            currentQuestion.type === "dropdown_multiple";
          const hasEmptyOptions =
            !currentQuestion.options ||
            Object.keys(currentQuestion.options).length === 0;

          // 如果选项为空或为下拉选择题型，不进行传统选项高亮
          if (hasEmptyOptions) {
            console.log("Options are empty, skipping highlight");
            return;
          }

          if (isDropdownMultiple) {
            // 下拉选择题型：高亮下拉选择框
            this.highlightDropdownMultipleAnswers(userAnswer, correctAnswer);
            return;
          }

          // 先清除所有选项的高亮
          document.querySelectorAll(".option").forEach((opt) => {
            opt.classList.remove("selected", "correct-answer", "wrong-answer");
          });

          const isMultiChoice = this.isMultiChoiceQuestion(correctAnswer);

          if (isMultiChoice) {
            // 多选题：分别处理每个选项
            const correctAnswers = correctAnswer.split("");

            // 高亮所有正确答案（绿色）
            correctAnswers.forEach((answer) => {
              // Find option by original key
              const correctOptionDiv = document.querySelector(
                `.option[data-original-key="${answer}"]`
              );
              if (
                correctOptionDiv &&
                correctOptionDiv.style.display !== "none"
              ) {
                correctOptionDiv.classList.add("correct-answer");
              }
            });

            // 高亮用户选择的选项
            if (userAnswer) {
              const userAnswers = userAnswer.split("");
              userAnswers.forEach((answer) => {
                // Find option by original key
                const userOptionDiv = document.querySelector(
                  `.option[data-original-key="${answer}"]`
                );
                if (userOptionDiv && userOptionDiv.style.display !== "none") {
                  const userOption = userOptionDiv.querySelector(
                    'input[name="answer"]'
                  );
                  if (userOption) {
                    userOption.checked = true;
                  }
                  userOptionDiv.classList.add("selected");

                  // 如果用户选择的选项是正确答案，显示绿色；否则显示红色
                  if (correctAnswers.includes(answer)) {
                    userOptionDiv.classList.add("correct-answer");
                  } else {
                    userOptionDiv.classList.add("wrong-answer");
                  }
                }
              });
            }
          } else {
            // 单选题：原有逻辑
            // 高亮用户选择的选项
            const userOptionDiv = document.querySelector(
              `.option[data-original-key="${userAnswer}"]`
            );
            if (userOptionDiv && userOptionDiv.style.display !== "none") {
              const userOption = userOptionDiv.querySelector(
                'input[name="answer"]'
              );
              if (userOption) {
                userOption.checked = true;
              }
              userOptionDiv.classList.add("selected");

              // 根据对错添加不同颜色
              if (userAnswer === correctAnswer) {
                userOptionDiv.classList.add("correct-answer");
              } else {
                userOptionDiv.classList.add("wrong-answer");
              }
            }

            // 如果答错了，高亮正确答案
            if (userAnswer !== correctAnswer) {
              const correctOptionDiv = document.querySelector(
                `.option[data-original-key="${correctAnswer}"]`
              );
              console.log(
                "Looking for correct answer:",
                correctAnswer,
                "Found element:",
                correctOptionDiv
              );
              if (
                correctOptionDiv &&
                correctOptionDiv.style.display !== "none"
              ) {
                correctOptionDiv.classList.add("correct-answer");
                console.log("Added correct-answer class to:", correctOptionDiv);
              }
            }
          }
        }

        showToast(message, type, duration = 3000) {
          const toast = document.getElementById("toast");
          const icon = type === "success" ? "✓" : "✗";
          toast.innerHTML = `<span class="toast-icon">${icon}</span><span class="toast-content">${message}</span>`;
          toast.className = `toast ${type}`;

          // 显示toast
          setTimeout(() => {
            toast.classList.add("show");
          }, 100);

          // 隐藏toast
          setTimeout(() => {
            toast.classList.remove("show");
          }, duration);
        }

        showAnswerResult(userAnswer, correctAnswer) {
          this.showingExplanation = true;

          // 检查当前题目选项是否为空
          const currentQuestion = this.questions[this.currentQuestionIndex];
          const isDropdownMultiple =
            currentQuestion.type === "dropdown_multiple";
          const hasEmptyOptions =
            !currentQuestion.options ||
            Object.keys(currentQuestion.options).length === 0;

          let isCorrect;

          if (hasEmptyOptions) {
            // 选项为空的题目，提交即算正确
            isCorrect = true;
          } else if (isDropdownMultiple) {
            // 下拉选择题型：比较JSON对象
            try {
              const userAnswers = JSON.parse(userAnswer);
              const correctAnswers = correctAnswer;
              let allCorrect = true;

              for (const scenarioId in correctAnswers) {
                if (userAnswers[scenarioId] !== correctAnswers[scenarioId]) {
                  allCorrect = false;
                  break;
                }
              }

              isCorrect = allCorrect;
            } catch (e) {
              console.error("Error parsing dropdown multiple answers:", e);
              isCorrect = false;
            }
          } else {
            const isMultiChoice = this.isMultiChoiceQuestion(correctAnswer);

            if (isMultiChoice) {
              // 对于多选题，将答案排序后比较
              const userAnswersSorted = userAnswer.split("").sort().join("");
              const correctAnswersSorted = correctAnswer
                .split("")
                .sort()
                .join("");
              isCorrect = userAnswersSorted === correctAnswersSorted;
            } else {
              // 对于单选题，直接比较
              isCorrect = userAnswer === correctAnswer;
            }
          }

          // 更新导航按钮样式
          const navBtn = document.querySelector(
            `.nav-btn[data-question="${this.currentQuestionIndex}"]`
          );
          if (navBtn) {
            // 先清除之前的状态样式
            navBtn.classList.remove("correct", "incorrect", "answered");
            // 为所有已回答的题目添加answered类
            navBtn.classList.add("answered");
            // 添加对错状态样式
            if (isCorrect) {
              navBtn.classList.add("correct");
              this.correctAnswers++;
            } else {
              navBtn.classList.add("incorrect");
            }
            // 确保current样式存在
            navBtn.classList.add("current");
          }

          this.totalAnswered++;

          // 高亮显示用户的选择和正确答案
          this.highlightAnswerOptions(userAnswer, correctAnswer);

          // 显示答案和解释
          document.getElementById("explanation").style.display = "block";
          document.getElementById("explanation-text").textContent =
            this.questions[this.currentQuestionIndex].explanation;

          // 对于刚提交答案的题目，隐藏提交按钮
          const submitBtn = document.getElementById("submit-btn");
          submitBtn.style.display = "none";

          // 显示toast提示
          if (isCorrect) {
            this.showToast("回答正确！", "success", 2000);

            // 答对后3秒自动跳转到随机题目
            setTimeout(() => {
              this.randomQuestion();
            }, 3000);
          } else {
            this.showToast("回答错误，请查看解释", "error", 4000);
          }

          this.updateProgress();
        }

        nextQuestion() {
          if (this.currentQuestionIndex < this.questions.length - 1) {
            this.loadQuestion(this.currentQuestionIndex + 1);
          }
        }

        previousQuestion() {
          if (this.currentQuestionIndex > 0) {
            this.loadQuestion(this.currentQuestionIndex - 1);
          }
        }

        randomQuestion() {
          // 获取所有未回答的题目索引
          const unansweredQuestions = [];
          for (let i = 0; i < this.questions.length; i++) {
            if (
              this.userAnswers[i] === undefined &&
              i !== this.currentQuestionIndex
            ) {
              unansweredQuestions.push(i);
            }
          }

          // 如果还有未回答的题目，从中随机选择一个
          if (unansweredQuestions.length > 0) {
            const randomIndex =
              unansweredQuestions[
                Math.floor(Math.random() * unansweredQuestions.length)
              ];
            this.loadQuestion(randomIndex);
          } else {
            // 如果所有题目都已回答过，显示提示信息
            this.showToast("🎉 恭喜！你已经完成了所有题目！", "success", 5000);

            // 可以选择回到第一题或保持在当前页面
            // 这里选择回到第一题重新开始
            setTimeout(() => {
              if (confirm("是否要重新开始答题？")) {
                this.resetQuiz();
              }
            }, 2000);
          }
        }

        generateNavigation() {
          const navGrid = document.getElementById("nav-grid");
          navGrid.innerHTML = "";

          // 计算网格行数
          const questionsPerRow = 10;
          const rows = Math.ceil(this.questions.length / questionsPerRow);

          for (let i = 0; i < this.questions.length; i++) {
            const btn = document.createElement("button");
            btn.className = "nav-btn";
            btn.setAttribute("data-question", i);
            btn.onclick = () => this.loadQuestion(i);

            const question = this.questions[i];

            // 添加题型图标和数据属性
            const questionTypeIcon = this.getQuestionTypeIcon(question);
            let questionTypeClass = "single";
            if (question.type === "dropdown_multiple") {
              questionTypeClass = "dropdown";
            } else if (this.isMultiChoiceQuestion(question.correct_answer)) {
              questionTypeClass = "multiple";
            }

            btn.setAttribute("data-type", questionTypeClass);
            btn.innerHTML = `
              <span class="nav-question-number">${i + 1}</span>
              <span class="nav-question-type">${questionTypeIcon}</span>
            `;

            // 如果用户已经回答过这道题，添加样式
            if (this.userAnswers[i]) {
              // 为所有已回答的题目添加answered类
              btn.classList.add("answered");
              if (this.isAnswerCorrect(question, this.userAnswers[i])) {
                btn.classList.add("correct");
              } else {
                btn.classList.add("incorrect");
              }
            }

            // 如果这是当前题目，添加current样式
            if (i === this.currentQuestionIndex) {
              btn.classList.add("current");
            }

            navGrid.appendChild(btn);
          }
        }

        updateProgress() {
          // 使用实际回答过的题目数量来计算进度，更准确反映用户完成情况
          const progress = (this.totalAnswered / this.questions.length) * 100;
          document.querySelector(".progress-fill").style.width = `${progress}%`;

          document.querySelector(".current-question").textContent = `题目 ${
            this.currentQuestionIndex + 1
          } / ${this.questions.length} (已答: ${this.totalAnswered})`;

          document.querySelector(
            ".score"
          ).textContent = `答对: ${this.correctAnswers} / ${this.totalAnswered}`;

          // Update accuracy rate
          const accuracy =
            this.totalAnswered > 0
              ? ((this.correctAnswers / this.totalAnswered) * 100).toFixed(1)
              : 0;
          const accuracyElement = document.querySelector(".accuracy");
          if (accuracyElement) {
            accuracyElement.textContent = `正确率: ${accuracy}%`;
          }

          // Update average answer time
          const avgTime = this.calculateAverageTime();
          const avgTimeElement = document.querySelector(".avg-time");
          if (avgTimeElement) {
            avgTimeElement.textContent = `平均每题: ${this.formatTime(
              avgTime
            )}`;
          }

          // Update estimated remaining time
          const remainingQuestions = this.questions.length - this.totalAnswered;
          const estimatedTimeElement =
            document.querySelector(".estimated-time");
          if (estimatedTimeElement) {
            if (this.totalAnswered > 0 && remainingQuestions > 0) {
              const estimatedSeconds = Math.ceil(
                (avgTime / 1000) * remainingQuestions
              );
              estimatedTimeElement.textContent = `预计剩余: ${this.formatTime(
                estimatedSeconds * 1000
              )}`;
            } else if (remainingQuestions === 0) {
              estimatedTimeElement.textContent = `预计剩余: 已完成`;
            } else {
              estimatedTimeElement.textContent = `预计剩余: --`;
            }
          }
        }
      }

      // 创建全局函数供HTML按钮调用
      let quizApp;

      function previousQuestion() {
        if (quizApp) {
          quizApp.previousQuestion();
        }
      }

      function nextQuestion() {
        if (quizApp) {
          quizApp.nextQuestion();
        }
      }

      function randomQuestion() {
        if (quizApp) {
          quizApp.randomQuestion();
        }
      }

      function resetQuiz() {
        if (quizApp) {
          quizApp.resetQuiz();
        }
      }

      // 初始化答题系统
      document.addEventListener("DOMContentLoaded", () => {
        quizApp = new QuizApp();
      });
    </script>
  </body>
</html>
